{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cb953b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "import threading\n",
    "import time\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9f8144",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "014327cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'actions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(FRAMES_PATH, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(AUDIO_PATH, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[43mactions\u001b[49m:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sequence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(no_sequences):\n\u001b[0;32m     18\u001b[0m         os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATASET_PATH, action, \u001b[38;5;28mstr\u001b[39m(sequence)), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'actions' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# === CONFIG ===\n",
    "DATASET_PATH = 'C://MajorProject'\n",
    "FRAMES_PATH = os.path.join(DATASET_PATH, 'frames')\n",
    "AUDIO_PATH = os.path.join(DATASET_PATH, 'audio')\n",
    "SEQUENCE_ID = 0   # Increment manually or via loop for multiple sequences\n",
    "N_FRAMES = 100\n",
    "SR = 16000  # Audio sampling rate\n",
    "CROP_SIZE = 150  # Size of the square crop\n",
    "\n",
    "LIPS_IDX = sorted(set([0,57, 185, 40, 39, 37, 267, 269, 270, 409, 310]))\n",
    "CENTER_IDX = LIPS_IDX[0]  # Use first lip landmark to center crop\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(FRAMES_PATH, exist_ok=True)\n",
    "os.makedirs(AUDIO_PATH, exist_ok=True)\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        os.makedirs(os.path.join(DATASET_PATH, action, str(sequence)), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16c6ebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'C://MajorProject'\n",
    "AUDIO_PATH = os.path.join(DATA_PATH, 'audio')\n",
    "FRAME_PATH = os.path.join(DATA_PATH, 'frames')\n",
    "\n",
    "actions = np.array(['Namaste', 'Dhanyawad', 'Huss'])  # Modify as needed\n",
    "no_sequences = 10\n",
    "sequence_length = 80\n",
    "SR = 16000  # Audio sample rate\n",
    "CROP_SIZE = 120  # Square size for lip crop\n",
    "\n",
    "LIPS_IDX =  sorted(set([0,57, 185, 40, 39, 37, 267, 269, 270, 409, 310]))\n",
    "CENTER_IDX = LIPS_IDX[0]  # Use first lip landmark to center crop\n",
    "\n",
    "# Make sure base directories exist\n",
    "os.makedirs(FRAME_PATH, exist_ok=True)\n",
    "os.makedirs(AUDIO_PATH, exist_ok=True)\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "def record_audio(duration, audio_buffer):\n",
    "    print(\"ðŸŽ™ Audio recording started...\")\n",
    "    audio = sd.rec(int(duration * SR), samplerate=SR, channels=1, dtype='int16')\n",
    "    sd.wait()\n",
    "    audio_buffer.append(audio)\n",
    "    print(\"ðŸŽ™ Audio recording completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6947e039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting capture...\n",
      "ðŸŽ™ Audio recording started...\n",
      "âœ… Frame capture completed.\n",
      "ðŸŽ™ Audio recording completed.\n",
      "âœ… Audio saved to seq_0.wav\n"
     ]
    }
   ],
   "source": [
    "# === AUDIO RECORDING ===\n",
    "audio_recording = []\n",
    "\n",
    "def record_audio(duration):\n",
    "    global audio_recording\n",
    "    print(\"ðŸŽ™ Audio recording started...\")\n",
    "    audio_recording = sd.rec(int(duration * SR), samplerate=SR, channels=1, dtype='int16')\n",
    "    sd.wait()\n",
    "    print(\"ðŸŽ™ Audio recording completed.\")\n",
    "\n",
    "# === VIDEO CAPTURE ===\n",
    "cap = cv2.VideoCapture(0)\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "with mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1,\n",
    "                           min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n",
    "    capturing = False\n",
    "    saved_frames = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(image_rgb)\n",
    "\n",
    "        display_frame = frame.copy()\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            landmarks = results.multi_face_landmarks[0]\n",
    "            h, w, _ = frame.shape\n",
    "            center_lm = landmarks.landmark[CENTER_IDX]\n",
    "            cx, cy = int(center_lm.x * w), int(center_lm.y * h)\n",
    "\n",
    "            # Draw center point\n",
    "            cv2.circle(display_frame, (cx, cy), 3, (0, 255, 0), -1)\n",
    "\n",
    "            # Compute crop\n",
    "            x1 = max(cx - CROP_SIZE // 2, 0)\n",
    "            y1 = max(cy - CROP_SIZE // 2, 0)\n",
    "            x2 = min(cx + CROP_SIZE // 2, w)\n",
    "            y2 = min(cy + CROP_SIZE // 2, h)\n",
    "\n",
    "            cropped = frame[y1:y2, x1:x2]\n",
    "            gray = cv2.cvtColor(cropped, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # If capturing, save frames\n",
    "            if capturing and saved_frames < N_FRAMES:\n",
    "                seq_path = os.path.join(FRAMES_PATH, f'seq_{SEQUENCE_ID}')\n",
    "                os.makedirs(seq_path, exist_ok=True)\n",
    "                cv2.imwrite(os.path.join(seq_path, f'{saved_frames}.jpg'), gray)\n",
    "\n",
    "                # Show cropped frame in window\n",
    "                cv2.putText(display_frame, f'Capturing Frame {saved_frames+1}/{N_FRAMES}', \n",
    "                            (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                cv2.imshow('Captured Crop', gray)\n",
    "\n",
    "                saved_frames += 1\n",
    "\n",
    "                # Stop after N_FRAMES\n",
    "                if saved_frames == N_FRAMES:\n",
    "                    capturing = False\n",
    "                    print(\"âœ… Frame capture completed.\")\n",
    "                    audio_thread.join()\n",
    "                    write(os.path.join(AUDIO_PATH, f'seq_{SEQUENCE_ID}.wav'), SR, audio_recording)\n",
    "                    print(f\"âœ… Audio saved to seq_{SEQUENCE_ID}.wav\")\n",
    "\n",
    "        cv2.imshow('Feedback', display_frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('s') and not capturing:\n",
    "            # Start capture\n",
    "            print(\"ðŸš€ Starting capture...\")\n",
    "            capturing = True\n",
    "            saved_frames = 0\n",
    "            audio_thread = threading.Thread(target=record_audio, args=(N_FRAMES / 30,))\n",
    "            audio_thread.start()\n",
    "        elif key == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ea4e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_next_sequence_number(base_path, action):\n",
    "    action_path = os.path.join(base_path, action)\n",
    "    if not os.path.exists(action_path):\n",
    "        return 0\n",
    "    entries = os.listdir(action_path)\n",
    "    numeric_dirs = [int(name) for name in entries if name.isdigit()]\n",
    "    return max(numeric_dirs) + 1 if numeric_dirs else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d31cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‰ Ready for action: Namaste, sequence: 0. Press 's' to start.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ™ Audio recording started...\n",
      "ðŸŽ™ Audio recording completed.\n",
      "âœ… Saved audio for Namaste sequence 0\n",
      "ðŸ‘‰ Ready for action: Namaste, sequence: 1. Press 's' to start.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m---> 12\u001b[0m     display_frame \u001b[38;5;241m=\u001b[39m \u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m()\n\u001b[0;32m     13\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     14\u001b[0m     display_frame \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#Main\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1,\n",
    "                           min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n",
    "    for action in actions:\n",
    "        for sequence in range(no_sequences):\n",
    "            print(f\" Ready for action: {action}, sequence: {sequence}. Press 's' to start.\")\n",
    "\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                display_frame = frame.copy()\n",
    "                ret, frame = cap.read()\n",
    "                display_frame = frame.copy()\n",
    "                image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                results = face_mesh.process(image_rgb)\n",
    "\n",
    "                cv2.putText(display_frame, f\"Press 's' to start {action} {sequence}\", (10, 30),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                cv2.imshow('OpenCV Feed', display_frame)\n",
    "\n",
    "                if results.multi_face_landmarks:\n",
    "                    landmarks = results.multi_face_landmarks[0]\n",
    "                    h, w, _ = frame.shape\n",
    "                    center_lm = landmarks.landmark[CENTER_IDX]\n",
    "                    cx, cy = int(center_lm.x * w), int(center_lm.y * h)\n",
    "\n",
    "                    x1 = max(cx - CROP_SIZE // 2, 0)\n",
    "                    y1 = max(cy - CROP_SIZE // 2, 0)\n",
    "                    x2 = min(cx + CROP_SIZE // 2, w)\n",
    "                    y2 = min(cy + CROP_SIZE // 2, h)\n",
    "\n",
    "                    # Draw crop rectangle\n",
    "                    cv2.rectangle(display_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "                key = cv2.waitKey(10) & 0xFF\n",
    "                if key == ord('s'):\n",
    "                   \n",
    "                    #time.sleep(3)\n",
    "                    break\n",
    "                elif key == ord('q'):\n",
    "                    cap.release()\n",
    "                    cv2.destroyAllWindows()\n",
    "                    exit()\n",
    "\n",
    "            offset=get_next_sequence_number('C://MajorProject//frames',action)\n",
    "            seq_frame_path = os.path.join(FRAME_PATH, action, str(offset+sequence))\n",
    "            os.makedirs(seq_frame_path, exist_ok=True)\n",
    "            action_audio_path = os.path.join(AUDIO_PATH, action)\n",
    "            os.makedirs(action_audio_path, exist_ok=True)\n",
    "\n",
    "            audio_buffer = []\n",
    "            audio_thread = threading.Thread(target=record_audio, args=(sequence_length / 30, audio_buffer))\n",
    "            audio_thread.start()\n",
    "\n",
    "            saved_frames = 0\n",
    "            prev_time = time.time()\n",
    "\n",
    "            while saved_frames < sequence_length:\n",
    "                ret, frame = cap.read()\n",
    "                display_frame = frame.copy()\n",
    "                image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                results = face_mesh.process(image_rgb)\n",
    "\n",
    "                # FPS calculation\n",
    "                curr_time = time.time()\n",
    "                fps = 1 / (curr_time - prev_time)\n",
    "                prev_time = curr_time\n",
    "\n",
    "                if results.multi_face_landmarks:\n",
    "                    landmarks = results.multi_face_landmarks[0]\n",
    "                    h, w, _ = frame.shape\n",
    "                    center_lm = landmarks.landmark[CENTER_IDX]\n",
    "                    cx, cy = int(center_lm.x * w), int(center_lm.y * h)\n",
    "\n",
    "                    x1 = max(cx - CROP_SIZE // 2, 0)\n",
    "                    y1 = max(cy - CROP_SIZE // 2, 0)\n",
    "                    x2 = min(cx + CROP_SIZE // 2, w)\n",
    "                    y2 = min(cy + CROP_SIZE // 2, h)\n",
    "\n",
    "                    # Draw crop rectangle\n",
    "                    cv2.rectangle(display_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "                    crop = frame[y1:y2, x1:x2]\n",
    "                    gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                    # Save cropped grayscale frame\n",
    "                    frame_path = os.path.join(seq_frame_path, f'{saved_frames}.jpg')\n",
    "                    cv2.imwrite(frame_path, gray)\n",
    "\n",
    "                    saved_frames += 1\n",
    "\n",
    "                # Draw FPS\n",
    "                cv2.putText(display_frame, f'FPS: {fps:.2f}', (10, 70),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)\n",
    "\n",
    "                # Draw progress bar\n",
    "                bar_length = int((saved_frames / sequence_length) * (w - 20))\n",
    "                cv2.rectangle(display_frame, (10, h - 20), (10 + bar_length, h - 10), (0, 255, 0), -1)\n",
    "                cv2.rectangle(display_frame, (10, h - (2*10)), (w - 10, h - 10), (255, 255, 255), 2)\n",
    "\n",
    "                # Show windows\n",
    "                cv2.imshow('OpenCV Feed', display_frame)\n",
    "                cv2.imshow('Cropped Lips', gray if results.multi_face_landmarks else np.zeros((CROP_SIZE, CROP_SIZE), dtype=np.uint8))\n",
    "\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    cap.release()\n",
    "                    cv2.destroyAllWindows()\n",
    "                    exit()\n",
    "\n",
    "            audio_thread.join()\n",
    "            write(os.path.join(action_audio_path, f'{offset+sequence}.wav'), SR, audio_buffer[0])\n",
    "            print(f\"âœ… Saved audio for {action} sequence {sequence}\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcd08d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
