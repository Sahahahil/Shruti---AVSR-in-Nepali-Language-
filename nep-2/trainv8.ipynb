{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ce0bd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Any, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcca587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65d01e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Audio\n",
    "from transformers import (\n",
    "    Wav2Vec2Model,\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    logging as hf_logging,\n",
    ")\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "923f7de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librosa available: True\n",
      "Torchaudio available: True\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import torchaudio\n",
    "    TORCHAUDIO_AVAILABLE = True\n",
    "except Exception:\n",
    "    TORCHAUDIO_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import librosa\n",
    "    LIBROSA_AVAILABLE = True\n",
    "except Exception:\n",
    "    LIBROSA_AVAILABLE = False\n",
    "\n",
    "print(f\"Librosa available: {LIBROSA_AVAILABLE}\")\n",
    "print(f\"Torchaudio available: {TORCHAUDIO_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b48739c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "TSV Path: /home/archy_sahil/MajorProject/College_dataset/Dataset_College_Server/dataset-dec-5-a/07-Dec.tsv\n",
      "Audio Base: /home/archy_sahil/MajorProject/College_dataset/Dataset_College_Server/dataset-dec-5-a/audio\n",
      "HF Cache Dir: /home/archy_sahil/MajorProject/Shruti---AVSR-in-Nepali-Language-/nep-2/cache\n"
     ]
    }
   ],
   "source": [
    "TSV_PATH = \"/home/archy_sahil/MajorProject/College_dataset/Dataset_College_Server/dataset-dec-5-a/07-Dec.tsv\"\n",
    "AUDIO_BASE = \"/home/archy_sahil/MajorProject/College_dataset/Dataset_College_Server/dataset-dec-5-a/audio\"\n",
    "TOKENIZER_DIR = \"./tokenizer_nepali-3\"\n",
    "PRETRAINED_W2V = \"/home/archy_sahil/MajorProject/Models & Processors/wav2vec2-nepali-finetuned-v2\"\n",
    "CACHE_DIR = \"./cache\"\n",
    "OUTPUT_DIR = \"./wav2vec2_custom_head-2-5Dec\"\n",
    "SAMPLE_RATE = 16000\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"TSV Path: {TSV_PATH}\")\n",
    "print(f\"Audio Base: {AUDIO_BASE}\")\n",
    "print(f\"HF Cache Dir: {os.path.abspath(CACHE_DIR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35155e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeze Wav2Vec: True\n",
      "Use Adapters: True\n"
     ]
    }
   ],
   "source": [
    "FREEZE_W2V = True\n",
    "USE_ADAPTERS = True\n",
    "ADAPTER_BOTTLENECK = 128\n",
    "CUSTOM_TRANSFORMER_LAYERS = 4\n",
    "CUSTOM_TRANSFORMER_DIM = 768\n",
    "CUSTOM_TRANSFORMER_HEADS = 8\n",
    "DROPOUT = 0.1\n",
    "USE_SPEC_AUG = True\n",
    "USE_AUDIO_AUG = True\n",
    "\n",
    "print(f\"Freeze Wav2Vec: {FREEZE_W2V}\")\n",
    "print(f\"Use Adapters: {USE_ADAPTERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05a2ca26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5, LR: 3e-05, Batch Size: 1\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "# Lowered learning rate for stability\n",
    "LR = 3e-5 \n",
    "BATCH_SIZE = 1\n",
    "\n",
    "print(f\"Epochs: {EPOCHS}, LR: {LR}, Batch Size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "124d57f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsCallback(TrainerCallback):\n",
    "    \"\"\"Custom callback to track training and validation metrics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.training_loss = []\n",
    "        self.validation_loss = []\n",
    "        self.wer_scores = []\n",
    "        self.cer_scores = [] # New: Track Character Error Rate\n",
    "        self.steps = []\n",
    "        self.eval_steps = []\n",
    "        self.trainer = None\n",
    "        \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"Called when logging happens\"\"\"\n",
    "        if logs is not None:\n",
    "            if 'loss' in logs:\n",
    "                self.training_loss.append(logs['loss'])\n",
    "                self.steps.append(state.global_step)\n",
    "            \n",
    "            if 'eval_loss' in logs:\n",
    "                self.validation_loss.append(logs['eval_loss'])\n",
    "                self.eval_steps.append(state.global_step)\n",
    "            \n",
    "            if 'eval_wer' in logs:\n",
    "                self.wer_scores.append(logs['eval_wer'])\n",
    "            \n",
    "            if 'eval_cer' in logs: # New: Track CER\n",
    "                self.cer_scores.append(logs['eval_cer'])\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def plot_metrics(self, save_path=None):\n",
    "        \"\"\"Plot all metrics\"\"\"\n",
    "        try:\n",
    "            # Create two plots: Loss and Error Rates (WER/CER)\n",
    "            fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "            \n",
    "            # Plot 1: Loss\n",
    "            if self.training_loss and self.validation_loss:\n",
    "                axes[0].plot(self.steps, self.training_loss,\n",
    "                            label='Training Loss', linewidth=2, alpha=0.7, color='blue')\n",
    "                axes[0].plot(self.eval_steps, self.validation_loss,\n",
    "                            marker='o', label='Validation Loss', linewidth=2, color='orange')\n",
    "                axes[0].set_xlabel('Steps')\n",
    "                axes[0].set_ylabel('Loss')\n",
    "                axes[0].set_title('Training and Validation Loss')\n",
    "                axes[0].legend()\n",
    "                axes[0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot 2: WER and CER\n",
    "            if self.wer_scores:\n",
    "                axes[1].plot(self.eval_steps, self.wer_scores, \n",
    "                            marker='o', color='red', label='WER', linewidth=2)\n",
    "            if self.cer_scores:\n",
    "                axes[1].plot(self.eval_steps, self.cer_scores, \n",
    "                            marker='x', color='green', label='CER', linewidth=2, linestyle='--')\n",
    "\n",
    "            if self.wer_scores or self.cer_scores:\n",
    "                axes[1].set_xlabel('Steps')\n",
    "                axes[1].set_ylabel('Error Rate (Lower is Better)')\n",
    "                axes[1].set_title('Word Error Rate (WER) and Character Error Rate (CER)')\n",
    "                axes[1].legend()\n",
    "                axes[1].grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            if save_path:\n",
    "                plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "                print(f\"‚úÖ Metrics plot saved to {save_path}\")\n",
    "            \n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Plotting failed: {e}\")\n",
    "\n",
    "    def print_summary(self):\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TRAINING METRICS SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        if self.validation_loss:\n",
    "            print(f\"Final Val Loss: {self.validation_loss[-1]:.4f}\")\n",
    "        if self.wer_scores:\n",
    "            print(f\"Final WER: {self.wer_scores[-1]:.4f}\")\n",
    "        if self.cer_scores:\n",
    "            print(f\"Final CER: {self.cer_scores[-1]:.4f}\")\n",
    "        print(\"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb27e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_create_tokenizer(tsv_path: str, tokenizer_dir: str):\n",
    "    os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "    vocab_file = os.path.join(tokenizer_dir, \"vocab.json\")\n",
    "    \n",
    "    if os.path.exists(vocab_file):\n",
    "        try:\n",
    "            with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "                vocab_dict = json.load(f)\n",
    "            \n",
    "            tokenizer = Wav2Vec2CTCTokenizer(\n",
    "                vocab_file,\n",
    "                unk_token=\"<unk>\",\n",
    "                pad_token=\"<pad>\",\n",
    "                word_delimiter_token=\"|\"\n",
    "            )\n",
    "            print(f\"‚úÖ Loaded existing tokenizer from {vocab_file}\")\n",
    "            return tokenizer\n",
    "        except Exception as e:\n",
    "            print(f\"Loading existing tokenizer failed: {e}\")\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(tsv_path, sep=\"\\t\", low_memory=False)\n",
    "    \n",
    "    text_column = None\n",
    "    possible_text_cols = ['text', 'transcription', 'transcript', 'sentence', 'label']\n",
    "    for col in possible_text_cols:\n",
    "        if col in df.columns:\n",
    "            text_column = col\n",
    "            break\n",
    "    \n",
    "    if text_column is None and len(df.columns) >= 2:\n",
    "        text_column = df.columns[1]\n",
    "    \n",
    "    texts = df[text_column].astype(str).tolist()\n",
    "    \n",
    "    chars = set()\n",
    "    for text in texts:\n",
    "        chars.update(text.lower())\n",
    "    \n",
    "    chars.add('|')\n",
    "    \n",
    "    vocab_list = ['<blank>', '<unk>', '<pad>'] + sorted(list(chars))\n",
    "    vocab_dict = {char: i for i, char in enumerate(vocab_list)}\n",
    "    \n",
    "    with open(vocab_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(vocab_dict, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    tokenizer = Wav2Vec2CTCTokenizer(\n",
    "        vocab_file,\n",
    "        unk_token=\"<unk>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        word_delimiter_token=\"|\"\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Created new tokenizer and saved to {vocab_file}\")\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eec39c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speed_perturb(wave: np.ndarray, sr: int, factors=(0.9, 1.0, 1.1)) -> np.ndarray:\n",
    "    if not LIBROSA_AVAILABLE:\n",
    "        return wave\n",
    "    f = random.choice(factors)\n",
    "    if f == 1.0:\n",
    "        return wave\n",
    "    return librosa.effects.time_stretch(wave, rate=f)\n",
    "\n",
    "def pitch_shift(wave: np.ndarray, sr: int, n_steps=(-2, 0, 2)) -> np.ndarray:\n",
    "    if not LIBROSA_AVAILABLE:\n",
    "        return wave\n",
    "    step = random.choice(n_steps)\n",
    "    return librosa.effects.pitch_shift(wave, sr=sr, n_steps=step)\n",
    "\n",
    "def add_background_noise(wave: np.ndarray, snr_db_min=5, snr_db_max=20) -> np.ndarray:\n",
    "    rms = np.sqrt(np.mean(wave**2))\n",
    "    if rms == 0:\n",
    "        return wave\n",
    "    snr_db = random.uniform(snr_db_min, snr_db_max)\n",
    "    snr = 10 ** (snr_db / 20.0)\n",
    "    noise_rms = rms / snr\n",
    "    noise = np.random.normal(0, noise_rms, wave.shape)\n",
    "    return wave + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fe7bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adapter(nn.Module):\n",
    "    def __init__(self, dim, bottleneck=128):\n",
    "        super().__init__()\n",
    "        self.down = nn.Linear(dim, bottleneck)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.up = nn.Linear(bottleneck, dim)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.xavier_uniform_(self.down.weight)\n",
    "        nn.init.xavier_uniform_(self.up.weight)\n",
    "        nn.init.zeros_(self.down.bias)\n",
    "        nn.init.zeros_(self.up.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.down(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.up(x)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24d31b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformerCTCHead(nn.Module):\n",
    "    def __init__(self, input_dim: int, model_dim: int, num_layers: int, \n",
    "                 num_heads: int, dropout: float, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.model_dim = model_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        if input_dim != model_dim:\n",
    "            self.input_proj = nn.Linear(input_dim, model_dim)\n",
    "        else:\n",
    "            self.input_proj = nn.Identity()\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=model_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.output_proj = nn.Linear(model_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights to prevent exploding gradients\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "        \n",
    "    def forward(self, features: torch.Tensor, src_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        x = self.input_proj(features)\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        logits = self.output_proj(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2f38530",
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2V2WithCustomHead(nn.Module):\n",
    "    def __init__(self, pretrained_name: str, processor: Wav2Vec2Processor, \n",
    "                 tokenizer: Wav2Vec2CTCTokenizer, freeze_w2v: bool=True, \n",
    "                 use_adapters: bool=True, adapter_dim: int=128,\n",
    "                 trans_layers: int=4, trans_dim: int=768, trans_heads: int=8, \n",
    "                 dropout: float=0.1):\n",
    "        super().__init__()\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = len(tokenizer.get_vocab())\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        \n",
    "        print(f\"   Loading Wav2Vec2 from: {pretrained_name}\")\n",
    "        self.wav2vec = Wav2Vec2Model.from_pretrained(\n",
    "            pretrained_name,\n",
    "            cache_dir=CACHE_DIR,\n",
    "        )\n",
    "        self.wav2vec_feature_dim = self.wav2vec.config.hidden_size\n",
    "\n",
    "        if freeze_w2v:\n",
    "            for p in self.wav2vec.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.use_adapters = use_adapters\n",
    "        if use_adapters:\n",
    "            self.adapter = Adapter(self.wav2vec_feature_dim, bottleneck=adapter_dim)\n",
    "        else:\n",
    "            self.adapter = None\n",
    "\n",
    "        self.custom_head = CustomTransformerCTCHead(\n",
    "            input_dim=self.wav2vec_feature_dim, \n",
    "            model_dim=trans_dim, \n",
    "            num_layers=trans_layers, \n",
    "            num_heads=trans_heads, \n",
    "            dropout=dropout, \n",
    "            vocab_size=self.vocab_size\n",
    "        )\n",
    "\n",
    "    def forward(self, input_values: torch.Tensor, attention_mask: Optional[torch.Tensor]=None, \n",
    "                labels: Optional[torch.Tensor]=None):\n",
    "        \n",
    "        # 1. Forward Wav2Vec2\n",
    "        outputs = self.wav2vec(input_values, attention_mask=attention_mask)\n",
    "        features = outputs.last_hidden_state\n",
    "        \n",
    "        # 2. Compute padding mask for Transformer\n",
    "        transformer_mask = None\n",
    "        if attention_mask is not None:\n",
    "            batch_size, input_len = input_values.shape\n",
    "            output_len = features.shape[1]\n",
    "            reduced_mask_len = (attention_mask.sum(dim=1) / input_len * output_len).long()\n",
    "            transformer_mask = torch.zeros((batch_size, output_len), dtype=torch.bool, device=features.device)\n",
    "            for i in range(batch_size):\n",
    "                valid_len = reduced_mask_len[i].item()\n",
    "                if valid_len < output_len:\n",
    "                    transformer_mask[i, valid_len:] = True\n",
    "\n",
    "        # 3. Adapters\n",
    "        if self.adapter is not None:\n",
    "            features = self.adapter(features)\n",
    "\n",
    "        # 4. Custom Head\n",
    "        logits = self.custom_head(features, src_key_padding_mask=transformer_mask)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.to(logits.device)\n",
    "            # Ensure model output is float32 for stable log_softmax\n",
    "            log_probs = F.log_softmax(logits.float(), dim=-1)\n",
    "            log_probs_t = log_probs.transpose(0, 1).contiguous()\n",
    "            \n",
    "            # Input lengths (Time steps)\n",
    "            input_lengths = torch.full((logits.size(0),), logits.size(1), dtype=torch.long, device=logits.device)\n",
    "            if transformer_mask is not None:\n",
    "                input_lengths = (~transformer_mask).sum(dim=1)\n",
    "\n",
    "            # Target lengths\n",
    "            pad_id = self.pad_token_id\n",
    "            labels_mask = (labels != -100) & (labels != pad_id)\n",
    "            target_lengths = labels_mask.sum(dim=-1)\n",
    "            targets = labels[labels_mask]\n",
    "\n",
    "            # FIX: CLIP TARGET LENGTHS to ensure target length <= input length (CTC requirement)\n",
    "            target_lengths = torch.clamp(target_lengths, max=input_lengths)\n",
    "            \n",
    "            ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True, reduction='mean')\n",
    "            loss = ctc_loss(log_probs_t, targets, input_lengths, target_lengths)\n",
    "\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits': logits,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "587c962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithAugment:\n",
    "    processor: Wav2Vec2Processor\n",
    "    tokenizer: Wav2Vec2CTCTokenizer\n",
    "    sample_rate: int = SAMPLE_RATE\n",
    "    padding: bool = True\n",
    "    apply_spec_augment: bool = True\n",
    "    apply_audio_aug: bool = True\n",
    "    # Minimal length restriction to allow \"whatever data\"\n",
    "    min_audio_length: int = 10 \n",
    "    \n",
    "    pad_token_id: int = field(init=False)\n",
    "    unk_token_id: int = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.pad_token_id = self.tokenizer.pad_token_id\n",
    "        self.unk_token_id = self.tokenizer.unk_token_id\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        input_values = []\n",
    "        attention_masks = []\n",
    "        labels = []\n",
    "\n",
    "        def _load_audio(path: str):\n",
    "            if LIBROSA_AVAILABLE:\n",
    "                try:\n",
    "                    arr, sr = librosa.load(path, sr=self.sample_rate)\n",
    "                    return arr, sr\n",
    "                except:\n",
    "                    return None, None\n",
    "            return None, None\n",
    "\n",
    "        for f in features:\n",
    "            try:\n",
    "                audio_path = f.get(\"audio_path\", \"\")\n",
    "                text = str(f.get(\"text\", \"\")).lower().strip()\n",
    "                if not audio_path or not text: continue\n",
    "\n",
    "                arr, sr = _load_audio(audio_path)\n",
    "                if arr is None: continue\n",
    "                if len(arr) < self.min_audio_length: continue \n",
    "\n",
    "                if self.apply_audio_aug and random.random() < 0.3:\n",
    "                     arr = speed_perturb(arr, self.sample_rate)\n",
    "\n",
    "                inputs = self.processor(arr, sampling_rate=self.sample_rate, return_tensors=\"pt\")\n",
    "                input_tensor = inputs.input_values.squeeze(0)\n",
    "                \n",
    "                token_ids = []\n",
    "                vocab = self.tokenizer.get_vocab()\n",
    "                for char in text:\n",
    "                    token_ids.append(vocab.get(char, self.unk_token_id))\n",
    "\n",
    "                if not token_ids: continue\n",
    "\n",
    "                input_values.append(input_tensor)\n",
    "                attention_masks.append(inputs.attention_mask.squeeze(0) if hasattr(inputs, 'attention_mask') else torch.ones_like(input_tensor, dtype=torch.long))\n",
    "                labels.append(torch.tensor(token_ids, dtype=torch.long))\n",
    "\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if len(input_values) == 0:\n",
    "            dummy = torch.zeros(self.sample_rate)\n",
    "            return {\n",
    "                \"input_values\": dummy.unsqueeze(0),\n",
    "                \"attention_mask\": torch.ones_like(dummy, dtype=torch.long).unsqueeze(0),\n",
    "                \"labels\": torch.tensor([-100]).unsqueeze(0)\n",
    "            }\n",
    "\n",
    "        input_values = nn.utils.rnn.pad_sequence(input_values, batch_first=True, padding_value=0.0)\n",
    "        attention_masks = nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "        labels = nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=self.pad_token_id)\n",
    "        labels[labels == self.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_values\": input_values,\n",
    "            \"attention_mask\": attention_masks,\n",
    "            \"labels\": labels,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6047d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(tsv_path: str, audio_base: str) -> Dataset:\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(tsv_path, sep='\\t', low_memory=False)\n",
    "    \n",
    "    cols = df.columns.tolist()\n",
    "    if len(cols) >= 2: df = df.rename(columns={cols[0]: 'path', cols[1]: 'text'})\n",
    "    \n",
    "    df = df.dropna(subset=['path', 'text'])\n",
    "    df['path'] = df['path'].astype(str)\n",
    "    df['text'] = df['text'].astype(str)\n",
    "    \n",
    "    def get_full_path(p):\n",
    "        if os.path.isabs(p): return p\n",
    "        full = os.path.join(audio_base, p)\n",
    "        if os.path.exists(full): return full\n",
    "        if os.path.exists(full + '.wav'): return full + '.wav'\n",
    "        return None\n",
    "    \n",
    "    df['audio_path'] = df['path'].apply(get_full_path)\n",
    "    df = df[df['audio_path'].notna()]\n",
    "    \n",
    "    print(f\"‚úÖ Final dataset size: {len(df)} samples\")\n",
    "    dataset = Dataset.from_pandas(df[['audio_path', 'text']])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16dca41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFWrapperModel(nn.Module):\n",
    "    def __init__(self, inner_model):\n",
    "        super().__init__()\n",
    "        self.inner = inner_model\n",
    "    \n",
    "    def forward(self, input_values=None, labels=None, attention_mask=None, **kwargs):\n",
    "        outputs = self.inner(input_values=input_values, attention_mask=attention_mask, labels=labels)\n",
    "        return {\"loss\": outputs['loss'], \"logits\": outputs['logits']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e8656c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = load('wer')\n",
    "# Load CER metric\n",
    "cer_metric = load('cer')\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits = pred.predictions\n",
    "    if isinstance(logits, tuple): logits = logits[0]\n",
    "    \n",
    "    # Decode\n",
    "    pred_ids = np.argmax(logits, axis=-1)\n",
    "    label_ids = pred.label_ids \n",
    "    \n",
    "    # Replace -100 with pad token for decoding\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    pred_str = tokenizer.batch_decode(pred_ids)\n",
    "    label_str = tokenizer.batch_decode(label_ids, group_tokens=False)\n",
    "    \n",
    "    # Compute WER\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    # Compute CER\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer, \"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa8efce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ STARTING TRAINING\n",
      "‚úÖ Loaded existing tokenizer from ./tokenizer_nepali-3/vocab.json\n",
      "‚úÖ Final dataset size: 409 samples\n",
      "   Loading Wav2Vec2 from: /home/archy_sahil/MajorProject/Models & Processors/wav2vec2-nepali-finetuned-v2\n",
      "\n",
      "==================================================\n",
      "üèãÔ∏è STARTING TRAINING\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='73' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 73/460 01:10 < 06:25, 1.00 it/s, Epoch 0.78/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 3.63 GiB of which 10.44 MiB is free. Process 1499 has 4.65 MiB memory in use. Process 1541 has 43.90 MiB memory in use. Process 2926 has 4.84 MiB memory in use. Including non-PyTorch memory, this process has 3.52 GiB memory in use. Of the allocated memory 3.26 GiB is allocated by PyTorch, and 175.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müèãÔ∏è STARTING TRAINING\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ TRAINING COMPLETED\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/MajorProject/Major/lib/python3.10/site-packages/transformers/trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MajorProject/Major/lib/python3.10/site-packages/transformers/trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2667\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2668\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2672\u001b[0m )\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2680\u001b[0m ):\n\u001b[1;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/MajorProject/Major/lib/python3.10/site-packages/transformers/trainer.py:4020\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   4019\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 4020\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4022\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   4023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4024\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4025\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   4026\u001b[0m ):\n",
      "File \u001b[0;32m~/MajorProject/Major/lib/python3.10/site-packages/transformers/trainer.py:4110\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4108\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   4109\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m-> 4110\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   4112\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   4113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/MajorProject/Major/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MajorProject/Major/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m, in \u001b[0;36mHFWrapperModel.forward\u001b[0;34m(self, input_values, labels, attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m----> 7\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m: outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n",
      "File \u001b[0;32m~/MajorProject/Major/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MajorProject/Major/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[13], line 63\u001b[0m, in \u001b[0;36mW2V2WithCustomHead.forward\u001b[0;34m(self, input_values, attention_mask, labels)\u001b[0m\n\u001b[1;32m     60\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter(features)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# 4. Custom Head\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/MajorProject/Major/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MajorProject/Major/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[12], line 37\u001b[0m, in \u001b[0;36mCustomTransformerCTCHead.forward\u001b[0;34m(self, features, src_key_padding_mask)\u001b[0m\n\u001b[1;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_proj(features)\n\u001b[1;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[0;32m---> 37\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_proj(x)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/MajorProject/Major/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MajorProject/Major/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/MajorProject/Major/lib/python3.10/site-packages/torch/nn/modules/transformer.py:524\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    521\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 524\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    532\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.0\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m~/MajorProject/Major/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MajorProject/Major/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/MajorProject/Major/lib/python3.10/site-packages/torch/nn/modules/transformer.py:937\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\n\u001b[1;32m    934\u001b[0m         x\n\u001b[1;32m    935\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m    936\u001b[0m     )\n\u001b[0;32m--> 937\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ff_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/MajorProject/Major/lib/python3.10/site-packages/torch/nn/modules/transformer.py:962\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._ff_block\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_ff_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 962\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[0;32m~/MajorProject/Major/lib/python3.10/site-packages/torch/nn/functional.py:1697\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1695\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1697\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 3.63 GiB of which 10.44 MiB is free. Process 1499 has 4.65 MiB memory in use. Process 1541 has 43.90 MiB memory in use. Process 2926 has 4.84 MiB memory in use. Including non-PyTorch memory, this process has 3.52 GiB memory in use. Of the allocated memory 3.26 GiB is allocated by PyTorch, and 175.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nüöÄ STARTING TRAINING\")\n",
    "    \n",
    "    # Tokenizer\n",
    "    tokenizer = load_or_create_tokenizer(TSV_PATH, TOKENIZER_DIR)\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=SAMPLE_RATE, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "    processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "    \n",
    "    # Dataset\n",
    "    dataset = prepare_dataset(TSV_PATH, AUDIO_BASE)\n",
    "    dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    \n",
    "    # Model\n",
    "    model = W2V2WithCustomHead(\n",
    "        pretrained_name=PRETRAINED_W2V,\n",
    "        processor=processor,\n",
    "        tokenizer=tokenizer,\n",
    "        freeze_w2v=FREEZE_W2V,\n",
    "        use_adapters=USE_ADAPTERS,\n",
    "        adapter_dim=ADAPTER_BOTTLENECK,\n",
    "        trans_layers=CUSTOM_TRANSFORMER_LAYERS,\n",
    "        trans_dim=CUSTOM_TRANSFORMER_DIM,\n",
    "        trans_heads=CUSTOM_TRANSFORMER_HEADS,\n",
    "        dropout=DROPOUT\n",
    "    )\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    # Collator\n",
    "    data_collator = DataCollatorCTCWithAugment(\n",
    "        processor=processor,\n",
    "        tokenizer=tokenizer,\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        apply_spec_augment=USE_SPEC_AUG,\n",
    "        apply_audio_aug=USE_AUDIO_AUG\n",
    "    )\n",
    "    \n",
    "    # Initialize the callback\n",
    "    metrics_callback = MetricsCallback()\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=HFWrapperModel(model),\n",
    "        args=TrainingArguments(\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            gradient_accumulation_steps=1,\n",
    "            num_train_epochs=EPOCHS,\n",
    "            learning_rate=LR,\n",
    "            # CRITICAL FIX: Disabled FP16 for stability\n",
    "            fp16=False, \n",
    "            save_total_limit=2,\n",
    "            logging_steps=10,\n",
    "            save_steps=100,\n",
    "            eval_steps=100,\n",
    "            eval_strategy=\"steps\",\n",
    "            report_to=None,\n",
    "            # CRITICAL FIX: Retains audio_path and text columns for DataCollator\n",
    "            remove_unused_columns=False,\n",
    "            # Added for gradient stability\n",
    "            max_grad_norm=0.5,\n",
    "        ),\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['test'],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[metrics_callback]\n",
    "    )\n",
    "    \n",
    "    # Start training\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üèãÔ∏è STARTING TRAINING\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"‚úÖ TRAINING COMPLETED\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"üìä Running final evaluation...\")\n",
    "    metrics = trainer.evaluate()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL METRICS\")\n",
    "    print(\"=\"*50)\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    # Save model\n",
    "    print(\"\\nüíæ Saving model...\")\n",
    "    trainer.save_model()\n",
    "    # Save processor and tokenizer as well\n",
    "    processor.save_pretrained(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "    print(f\"‚úÖ Model, Processor, and Tokenizer saved to {OUTPUT_DIR}\")\n",
    "    \n",
    "    # Print metrics summary and generate plots\n",
    "    print(\"\\nüìä Generating training metrics visualization...\")\n",
    "    metrics_callback.print_summary()\n",
    "    metrics_callback.plot_metrics(save_path=os.path.join(OUTPUT_DIR, \"training_metrics.png\"))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ALL DONE.\")\n",
    "    print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Major",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
