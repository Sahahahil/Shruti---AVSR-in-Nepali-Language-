{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ce0bd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Any, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcca587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65d01e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Audio\n",
    "from transformers import (\n",
    "    Wav2Vec2Model,\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    logging as hf_logging,\n",
    ")\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "923f7de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librosa available: True\n",
      "Torchaudio available: True\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import torchaudio\n",
    "    TORCHAUDIO_AVAILABLE = True\n",
    "except Exception:\n",
    "    TORCHAUDIO_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import librosa\n",
    "    LIBROSA_AVAILABLE = True\n",
    "except Exception:\n",
    "    LIBROSA_AVAILABLE = False\n",
    "\n",
    "print(f\"Librosa available: {LIBROSA_AVAILABLE}\")\n",
    "print(f\"Torchaudio available: {TORCHAUDIO_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b48739c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "TSV Path: /home/archy_sahil/MajorProject/College_dataset/Dataset_College_Server/dataset-dec-5-a/07-Dec.tsv\n",
      "Audio Base: /home/archy_sahil/MajorProject/College_dataset/Dataset_College_Server/dataset-dec-5-a/audio\n",
      "HF Cache Dir: /home/archy_sahil/MajorProject/Shruti---AVSR-in-Nepali-Language-/nep-2/cache\n"
     ]
    }
   ],
   "source": [
    "TSV_PATH = \"/home/archy_sahil/MajorProject/College_dataset/Dataset_College_Server/dataset-dec-5-a/07-Dec.tsv\"\n",
    "AUDIO_BASE = \"/home/archy_sahil/MajorProject/College_dataset/Dataset_College_Server/dataset-dec-5-a/audio\"\n",
    "TOKENIZER_DIR = \"./tokenizer_nepali-3\"\n",
    "PRETRAINED_W2V = \"/home/archy_sahil/MajorProject/Models & Processors/wav2vec2-nepali-finetuned-v2\"\n",
    "CACHE_DIR = \"./cache\"\n",
    "OUTPUT_DIR = \"./wav2vec2_custom_head-2-5Dec\"\n",
    "SAMPLE_RATE = 16000\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"TSV Path: {TSV_PATH}\")\n",
    "print(f\"Audio Base: {AUDIO_BASE}\")\n",
    "print(f\"HF Cache Dir: {os.path.abspath(CACHE_DIR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35155e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeze Wav2Vec: True\n",
      "Use Adapters: True\n"
     ]
    }
   ],
   "source": [
    "FREEZE_W2V = True\n",
    "USE_ADAPTERS = True\n",
    "ADAPTER_BOTTLENECK = 128\n",
    "CUSTOM_TRANSFORMER_LAYERS = 4\n",
    "CUSTOM_TRANSFORMER_DIM = 768\n",
    "CUSTOM_TRANSFORMER_HEADS = 8\n",
    "DROPOUT = 0.1\n",
    "USE_SPEC_AUG = True\n",
    "USE_AUDIO_AUG = True\n",
    "\n",
    "print(f\"Freeze Wav2Vec: {FREEZE_W2V}\")\n",
    "print(f\"Use Adapters: {USE_ADAPTERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05a2ca26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5, LR: 3e-05, Batch Size: 1\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "# Lowered learning rate for stability\n",
    "LR = 3e-5 \n",
    "BATCH_SIZE = 1\n",
    "\n",
    "print(f\"Epochs: {EPOCHS}, LR: {LR}, Batch Size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "124d57f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsCallback(TrainerCallback):\n",
    "    \"\"\"Custom callback to track training and validation metrics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.training_loss = []\n",
    "        self.validation_loss = []\n",
    "        self.wer_scores = []\n",
    "        self.cer_scores = [] # New: Track Character Error Rate\n",
    "        self.steps = []\n",
    "        self.eval_steps = []\n",
    "        self.trainer = None\n",
    "        \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"Called when logging happens\"\"\"\n",
    "        if logs is not None:\n",
    "            if 'loss' in logs:\n",
    "                self.training_loss.append(logs['loss'])\n",
    "                self.steps.append(state.global_step)\n",
    "            \n",
    "            if 'eval_loss' in logs:\n",
    "                self.validation_loss.append(logs['eval_loss'])\n",
    "                self.eval_steps.append(state.global_step)\n",
    "            \n",
    "            if 'eval_wer' in logs:\n",
    "                self.wer_scores.append(logs['eval_wer'])\n",
    "            \n",
    "            if 'eval_cer' in logs: # New: Track CER\n",
    "                self.cer_scores.append(logs['eval_cer'])\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def plot_metrics(self, save_path=None):\n",
    "        \"\"\"Plot all metrics\"\"\"\n",
    "        try:\n",
    "            # Create two plots: Loss and Error Rates (WER/CER)\n",
    "            fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "            \n",
    "            # Plot 1: Loss\n",
    "            if self.training_loss and self.validation_loss:\n",
    "                axes[0].plot(self.steps, self.training_loss,\n",
    "                            label='Training Loss', linewidth=2, alpha=0.7, color='blue')\n",
    "                axes[0].plot(self.eval_steps, self.validation_loss,\n",
    "                            marker='o', label='Validation Loss', linewidth=2, color='orange')\n",
    "                axes[0].set_xlabel('Steps')\n",
    "                axes[0].set_ylabel('Loss')\n",
    "                axes[0].set_title('Training and Validation Loss')\n",
    "                axes[0].legend()\n",
    "                axes[0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot 2: WER and CER\n",
    "            if self.wer_scores:\n",
    "                axes[1].plot(self.eval_steps, self.wer_scores, \n",
    "                            marker='o', color='red', label='WER', linewidth=2)\n",
    "            if self.cer_scores:\n",
    "                axes[1].plot(self.eval_steps, self.cer_scores, \n",
    "                            marker='x', color='green', label='CER', linewidth=2, linestyle='--')\n",
    "\n",
    "            if self.wer_scores or self.cer_scores:\n",
    "                axes[1].set_xlabel('Steps')\n",
    "                axes[1].set_ylabel('Error Rate (Lower is Better)')\n",
    "                axes[1].set_title('Word Error Rate (WER) and Character Error Rate (CER)')\n",
    "                axes[1].legend()\n",
    "                axes[1].grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            if save_path:\n",
    "                plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "                print(f\"‚úÖ Metrics plot saved to {save_path}\")\n",
    "            \n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Plotting failed: {e}\")\n",
    "\n",
    "    def print_summary(self):\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TRAINING METRICS SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        if self.validation_loss:\n",
    "            print(f\"Final Val Loss: {self.validation_loss[-1]:.4f}\")\n",
    "        if self.wer_scores:\n",
    "            print(f\"Final WER: {self.wer_scores[-1]:.4f}\")\n",
    "        if self.cer_scores:\n",
    "            print(f\"Final CER: {self.cer_scores[-1]:.4f}\")\n",
    "        print(\"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb27e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_create_tokenizer(tsv_path: str, tokenizer_dir: str):\n",
    "    os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "    vocab_file = os.path.join(tokenizer_dir, \"vocab.json\")\n",
    "    \n",
    "    if os.path.exists(vocab_file):\n",
    "        try:\n",
    "            with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "                vocab_dict = json.load(f)\n",
    "            \n",
    "            tokenizer = Wav2Vec2CTCTokenizer(\n",
    "                vocab_file,\n",
    "                unk_token=\"<unk>\",\n",
    "                pad_token=\"<pad>\",\n",
    "                word_delimiter_token=\"|\"\n",
    "            )\n",
    "            print(f\"‚úÖ Loaded existing tokenizer from {vocab_file}\")\n",
    "            return tokenizer\n",
    "        except Exception as e:\n",
    "            print(f\"Loading existing tokenizer failed: {e}\")\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(tsv_path, sep=\"\\t\", low_memory=False)\n",
    "    \n",
    "    text_column = None\n",
    "    possible_text_cols = ['text', 'transcription', 'transcript', 'sentence', 'label']\n",
    "    for col in possible_text_cols:\n",
    "        if col in df.columns:\n",
    "            text_column = col\n",
    "            break\n",
    "    \n",
    "    if text_column is None and len(df.columns) >= 2:\n",
    "        text_column = df.columns[1]\n",
    "    \n",
    "    texts = df[text_column].astype(str).tolist()\n",
    "    \n",
    "    chars = set()\n",
    "    for text in texts:\n",
    "        chars.update(text.lower())\n",
    "    \n",
    "    chars.add('|')\n",
    "    \n",
    "    vocab_list = ['<blank>', '<unk>', '<pad>'] + sorted(list(chars))\n",
    "    vocab_dict = {char: i for i, char in enumerate(vocab_list)}\n",
    "    \n",
    "    with open(vocab_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(vocab_dict, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    tokenizer = Wav2Vec2CTCTokenizer(\n",
    "        vocab_file,\n",
    "        unk_token=\"<unk>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        word_delimiter_token=\"|\"\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Created new tokenizer and saved to {vocab_file}\")\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eec39c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speed_perturb(wave: np.ndarray, sr: int, factors=(0.9, 1.0, 1.1)) -> np.ndarray:\n",
    "    if not LIBROSA_AVAILABLE:\n",
    "        return wave\n",
    "    f = random.choice(factors)\n",
    "    if f == 1.0:\n",
    "        return wave\n",
    "    return librosa.effects.time_stretch(wave, rate=f)\n",
    "\n",
    "def pitch_shift(wave: np.ndarray, sr: int, n_steps=(-2, 0, 2)) -> np.ndarray:\n",
    "    if not LIBROSA_AVAILABLE:\n",
    "        return wave\n",
    "    step = random.choice(n_steps)\n",
    "    return librosa.effects.pitch_shift(wave, sr=sr, n_steps=step)\n",
    "\n",
    "def add_background_noise(wave: np.ndarray, snr_db_min=5, snr_db_max=20) -> np.ndarray:\n",
    "    rms = np.sqrt(np.mean(wave**2))\n",
    "    if rms == 0:\n",
    "        return wave\n",
    "    snr_db = random.uniform(snr_db_min, snr_db_max)\n",
    "    snr = 10 ** (snr_db / 20.0)\n",
    "    noise_rms = rms / snr\n",
    "    noise = np.random.normal(0, noise_rms, wave.shape)\n",
    "    return wave + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fe7bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adapter(nn.Module):\n",
    "    def __init__(self, dim, bottleneck=128):\n",
    "        super().__init__()\n",
    "        self.down = nn.Linear(dim, bottleneck)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.up = nn.Linear(bottleneck, dim)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.xavier_uniform_(self.down.weight)\n",
    "        nn.init.xavier_uniform_(self.up.weight)\n",
    "        nn.init.zeros_(self.down.bias)\n",
    "        nn.init.zeros_(self.up.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.down(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.up(x)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24d31b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformerCTCHead(nn.Module):\n",
    "    def __init__(self, input_dim: int, model_dim: int, num_layers: int, \n",
    "                 num_heads: int, dropout: float, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.model_dim = model_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        if input_dim != model_dim:\n",
    "            self.input_proj = nn.Linear(input_dim, model_dim)\n",
    "        else:\n",
    "            self.input_proj = nn.Identity()\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=model_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.output_proj = nn.Linear(model_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights to prevent exploding gradients\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "        \n",
    "    def forward(self, features: torch.Tensor, src_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        x = self.input_proj(features)\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        logits = self.output_proj(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2f38530",
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2V2WithCustomHead(nn.Module):\n",
    "    def __init__(self, pretrained_name: str, processor: Wav2Vec2Processor, \n",
    "                 tokenizer: Wav2Vec2CTCTokenizer, freeze_w2v: bool=True, \n",
    "                 use_adapters: bool=True, adapter_dim: int=128,\n",
    "                 trans_layers: int=4, trans_dim: int=768, trans_heads: int=8, \n",
    "                 dropout: float=0.1):\n",
    "        super().__init__()\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = len(tokenizer.get_vocab())\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        \n",
    "        print(f\"   Loading Wav2Vec2 from: {pretrained_name}\")\n",
    "        self.wav2vec = Wav2Vec2Model.from_pretrained(\n",
    "            pretrained_name,\n",
    "            cache_dir=CACHE_DIR,\n",
    "        )\n",
    "        self.wav2vec_feature_dim = self.wav2vec.config.hidden_size\n",
    "\n",
    "        if freeze_w2v:\n",
    "            for p in self.wav2vec.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.use_adapters = use_adapters\n",
    "        if use_adapters:\n",
    "            self.adapter = Adapter(self.wav2vec_feature_dim, bottleneck=adapter_dim)\n",
    "        else:\n",
    "            self.adapter = None\n",
    "\n",
    "        self.custom_head = CustomTransformerCTCHead(\n",
    "            input_dim=self.wav2vec_feature_dim, \n",
    "            model_dim=trans_dim, \n",
    "            num_layers=trans_layers, \n",
    "            num_heads=trans_heads, \n",
    "            dropout=dropout, \n",
    "            vocab_size=self.vocab_size\n",
    "        )\n",
    "\n",
    "    def forward(self, input_values: torch.Tensor, attention_mask: Optional[torch.Tensor]=None, \n",
    "                labels: Optional[torch.Tensor]=None):\n",
    "        \n",
    "        # 1. Forward Wav2Vec2\n",
    "        outputs = self.wav2vec(input_values, attention_mask=attention_mask)\n",
    "        features = outputs.last_hidden_state\n",
    "        \n",
    "        # 2. Compute padding mask for Transformer\n",
    "        transformer_mask = None\n",
    "        if attention_mask is not None:\n",
    "            batch_size, input_len = input_values.shape\n",
    "            output_len = features.shape[1]\n",
    "            reduced_mask_len = (attention_mask.sum(dim=1) / input_len * output_len).long()\n",
    "            transformer_mask = torch.zeros((batch_size, output_len), dtype=torch.bool, device=features.device)\n",
    "            for i in range(batch_size):\n",
    "                valid_len = reduced_mask_len[i].item()\n",
    "                if valid_len < output_len:\n",
    "                    transformer_mask[i, valid_len:] = True\n",
    "\n",
    "        # 3. Adapters\n",
    "        if self.adapter is not None:\n",
    "            features = self.adapter(features)\n",
    "\n",
    "        # 4. Custom Head\n",
    "        logits = self.custom_head(features, src_key_padding_mask=transformer_mask)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.to(logits.device)\n",
    "            # Ensure model output is float32 for stable log_softmax\n",
    "            log_probs = F.log_softmax(logits.float(), dim=-1)\n",
    "            log_probs_t = log_probs.transpose(0, 1).contiguous()\n",
    "            \n",
    "            # Input lengths (Time steps)\n",
    "            input_lengths = torch.full((logits.size(0),), logits.size(1), dtype=torch.long, device=logits.device)\n",
    "            if transformer_mask is not None:\n",
    "                input_lengths = (~transformer_mask).sum(dim=1)\n",
    "\n",
    "            # Target lengths\n",
    "            pad_id = self.pad_token_id\n",
    "            labels_mask = (labels != -100) & (labels != pad_id)\n",
    "            target_lengths = labels_mask.sum(dim=-1)\n",
    "            targets = labels[labels_mask]\n",
    "\n",
    "            # FIX: CLIP TARGET LENGTHS to ensure target length <= input length (CTC requirement)\n",
    "            target_lengths = torch.clamp(target_lengths, max=input_lengths)\n",
    "            \n",
    "            ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True, reduction='mean')\n",
    "            loss = ctc_loss(log_probs_t, targets, input_lengths, target_lengths)\n",
    "\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits': logits,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "587c962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithAugment:\n",
    "    processor: Wav2Vec2Processor\n",
    "    tokenizer: Wav2Vec2CTCTokenizer\n",
    "    sample_rate: int = SAMPLE_RATE\n",
    "    padding: bool = True\n",
    "    apply_spec_augment: bool = True\n",
    "    apply_audio_aug: bool = True\n",
    "    # Minimal length restriction to allow \"whatever data\"\n",
    "    min_audio_length: int = 10 \n",
    "    \n",
    "    pad_token_id: int = field(init=False)\n",
    "    unk_token_id: int = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.pad_token_id = self.tokenizer.pad_token_id\n",
    "        self.unk_token_id = self.tokenizer.unk_token_id\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        input_values = []\n",
    "        attention_masks = []\n",
    "        labels = []\n",
    "\n",
    "        def _load_audio(path: str):\n",
    "            if LIBROSA_AVAILABLE:\n",
    "                try:\n",
    "                    arr, sr = librosa.load(path, sr=self.sample_rate)\n",
    "                    return arr, sr\n",
    "                except:\n",
    "                    return None, None\n",
    "            return None, None\n",
    "\n",
    "        for f in features:\n",
    "            try:\n",
    "                audio_path = f.get(\"audio_path\", \"\")\n",
    "                text = str(f.get(\"text\", \"\")).lower().strip()\n",
    "                if not audio_path or not text: continue\n",
    "\n",
    "                arr, sr = _load_audio(audio_path)\n",
    "                if arr is None: continue\n",
    "                if len(arr) < self.min_audio_length: continue \n",
    "\n",
    "                if self.apply_audio_aug and random.random() < 0.3:\n",
    "                     arr = speed_perturb(arr, self.sample_rate)\n",
    "\n",
    "                inputs = self.processor(arr, sampling_rate=self.sample_rate, return_tensors=\"pt\")\n",
    "                input_tensor = inputs.input_values.squeeze(0)\n",
    "                \n",
    "                token_ids = []\n",
    "                vocab = self.tokenizer.get_vocab()\n",
    "                for char in text:\n",
    "                    token_ids.append(vocab.get(char, self.unk_token_id))\n",
    "\n",
    "                if not token_ids: continue\n",
    "\n",
    "                input_values.append(input_tensor)\n",
    "                attention_masks.append(inputs.attention_mask.squeeze(0) if hasattr(inputs, 'attention_mask') else torch.ones_like(input_tensor, dtype=torch.long))\n",
    "                labels.append(torch.tensor(token_ids, dtype=torch.long))\n",
    "\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if len(input_values) == 0:\n",
    "            dummy = torch.zeros(self.sample_rate)\n",
    "            return {\n",
    "                \"input_values\": dummy.unsqueeze(0),\n",
    "                \"attention_mask\": torch.ones_like(dummy, dtype=torch.long).unsqueeze(0),\n",
    "                \"labels\": torch.tensor([-100]).unsqueeze(0)\n",
    "            }\n",
    "\n",
    "        input_values = nn.utils.rnn.pad_sequence(input_values, batch_first=True, padding_value=0.0)\n",
    "        attention_masks = nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "        labels = nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=self.pad_token_id)\n",
    "        labels[labels == self.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_values\": input_values,\n",
    "            \"attention_mask\": attention_masks,\n",
    "            \"labels\": labels,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6047d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(tsv_path: str, audio_base: str) -> Dataset:\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(tsv_path, sep='\\t', low_memory=False)\n",
    "    \n",
    "    cols = df.columns.tolist()\n",
    "    if len(cols) >= 2: df = df.rename(columns={cols[0]: 'path', cols[1]: 'text'})\n",
    "    \n",
    "    df = df.dropna(subset=['path', 'text'])\n",
    "    df['path'] = df['path'].astype(str)\n",
    "    df['text'] = df['text'].astype(str)\n",
    "    \n",
    "    def get_full_path(p):\n",
    "        if os.path.isabs(p): return p\n",
    "        full = os.path.join(audio_base, p)\n",
    "        if os.path.exists(full): return full\n",
    "        if os.path.exists(full + '.wav'): return full + '.wav'\n",
    "        return None\n",
    "    \n",
    "    df['audio_path'] = df['path'].apply(get_full_path)\n",
    "    df = df[df['audio_path'].notna()]\n",
    "    \n",
    "    print(f\"‚úÖ Final dataset size: {len(df)} samples\")\n",
    "    dataset = Dataset.from_pandas(df[['audio_path', 'text']])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16dca41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFWrapperModel(nn.Module):\n",
    "    def __init__(self, inner_model):\n",
    "        super().__init__()\n",
    "        self.inner = inner_model\n",
    "    \n",
    "    def forward(self, input_values=None, labels=None, attention_mask=None, **kwargs):\n",
    "        outputs = self.inner(input_values=input_values, attention_mask=attention_mask, labels=labels)\n",
    "        return {\"loss\": outputs['loss'], \"logits\": outputs['logits']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e8656c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = load('wer')\n",
    "# Load CER metric\n",
    "cer_metric = load('cer')\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits = pred.predictions\n",
    "    if isinstance(logits, tuple): logits = logits[0]\n",
    "    \n",
    "    # Decode\n",
    "    pred_ids = np.argmax(logits, axis=-1)\n",
    "    label_ids = pred.label_ids \n",
    "    \n",
    "    # Replace -100 with pad token for decoding\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    pred_str = tokenizer.batch_decode(pred_ids)\n",
    "    label_str = tokenizer.batch_decode(label_ids, group_tokens=False)\n",
    "    \n",
    "    # Compute WER\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    # Compute CER\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer, \"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa8efce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ STARTING TRAINING\n",
      "‚úÖ Loaded existing tokenizer from ./tokenizer_nepali-3/vocab.json\n",
      "‚úÖ Final dataset size: 409 samples\n",
      "   Loading Wav2Vec2 from: /home/archy_sahil/MajorProject/Models & Processors/wav2vec2-nepali-finetuned-v2\n",
      "\n",
      "==================================================\n",
      "üèãÔ∏è STARTING TRAINING\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 26/460 00:23 < 07:12, 1.00 it/s, Epoch 0.27/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nüöÄ STARTING TRAINING\")\n",
    "    \n",
    "    # Tokenizer\n",
    "    tokenizer = load_or_create_tokenizer(TSV_PATH, TOKENIZER_DIR)\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=SAMPLE_RATE, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "    processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "    \n",
    "    # Dataset\n",
    "    dataset = prepare_dataset(TSV_PATH, AUDIO_BASE)\n",
    "    dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    \n",
    "    # Model\n",
    "    model = W2V2WithCustomHead(\n",
    "        pretrained_name=PRETRAINED_W2V,\n",
    "        processor=processor,\n",
    "        tokenizer=tokenizer,\n",
    "        freeze_w2v=FREEZE_W2V,\n",
    "        use_adapters=USE_ADAPTERS,\n",
    "        adapter_dim=ADAPTER_BOTTLENECK,\n",
    "        trans_layers=CUSTOM_TRANSFORMER_LAYERS,\n",
    "        trans_dim=CUSTOM_TRANSFORMER_DIM,\n",
    "        trans_heads=CUSTOM_TRANSFORMER_HEADS,\n",
    "        dropout=DROPOUT\n",
    "    )\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    # Collator\n",
    "    data_collator = DataCollatorCTCWithAugment(\n",
    "        processor=processor,\n",
    "        tokenizer=tokenizer,\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        apply_spec_augment=USE_SPEC_AUG,\n",
    "        apply_audio_aug=USE_AUDIO_AUG\n",
    "    )\n",
    "    \n",
    "    # Initialize the callback\n",
    "    metrics_callback = MetricsCallback()\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=HFWrapperModel(model),\n",
    "        args=TrainingArguments(\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            gradient_accumulation_steps=4,\n",
    "            num_train_epochs=EPOCHS,\n",
    "            learning_rate=LR,\n",
    "            # CRITICAL FIX: Disabled FP16 for stability\n",
    "            fp16=False, \n",
    "            save_total_limit=2,\n",
    "            logging_steps=10,\n",
    "            save_steps=200,\n",
    "            eval_steps=200,\n",
    "            eval_strategy=\"steps\",\n",
    "            report_to=None,\n",
    "            # CRITICAL FIX: Retains audio_path and text columns for DataCollator\n",
    "            remove_unused_columns=False,\n",
    "            # Added for gradient stability\n",
    "            max_grad_norm=0.5,\n",
    "        ),\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['test'],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[metrics_callback]\n",
    "    )\n",
    "    \n",
    "    # Start training\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üèãÔ∏è STARTING TRAINING\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"‚úÖ TRAINING COMPLETED\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"üìä Running final evaluation...\")\n",
    "    metrics = trainer.evaluate()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL METRICS\")\n",
    "    print(\"=\"*50)\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    # Save model\n",
    "    print(\"\\nüíæ Saving model...\")\n",
    "    trainer.save_model()\n",
    "    # Save processor and tokenizer as well\n",
    "    processor.save_pretrained(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "    print(f\"‚úÖ Model, Processor, and Tokenizer saved to {OUTPUT_DIR}\")\n",
    "    \n",
    "    # Print metrics summary and generate plots\n",
    "    print(\"\\nüìä Generating training metrics visualization...\")\n",
    "    metrics_callback.print_summary()\n",
    "    metrics_callback.plot_metrics(save_path=os.path.join(OUTPUT_DIR, \"training_metrics.png\"))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ALL DONE.\")\n",
    "    print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Major",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
