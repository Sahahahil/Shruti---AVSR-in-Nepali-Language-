{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.849137931034483,
  "eval_steps": 500,
  "global_step": 4500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.05387931034482758,
      "grad_norm": 592.4740600585938,
      "learning_rate": 4.9000000000000005e-06,
      "loss": 69.0395,
      "step": 50
    },
    {
      "epoch": 0.10775862068965517,
      "grad_norm": 1158.819580078125,
      "learning_rate": 9.900000000000002e-06,
      "loss": 58.5423,
      "step": 100
    },
    {
      "epoch": 0.16163793103448276,
      "grad_norm": 2198.065673828125,
      "learning_rate": 1.49e-05,
      "loss": 71.1841,
      "step": 150
    },
    {
      "epoch": 0.21551724137931033,
      "grad_norm": 1248.8487548828125,
      "learning_rate": 1.9900000000000003e-05,
      "loss": 64.1009,
      "step": 200
    },
    {
      "epoch": 0.26939655172413796,
      "grad_norm": 3273.19189453125,
      "learning_rate": 2.4900000000000002e-05,
      "loss": 62.4413,
      "step": 250
    },
    {
      "epoch": 0.3232758620689655,
      "grad_norm": 1398.7808837890625,
      "learning_rate": 2.9900000000000002e-05,
      "loss": 69.7016,
      "step": 300
    },
    {
      "epoch": 0.3771551724137931,
      "grad_norm": 2388.12939453125,
      "learning_rate": 3.49e-05,
      "loss": 62.2864,
      "step": 350
    },
    {
      "epoch": 0.43103448275862066,
      "grad_norm": 1359.1966552734375,
      "learning_rate": 3.99e-05,
      "loss": 71.1203,
      "step": 400
    },
    {
      "epoch": 0.4849137931034483,
      "grad_norm": 1378.659423828125,
      "learning_rate": 4.49e-05,
      "loss": 73.541,
      "step": 450
    },
    {
      "epoch": 0.5387931034482759,
      "grad_norm": 1127.590576171875,
      "learning_rate": 4.99e-05,
      "loss": 74.2863,
      "step": 500
    },
    {
      "epoch": 0.5926724137931034,
      "grad_norm": 897.7801513671875,
      "learning_rate": 4.940821256038647e-05,
      "loss": 76.2416,
      "step": 550
    },
    {
      "epoch": 0.646551724137931,
      "grad_norm": 27759.837890625,
      "learning_rate": 4.880434782608696e-05,
      "loss": 76.4058,
      "step": 600
    },
    {
      "epoch": 0.7004310344827587,
      "grad_norm": 1040.2666015625,
      "learning_rate": 4.820048309178744e-05,
      "loss": 77.6522,
      "step": 650
    },
    {
      "epoch": 0.7543103448275862,
      "grad_norm": 4642.712890625,
      "learning_rate": 4.7596618357487925e-05,
      "loss": 68.6332,
      "step": 700
    },
    {
      "epoch": 0.8081896551724138,
      "grad_norm": 1636.437255859375,
      "learning_rate": 4.6992753623188404e-05,
      "loss": 69.746,
      "step": 750
    },
    {
      "epoch": 0.8620689655172413,
      "grad_norm": 3917.0498046875,
      "learning_rate": 4.638888888888889e-05,
      "loss": 80.4415,
      "step": 800
    },
    {
      "epoch": 0.915948275862069,
      "grad_norm": 1080.6971435546875,
      "learning_rate": 4.5785024154589376e-05,
      "loss": 73.1416,
      "step": 850
    },
    {
      "epoch": 0.9698275862068966,
      "grad_norm": 778.6304931640625,
      "learning_rate": 4.518115942028986e-05,
      "loss": 72.7613,
      "step": 900
    },
    {
      "epoch": 1.0237068965517242,
      "grad_norm": 1644.494873046875,
      "learning_rate": 4.457729468599034e-05,
      "loss": 64.9527,
      "step": 950
    },
    {
      "epoch": 1.0775862068965518,
      "grad_norm": 1579.1572265625,
      "learning_rate": 4.397342995169083e-05,
      "loss": 58.7709,
      "step": 1000
    },
    {
      "epoch": 1.1314655172413792,
      "grad_norm": 1493.3790283203125,
      "learning_rate": 4.336956521739131e-05,
      "loss": 65.4675,
      "step": 1050
    },
    {
      "epoch": 1.1853448275862069,
      "grad_norm": 2061.3232421875,
      "learning_rate": 4.2765700483091793e-05,
      "loss": 60.009,
      "step": 1100
    },
    {
      "epoch": 1.2392241379310345,
      "grad_norm": 6344.8193359375,
      "learning_rate": 4.216183574879227e-05,
      "loss": 61.8334,
      "step": 1150
    },
    {
      "epoch": 1.293103448275862,
      "grad_norm": 2317.126953125,
      "learning_rate": 4.155797101449276e-05,
      "loss": 58.7503,
      "step": 1200
    },
    {
      "epoch": 1.3469827586206897,
      "grad_norm": 1458.3165283203125,
      "learning_rate": 4.095410628019324e-05,
      "loss": 57.8618,
      "step": 1250
    },
    {
      "epoch": 1.4008620689655173,
      "grad_norm": 1553.3779296875,
      "learning_rate": 4.0350241545893725e-05,
      "loss": 59.5434,
      "step": 1300
    },
    {
      "epoch": 1.4547413793103448,
      "grad_norm": 1504.3399658203125,
      "learning_rate": 3.9746376811594204e-05,
      "loss": 61.2838,
      "step": 1350
    },
    {
      "epoch": 1.5086206896551724,
      "grad_norm": 1249.6456298828125,
      "learning_rate": 3.914251207729468e-05,
      "loss": 60.5486,
      "step": 1400
    },
    {
      "epoch": 1.5625,
      "grad_norm": 3290.87060546875,
      "learning_rate": 3.853864734299517e-05,
      "loss": 54.6674,
      "step": 1450
    },
    {
      "epoch": 1.6163793103448276,
      "grad_norm": 1060.04296875,
      "learning_rate": 3.793478260869565e-05,
      "loss": 57.3074,
      "step": 1500
    },
    {
      "epoch": 1.6702586206896552,
      "grad_norm": 1173.5145263671875,
      "learning_rate": 3.7330917874396135e-05,
      "loss": 58.5677,
      "step": 1550
    },
    {
      "epoch": 1.7241379310344827,
      "grad_norm": 833.424072265625,
      "learning_rate": 3.672705314009662e-05,
      "loss": 56.1495,
      "step": 1600
    },
    {
      "epoch": 1.7780172413793105,
      "grad_norm": 2466.543701171875,
      "learning_rate": 3.61231884057971e-05,
      "loss": 58.5404,
      "step": 1650
    },
    {
      "epoch": 1.831896551724138,
      "grad_norm": 927.916748046875,
      "learning_rate": 3.551932367149759e-05,
      "loss": 60.379,
      "step": 1700
    },
    {
      "epoch": 1.8857758620689655,
      "grad_norm": 950.1531982421875,
      "learning_rate": 3.491545893719807e-05,
      "loss": 57.5762,
      "step": 1750
    },
    {
      "epoch": 1.9396551724137931,
      "grad_norm": 1324.7359619140625,
      "learning_rate": 3.431159420289855e-05,
      "loss": 56.8536,
      "step": 1800
    },
    {
      "epoch": 1.9935344827586206,
      "grad_norm": 997.94287109375,
      "learning_rate": 3.370772946859904e-05,
      "loss": 52.3558,
      "step": 1850
    },
    {
      "epoch": 2.0474137931034484,
      "grad_norm": 1238.0692138671875,
      "learning_rate": 3.310386473429952e-05,
      "loss": 50.8921,
      "step": 1900
    },
    {
      "epoch": 2.101293103448276,
      "grad_norm": 1328.8031005859375,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 45.739,
      "step": 1950
    },
    {
      "epoch": 2.1551724137931036,
      "grad_norm": 1615.1373291015625,
      "learning_rate": 3.189613526570048e-05,
      "loss": 48.8099,
      "step": 2000
    },
    {
      "epoch": 2.209051724137931,
      "grad_norm": 1791.048095703125,
      "learning_rate": 3.129227053140097e-05,
      "loss": 51.1937,
      "step": 2050
    },
    {
      "epoch": 2.2629310344827585,
      "grad_norm": 1952.7728271484375,
      "learning_rate": 3.068840579710145e-05,
      "loss": 48.9528,
      "step": 2100
    },
    {
      "epoch": 2.3168103448275863,
      "grad_norm": 1654.9683837890625,
      "learning_rate": 3.0084541062801935e-05,
      "loss": 47.7689,
      "step": 2150
    },
    {
      "epoch": 2.3706896551724137,
      "grad_norm": 1258.6141357421875,
      "learning_rate": 2.9480676328502414e-05,
      "loss": 52.6733,
      "step": 2200
    },
    {
      "epoch": 2.4245689655172415,
      "grad_norm": 1389.5660400390625,
      "learning_rate": 2.88768115942029e-05,
      "loss": 45.8933,
      "step": 2250
    },
    {
      "epoch": 2.478448275862069,
      "grad_norm": 1049.1915283203125,
      "learning_rate": 2.8272946859903383e-05,
      "loss": 46.606,
      "step": 2300
    },
    {
      "epoch": 2.532327586206897,
      "grad_norm": 780.5433349609375,
      "learning_rate": 2.766908212560387e-05,
      "loss": 47.4161,
      "step": 2350
    },
    {
      "epoch": 2.586206896551724,
      "grad_norm": 1134.0263671875,
      "learning_rate": 2.706521739130435e-05,
      "loss": 38.0346,
      "step": 2400
    },
    {
      "epoch": 2.6400862068965516,
      "grad_norm": 1500.4932861328125,
      "learning_rate": 2.6461352657004835e-05,
      "loss": 40.5415,
      "step": 2450
    },
    {
      "epoch": 2.6939655172413794,
      "grad_norm": 1686.3135986328125,
      "learning_rate": 2.5857487922705314e-05,
      "loss": 47.369,
      "step": 2500
    },
    {
      "epoch": 2.747844827586207,
      "grad_norm": 2456.770751953125,
      "learning_rate": 2.52536231884058e-05,
      "loss": 42.2127,
      "step": 2550
    },
    {
      "epoch": 2.8017241379310347,
      "grad_norm": 822.7715454101562,
      "learning_rate": 2.464975845410628e-05,
      "loss": 49.0107,
      "step": 2600
    },
    {
      "epoch": 2.855603448275862,
      "grad_norm": 749.6536865234375,
      "learning_rate": 2.4045893719806763e-05,
      "loss": 44.1063,
      "step": 2650
    },
    {
      "epoch": 2.9094827586206895,
      "grad_norm": 536.4224243164062,
      "learning_rate": 2.344202898550725e-05,
      "loss": 41.8121,
      "step": 2700
    },
    {
      "epoch": 2.9633620689655173,
      "grad_norm": 1300.9522705078125,
      "learning_rate": 2.283816425120773e-05,
      "loss": 35.1567,
      "step": 2750
    },
    {
      "epoch": 3.0172413793103448,
      "grad_norm": 1194.2635498046875,
      "learning_rate": 2.2234299516908214e-05,
      "loss": 42.9484,
      "step": 2800
    },
    {
      "epoch": 3.0711206896551726,
      "grad_norm": 439.96673583984375,
      "learning_rate": 2.1630434782608697e-05,
      "loss": 35.231,
      "step": 2850
    },
    {
      "epoch": 3.125,
      "grad_norm": 858.6411743164062,
      "learning_rate": 2.102657004830918e-05,
      "loss": 39.7537,
      "step": 2900
    },
    {
      "epoch": 3.1788793103448274,
      "grad_norm": 394.2987060546875,
      "learning_rate": 2.0422705314009663e-05,
      "loss": 37.1714,
      "step": 2950
    },
    {
      "epoch": 3.2327586206896552,
      "grad_norm": 246.47291564941406,
      "learning_rate": 1.9818840579710145e-05,
      "loss": 37.7588,
      "step": 3000
    },
    {
      "epoch": 3.2866379310344827,
      "grad_norm": 1149.33447265625,
      "learning_rate": 1.9214975845410628e-05,
      "loss": 37.3902,
      "step": 3050
    },
    {
      "epoch": 3.3405172413793105,
      "grad_norm": 819.3804321289062,
      "learning_rate": 1.861111111111111e-05,
      "loss": 37.5421,
      "step": 3100
    },
    {
      "epoch": 3.394396551724138,
      "grad_norm": 769.21533203125,
      "learning_rate": 1.8007246376811597e-05,
      "loss": 40.7159,
      "step": 3150
    },
    {
      "epoch": 3.4482758620689653,
      "grad_norm": 1839.3126220703125,
      "learning_rate": 1.740338164251208e-05,
      "loss": 36.7283,
      "step": 3200
    },
    {
      "epoch": 3.502155172413793,
      "grad_norm": 2280.733154296875,
      "learning_rate": 1.6799516908212563e-05,
      "loss": 35.9928,
      "step": 3250
    },
    {
      "epoch": 3.5560344827586206,
      "grad_norm": 586.818359375,
      "learning_rate": 1.6195652173913045e-05,
      "loss": 38.1213,
      "step": 3300
    },
    {
      "epoch": 3.6099137931034484,
      "grad_norm": 2028.075439453125,
      "learning_rate": 1.5591787439613528e-05,
      "loss": 35.0144,
      "step": 3350
    },
    {
      "epoch": 3.663793103448276,
      "grad_norm": 526.91357421875,
      "learning_rate": 1.4987922705314011e-05,
      "loss": 37.2001,
      "step": 3400
    },
    {
      "epoch": 3.717672413793103,
      "grad_norm": 6566.3427734375,
      "learning_rate": 1.4384057971014494e-05,
      "loss": 33.8199,
      "step": 3450
    },
    {
      "epoch": 3.771551724137931,
      "grad_norm": 926.8680419921875,
      "learning_rate": 1.3780193236714978e-05,
      "loss": 38.5048,
      "step": 3500
    },
    {
      "epoch": 3.825431034482759,
      "grad_norm": 767.744873046875,
      "learning_rate": 1.3176328502415461e-05,
      "loss": 37.8988,
      "step": 3550
    },
    {
      "epoch": 3.8793103448275863,
      "grad_norm": 1766.1129150390625,
      "learning_rate": 1.2572463768115942e-05,
      "loss": 30.0432,
      "step": 3600
    },
    {
      "epoch": 3.9331896551724137,
      "grad_norm": 868.2440795898438,
      "learning_rate": 1.1968599033816427e-05,
      "loss": 36.736,
      "step": 3650
    },
    {
      "epoch": 3.987068965517241,
      "grad_norm": 1422.8466796875,
      "learning_rate": 1.136473429951691e-05,
      "loss": 42.1936,
      "step": 3700
    },
    {
      "epoch": 4.040948275862069,
      "grad_norm": 1720.161865234375,
      "learning_rate": 1.0760869565217392e-05,
      "loss": 33.9365,
      "step": 3750
    },
    {
      "epoch": 4.094827586206897,
      "grad_norm": 1212.2491455078125,
      "learning_rate": 1.0157004830917875e-05,
      "loss": 35.6557,
      "step": 3800
    },
    {
      "epoch": 4.148706896551724,
      "grad_norm": 1045.212890625,
      "learning_rate": 9.55314009661836e-06,
      "loss": 27.9892,
      "step": 3850
    },
    {
      "epoch": 4.202586206896552,
      "grad_norm": 1769.2327880859375,
      "learning_rate": 8.949275362318842e-06,
      "loss": 32.7808,
      "step": 3900
    },
    {
      "epoch": 4.256465517241379,
      "grad_norm": 958.0192260742188,
      "learning_rate": 8.345410628019323e-06,
      "loss": 30.9809,
      "step": 3950
    },
    {
      "epoch": 4.310344827586207,
      "grad_norm": 1021.2857666015625,
      "learning_rate": 7.741545893719806e-06,
      "loss": 35.9434,
      "step": 4000
    },
    {
      "epoch": 4.364224137931035,
      "grad_norm": 967.8096313476562,
      "learning_rate": 7.1376811594202895e-06,
      "loss": 34.8329,
      "step": 4050
    },
    {
      "epoch": 4.418103448275862,
      "grad_norm": 1125.4976806640625,
      "learning_rate": 6.533816425120773e-06,
      "loss": 27.2495,
      "step": 4100
    },
    {
      "epoch": 4.4719827586206895,
      "grad_norm": 408.8740234375,
      "learning_rate": 5.929951690821256e-06,
      "loss": 33.2189,
      "step": 4150
    },
    {
      "epoch": 4.525862068965517,
      "grad_norm": 1136.02783203125,
      "learning_rate": 5.3260869565217395e-06,
      "loss": 34.2234,
      "step": 4200
    },
    {
      "epoch": 4.579741379310345,
      "grad_norm": 781.651123046875,
      "learning_rate": 4.722222222222222e-06,
      "loss": 31.1817,
      "step": 4250
    },
    {
      "epoch": 4.633620689655173,
      "grad_norm": 2515.161376953125,
      "learning_rate": 4.118357487922706e-06,
      "loss": 29.7825,
      "step": 4300
    },
    {
      "epoch": 4.6875,
      "grad_norm": 1139.1689453125,
      "learning_rate": 3.5144927536231887e-06,
      "loss": 34.3372,
      "step": 4350
    },
    {
      "epoch": 4.741379310344827,
      "grad_norm": 581.9895629882812,
      "learning_rate": 2.910628019323672e-06,
      "loss": 30.1304,
      "step": 4400
    },
    {
      "epoch": 4.795258620689655,
      "grad_norm": 1036.6190185546875,
      "learning_rate": 2.3067632850241546e-06,
      "loss": 33.3531,
      "step": 4450
    },
    {
      "epoch": 4.849137931034483,
      "grad_norm": 1116.7611083984375,
      "learning_rate": 1.7028985507246378e-06,
      "loss": 31.83,
      "step": 4500
    }
  ],
  "logging_steps": 50,
  "max_steps": 4640,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.950242576476156e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
