{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e315e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# IMPORTS\n",
    "# ========================================\n",
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Any, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import Dataset, Audio\n",
    "from transformers import (\n",
    "    Wav2Vec2Model,\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoProcessor,\n",
    "    logging as hf_logging,\n",
    ")\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e5efd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# LIBRARY CHECKS\n",
    "# ========================================\n",
    "\n",
    "try:\n",
    "    import torchaudio\n",
    "    TORCHAUDIO_AVAILABLE = True\n",
    "except Exception:\n",
    "    TORCHAUDIO_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import librosa\n",
    "    LIBROSA_AVAILABLE = True\n",
    "except Exception:\n",
    "    LIBROSA_AVAILABLE = False\n",
    "\n",
    "print(f\"Librosa available: {LIBROSA_AVAILABLE}\")\n",
    "print(f\"Torchaudio available: {TORCHAUDIO_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2db2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CONFIGURATIONS\n",
    "# ========================================\n",
    "\n",
    "# CONFIGURATIONS\n",
    "TSV_PATH = \"/home/archy_sahil/Downloads/Dataset(Major)/ne_np_female/line_index.tsv\"\n",
    "AUDIO_BASE = \"/home/archy_sahil/Downloads/Dataset(Major)/ne_np_female/wavs\"\n",
    "TOKENIZER_DIR = \"./tokenizer_nepali-3\"\n",
    "# Use the model id and read from local cache\n",
    "PRETRAINED_W2V = \"/home/archy_sahil/MajorProject/Models & Processors/wav2vec2-nepali-finetuned-v2\"\n",
    "CACHE_DIR = \"./cache\"\n",
    "OUTPUT_DIR = \"./wav2vec2_custom_head-2\"\n",
    "SAMPLE_RATE = 16000\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"TSV Path: {TSV_PATH}\")\n",
    "print(f\"Audio Base: {AUDIO_BASE}\")\n",
    "print(f\"HF Cache Dir: {os.path.abspath(CACHE_DIR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb63124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# MODEL CONFIGURATION FLAGS\n",
    "# ========================================\n",
    "\n",
    "FREEZE_W2V = True\n",
    "USE_ADAPTERS = True\n",
    "ADAPTER_BOTTLENECK = 128\n",
    "CUSTOM_TRANSFORMER_LAYERS = 4\n",
    "CUSTOM_TRANSFORMER_DIM = 768\n",
    "CUSTOM_TRANSFORMER_HEADS = 8\n",
    "DROPOUT = 0.1\n",
    "USE_SPEC_AUG = True\n",
    "USE_AUDIO_AUG = True\n",
    "\n",
    "print(f\"Freeze Wav2Vec: {FREEZE_W2V}\")\n",
    "print(f\"Use Adapters: {USE_ADAPTERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3429fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# TRAINING HYPERPARAMETERS\n",
    "# ========================================\n",
    "\n",
    "EPOCHS = 20\n",
    "LR = 5e-5\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "print(f\"Epochs: {EPOCHS}, LR: {LR}, Batch Size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449839c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# TOKENIZER UTILITY FUNCTION\n",
    "# ========================================\n",
    "\n",
    "def load_or_create_tokenizer(tsv_path: str, tokenizer_dir: str):\n",
    "    os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "    vocab_file = os.path.join(tokenizer_dir, \"vocab.json\")\n",
    "    \n",
    "    if os.path.exists(vocab_file):\n",
    "        try:\n",
    "            with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "                vocab_dict = json.load(f)\n",
    "            \n",
    "            tokenizer = Wav2Vec2CTCTokenizer(\n",
    "                vocab_file,\n",
    "                unk_token=\"<unk>\",\n",
    "                pad_token=\"<pad>\",\n",
    "                word_delimiter_token=\"|\"\n",
    "            )\n",
    "            print(f\"‚úÖ Loaded existing tokenizer from {vocab_file}\")\n",
    "            return tokenizer\n",
    "        except Exception as e:\n",
    "            print(f\"Loading existing tokenizer failed: {e}\")\n",
    "\n",
    "    # Create character-level tokenizer for CTC\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(tsv_path, sep=\"\\t\", low_memory=False)\n",
    "    \n",
    "    print(f\"Available columns in TSV: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Find text column\n",
    "    text_column = None\n",
    "    possible_text_cols = ['text', 'transcription', 'transcript', 'sentence', 'label']\n",
    "    for col in possible_text_cols:\n",
    "        if col in df.columns:\n",
    "            text_column = col\n",
    "            break\n",
    "    \n",
    "    if text_column is None and len(df.columns) >= 2:\n",
    "        text_column = df.columns[1]\n",
    "        print(f\"Using second column '{text_column}' as text data\")\n",
    "    \n",
    "    texts = df[text_column].astype(str).tolist()\n",
    "    \n",
    "    # Build character-level vocabulary\n",
    "    chars = set()\n",
    "    for text in texts:\n",
    "        chars.update(text.lower())\n",
    "    \n",
    "    # Ensure word delimiter is included for decoding/joining\n",
    "    chars.add('|')\n",
    "    \n",
    "    # Create vocabulary with proper CTC blank token\n",
    "    # Order matters: <blank>=0, <unk>=1, <pad>=2\n",
    "    vocab_list = ['<blank>', '<unk>', '<pad>'] + sorted(list(chars))\n",
    "    vocab_dict = {char: i for i, char in enumerate(vocab_list)}\n",
    "    \n",
    "    print(f\"Created vocabulary with {len(vocab_dict)} characters\")\n",
    "    \n",
    "    # Save vocabulary\n",
    "    with open(vocab_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(vocab_dict, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Create tokenizer with proper blank token\n",
    "    tokenizer = Wav2Vec2CTCTokenizer(\n",
    "        vocab_file,\n",
    "        unk_token=\"<unk>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        word_delimiter_token=\"|\"\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Created new tokenizer and saved to {vocab_file}\")\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1d933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# DATA AUGMENTATION FUNCTIONS\n",
    "# ========================================\n",
    "\n",
    "def speed_perturb(wave: np.ndarray, sr: int, factors=(0.9, 1.0, 1.1)) -> np.ndarray:\n",
    "    if not LIBROSA_AVAILABLE:\n",
    "        return wave\n",
    "    f = random.choice(factors)\n",
    "    if f == 1.0:\n",
    "        return wave\n",
    "    return librosa.effects.time_stretch(wave, rate=f)\n",
    "\n",
    "def pitch_shift(wave: np.ndarray, sr: int, n_steps=(-2, 0, 2)) -> np.ndarray:\n",
    "    if not LIBROSA_AVAILABLE:\n",
    "        return wave\n",
    "    step = random.choice(n_steps)\n",
    "    return librosa.effects.pitch_shift(wave, sr=sr, n_steps=step)\n",
    "\n",
    "def add_background_noise(wave: np.ndarray, snr_db_min=5, snr_db_max=20) -> np.ndarray:\n",
    "    rms = np.sqrt(np.mean(wave**2))\n",
    "    if rms == 0:\n",
    "        return wave\n",
    "    snr_db = random.uniform(snr_db_min, snr_db_max)\n",
    "    snr = 10 ** (snr_db / 20.0)\n",
    "    noise_rms = rms / snr\n",
    "    noise = np.random.normal(0, noise_rms, wave.shape)\n",
    "    return wave + noise\n",
    "\n",
    "def spec_augment(features: torch.Tensor, time_mask_param=30, freq_mask_param=13, num_time_masks=2, num_freq_masks=2):\n",
    "    B, T, D = features.shape\n",
    "    for _ in range(num_time_masks):\n",
    "        t = random.randint(0, time_mask_param)\n",
    "        t0 = random.randint(0, max(0, T - t)) if T - t > 0 else 0\n",
    "        features[:, t0:t0+t, :] = 0\n",
    "    for _ in range(num_freq_masks):\n",
    "        f = random.randint(0, freq_mask_param)\n",
    "        f0 = random.randint(0, max(0, D - f)) if D - f > 0 else 0\n",
    "        features[:, :, f0:f0+f] = 0\n",
    "    return features\n",
    "\n",
    "print(\"‚úÖ Data augmentation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e630341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# ADAPTER MODULE\n",
    "# ========================================\n",
    "\n",
    "class Adapter(nn.Module):\n",
    "    def __init__(self, dim, bottleneck=128):\n",
    "        super().__init__()\n",
    "        self.down = nn.Linear(dim, bottleneck)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.up = nn.Linear(bottleneck, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.down(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.up(x)\n",
    "        return x + residual\n",
    "\n",
    "print(\"‚úÖ Adapter module defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec94078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CUSTOM TRANSFORMER CTC HEAD\n",
    "# ========================================\n",
    "\n",
    "class CustomTransformerCTCHead(nn.Module):\n",
    "    def __init__(self, input_dim: int, model_dim: int, num_layers: int, num_heads: int, dropout: float, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.model_dim = model_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Project input to model dimension if needed\n",
    "        if input_dim != model_dim:\n",
    "            self.input_proj = nn.Linear(input_dim, model_dim)\n",
    "        else:\n",
    "            self.input_proj = nn.Identity()\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=model_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Output projection to vocabulary\n",
    "        self.output_proj = nn.Linear(model_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.input_proj(features)\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        logits = self.output_proj(x)\n",
    "        return logits\n",
    "\n",
    "print(\"‚úÖ Custom Transformer CTC Head defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ba58cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# COMBINED MODEL WRAPPER\n",
    "# ========================================\n",
    "\n",
    "class W2V2WithCustomHead(nn.Module):\n",
    "    def __init__(self, pretrained_name: str, processor: Wav2Vec2Processor, tokenizer: Wav2Vec2CTCTokenizer,\n",
    "                 freeze_w2v: bool=True, use_adapters: bool=True, adapter_dim: int=128,\n",
    "                 trans_layers: int=4, trans_dim: int=768, trans_heads: int=8, dropout: float=0.1):\n",
    "        super().__init__()\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = len(tokenizer.get_vocab())\n",
    "        # Pad token ID is 2\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        \n",
    "        # FIX: Get blank token ID by token string, not by attribute.\n",
    "        self.blank_token_id = tokenizer.convert_tokens_to_ids('<blank>') \n",
    "\n",
    "        # Base wav2vec model\n",
    "        print(f\"   Loading Wav2Vec2 from: {pretrained_name} (cache: {os.path.abspath(CACHE_DIR)})\")\n",
    "        # FIX: Removed local_files_only=True to allow downloading if cache is empty\n",
    "        self.wav2vec = Wav2Vec2Model.from_pretrained(\n",
    "            pretrained_name,\n",
    "            cache_dir=CACHE_DIR,\n",
    "        )\n",
    "        self.wav2vec_feature_dim = self.wav2vec.config.hidden_size\n",
    "\n",
    "        if freeze_w2v:\n",
    "            for p in self.wav2vec.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.use_adapters = use_adapters\n",
    "        if use_adapters:\n",
    "            self.adapter = Adapter(self.wav2vec_feature_dim, bottleneck=adapter_dim)\n",
    "        else:\n",
    "            self.adapter = None\n",
    "\n",
    "        self.custom_head = CustomTransformerCTCHead(\n",
    "            input_dim=self.wav2vec_feature_dim, \n",
    "            model_dim=trans_dim, \n",
    "            num_layers=trans_layers, \n",
    "            num_heads=trans_heads, \n",
    "            dropout=dropout, \n",
    "            vocab_size=self.vocab_size\n",
    "        )\n",
    "\n",
    "    def forward(self, input_values: torch.Tensor, attention_mask: Optional[torch.Tensor]=None, \n",
    "            labels: Optional[torch.Tensor]=None):\n",
    "        \n",
    "        outputs = self.wav2vec(input_values, attention_mask=attention_mask)\n",
    "        features = outputs.last_hidden_state\n",
    "        \n",
    "        if self.adapter is not None:\n",
    "            features = self.adapter(features)\n",
    "\n",
    "        logits = self.custom_head(features)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels_copy = labels.clone()\n",
    "            labels_copy[labels_copy == -100] = self.pad_token_id\n",
    "\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            \n",
    "            input_lengths = torch.full(\n",
    "                size=(log_probs.shape[0],), \n",
    "                fill_value=log_probs.shape[1], \n",
    "                dtype=torch.long, \n",
    "                device=log_probs.device\n",
    "            ).clamp(min=1)\n",
    "            \n",
    "            target_lengths = (labels_copy != self.pad_token_id).sum(dim=-1).clamp(min=1)\n",
    "            \n",
    "            targets = labels_copy[labels_copy != self.pad_token_id]\n",
    "            \n",
    "            if targets.numel() > 0 and target_lengths.sum() > 0:\n",
    "                \n",
    "                ctc_loss_fn = nn.CTCLoss(\n",
    "                    blank=self.blank_token_id, # Should be 0\n",
    "                    zero_infinity=True, \n",
    "                    reduction='mean'\n",
    "                )\n",
    "                \n",
    "                log_probs_t = log_probs.transpose(0, 1) # CTC expects T x B x D\n",
    "\n",
    "                try:\n",
    "                    loss = ctc_loss_fn(log_probs_t, targets, input_lengths, target_lengths)\n",
    "                    \n",
    "                    if torch.isnan(loss):\n",
    "                        print(\"Warning: CTC loss NaN. Replacing with 0.0 loss (from graph).\")\n",
    "                        # FIX 1: Replace leaf tensor with graph-attached 0.0 loss\n",
    "                        loss = logits.sum() * 0.0\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"CTC loss computation failed: {e}. Replacing with 0.0 loss (from graph).\")\n",
    "                    # FIX 2: Replace leaf tensor with graph-attached 0.0 loss\n",
    "                    loss = logits.sum() * 0.0\n",
    "            else:\n",
    "                # Fallback for empty or problematic batch\n",
    "                print(\"Warning: Empty batch. Replacing with 0.0 loss (from graph).\")\n",
    "                # FIX 3: Replace leaf tensor with graph-attached 0.0 loss\n",
    "                loss = logits.sum() * 0.0\n",
    "\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits': logits,\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ W2V2WithCustomHead model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111eea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# DATA COLLATOR\n",
    "# ========================================\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithAugment:\n",
    "    processor: Wav2Vec2Processor\n",
    "    tokenizer: Wav2Vec2CTCTokenizer\n",
    "    sample_rate: int = SAMPLE_RATE\n",
    "    padding: bool = True\n",
    "    apply_spec_augment: bool = True\n",
    "    apply_audio_aug: bool = True\n",
    "    min_audio_length: int = 1600 # Used for reference, actual check is more lenient\n",
    "    \n",
    "    # Get pad token ID (2) and unk token ID (1)\n",
    "    pad_token_id: int = field(init=False)\n",
    "    unk_token_id: int = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.pad_token_id = self.tokenizer.pad_token_id\n",
    "        self.unk_token_id = self.tokenizer.unk_token_id\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        input_values = []\n",
    "        attention_masks = []\n",
    "        labels = []\n",
    "\n",
    "        def _load_audio(path: str):\n",
    "            if LIBROSA_AVAILABLE:\n",
    "                # librosa automatically handles resampling and mono conversion\n",
    "                arr, sr = librosa.load(path, sr=self.sample_rate)\n",
    "                return arr, sr\n",
    "            if TORCHAUDIO_AVAILABLE:\n",
    "                import torchaudio\n",
    "                wave, sr = torchaudio.load(path)\n",
    "                # mixdown to mono\n",
    "                if wave.dim() == 2 and wave.size(0) > 1:\n",
    "                    wave = wave.mean(dim=0)\n",
    "                else:\n",
    "                    wave = wave.squeeze(0)\n",
    "                wave = wave.numpy()\n",
    "                \n",
    "                # Resample if needed\n",
    "                if sr and sr != self.sample_rate:\n",
    "                    try:\n",
    "                        arr_t = torch.tensor(wave, dtype=torch.float32)\n",
    "                        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)\n",
    "                        wave = resampler(arr_t.unsqueeze(0)).squeeze(0).numpy()\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                return wave, sr\n",
    "            return None, None\n",
    "\n",
    "        for f in features:\n",
    "            try:\n",
    "                audio_path = f.get(\"audio_path\", \"\")\n",
    "                if not audio_path or not os.path.exists(audio_path):\n",
    "                    continue\n",
    "\n",
    "                arr, sr = _load_audio(audio_path)\n",
    "                if arr is None:\n",
    "                    continue\n",
    "\n",
    "                if isinstance(arr, np.ndarray) and arr.ndim > 1:\n",
    "                    arr = arr.flatten()\n",
    "\n",
    "                # Minimum length check\n",
    "                if len(arr) < int(1.0 * self.sample_rate):\n",
    "                    continue\n",
    "\n",
    "                # Optional audio augmentation\n",
    "                if self.apply_audio_aug:\n",
    "                    if LIBROSA_AVAILABLE:\n",
    "                        if random.random() < 0.3: arr = speed_perturb(arr, self.sample_rate)\n",
    "                        if random.random() < 0.3: arr = pitch_shift(arr, self.sample_rate)\n",
    "                    if random.random() < 0.3: arr = add_background_noise(arr)\n",
    "\n",
    "                # Process with wav2vec2\n",
    "                inputs = self.processor(\n",
    "                    arr,\n",
    "                    sampling_rate=self.sample_rate,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                \n",
    "                input_tensor = inputs.input_values.squeeze(0)\n",
    "                attn_mask = inputs.attention_mask.squeeze(0) if hasattr(inputs, 'attention_mask') else torch.ones_like(input_tensor, dtype=torch.long)\n",
    "                if input_tensor.dim() == 0:\n",
    "                    input_tensor = input_tensor.unsqueeze(0)\n",
    "                    attn_mask = torch.ones_like(input_tensor, dtype=torch.long)\n",
    "                input_values.append(input_tensor)\n",
    "                attention_masks.append(attn_mask)\n",
    "\n",
    "                # Process text\n",
    "                text = str(f.get(\"text\", \"\")).lower().strip()\n",
    "                if not text:\n",
    "                    continue\n",
    "\n",
    "                # Character-level tokenization\n",
    "                token_ids = []\n",
    "                vocab = self.tokenizer.get_vocab()\n",
    "                \n",
    "                for char in text:\n",
    "                    if char in vocab:\n",
    "                        token_ids.append(vocab[char])\n",
    "                    else:\n",
    "                        token_ids.append(self.unk_token_id)\n",
    "\n",
    "                if token_ids:\n",
    "                    labels.append(torch.tensor(token_ids, dtype=torch.long))\n",
    "\n",
    "            except Exception as e:\n",
    "                # print(f\"Error processing item: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Handle empty batch\n",
    "        if len(input_values) == 0 or len(labels) == 0:\n",
    "            dummy_audio = torch.zeros(self.sample_rate // 2)\n",
    "            dummy_mask = torch.ones_like(dummy_audio, dtype=torch.long)\n",
    "            # Dummy label must be padded with -100\n",
    "            dummy_labels = torch.tensor([self.pad_token_id], dtype=torch.long)\n",
    "            dummy_labels[dummy_labels == self.pad_token_id] = -100\n",
    "            \n",
    "            return {\n",
    "                \"input_values\": dummy_audio.unsqueeze(0),\n",
    "                \"attention_mask\": dummy_mask.unsqueeze(0),\n",
    "                \"labels\": dummy_labels.unsqueeze(0),\n",
    "            }\n",
    "\n",
    "        # Pad sequences\n",
    "        input_values = nn.utils.rnn.pad_sequence(input_values, batch_first=True, padding_value=0.0)\n",
    "        attention_masks = nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "        \n",
    "        # FIX: Pad with the pad_token_id (2), then replace with -100 for CTC loss compatibility in Trainer\n",
    "        labels = nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=self.pad_token_id)\n",
    "        labels[labels == self.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_values\": input_values,\n",
    "            \"attention_mask\": attention_masks,\n",
    "            \"labels\": labels, # <- Padded with -100\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ DataCollatorCTCWithAugment defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61bbdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# DATASET PREPARATION\n",
    "# ========================================\n",
    "\n",
    "def prepare_dataset(tsv_path: str, audio_base: str) -> Dataset:\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(tsv_path, sep='\\t', low_memory=False)\n",
    "    \n",
    "    # Handle non-standard column names\n",
    "    columns = df.columns.tolist()\n",
    "    if len(columns) >= 2:\n",
    "        df = df.rename(columns={columns[0]: 'path', columns[1]: 'text'})\n",
    "    \n",
    "    if 'path' not in df.columns:\n",
    "        raise ValueError('TSV must have at least 2 columns: audio path and text')\n",
    "\n",
    "    # Remove rows with NaN values in path or text columns\n",
    "    df = df.dropna(subset=['path', 'text'])\n",
    "    \n",
    "    # Convert path and text to strings\n",
    "    df['path'] = df['path'].astype(str)\n",
    "    df['text'] = df['text'].astype(str)\n",
    "    \n",
    "    # Create full audio paths\n",
    "    def get_full_path(p):\n",
    "        if not p or p == 'nan':\n",
    "            return None\n",
    "            \n",
    "        if os.path.isabs(p):\n",
    "            return p\n",
    "        else:\n",
    "            base_path = os.path.join(audio_base, p)\n",
    "            if os.path.exists(base_path):\n",
    "                return base_path\n",
    "            for ext in ['.wav', '.mp3', '.flac', '.ogg']:\n",
    "                if os.path.exists(base_path + ext):\n",
    "                    return base_path + ext\n",
    "            return None # Return None if file not found with common extensions\n",
    "    \n",
    "    df['audio_path'] = df['path'].apply(get_full_path)\n",
    "    \n",
    "    # Remove rows where audio_path is None\n",
    "    df = df[df['audio_path'].notna()]\n",
    "    \n",
    "    # Filter valid files\n",
    "    initial_count = len(df)\n",
    "    df = df[df['audio_path'].apply(os.path.exists)]\n",
    "    final_count = len(df)\n",
    "    \n",
    "    print(f\"Filtered dataset: {initial_count} -> {final_count} samples\")\n",
    "    \n",
    "    if final_count == 0:\n",
    "        raise ValueError(\"No valid audio files found!\")\n",
    "    \n",
    "    dataset = Dataset.from_pandas(df[['audio_path', 'text']])\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "print(\"‚úÖ prepare_dataset function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1631f5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# HUGGINGFACE WRAPPER\n",
    "# ========================================\n",
    "\n",
    "@dataclass\n",
    "class ModelOutput:\n",
    "    loss: Optional[torch.Tensor] = None\n",
    "    logits: Optional[torch.Tensor] = None\n",
    "\n",
    "class HFWrapperModel(nn.Module):\n",
    "    def __init__(self, inner_model):\n",
    "        super().__init__()\n",
    "        self.inner = inner_model\n",
    "    \n",
    "    def forward(self, input_values=None, labels=None, attention_mask=None, **kwargs):\n",
    "        outputs = self.inner(input_values=input_values, attention_mask=attention_mask, labels=labels)\n",
    "        # Return a plain dict for Trainer compatibility\n",
    "        return {\"loss\": outputs['loss'], \"logits\": outputs['logits']}\n",
    "\n",
    "print(\"‚úÖ HFWrapperModel defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2451e9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# METRICS\n",
    "# ========================================\n",
    "\n",
    "wer_metric = load('wer')\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits = pred.predictions\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    \n",
    "    if logits is None:\n",
    "        return {\"wer\": 1.0}\n",
    "        \n",
    "    pred_ids = np.argmax(logits, axis=-1)\n",
    "    # Label IDs were padded with -100 by the DataCollator\n",
    "    label_ids = pred.label_ids \n",
    "    \n",
    "    pred_texts = []\n",
    "    ref_texts = []\n",
    "    \n",
    "    # Get tokenizer from global scope (assumes it's defined, which it is in __main__)\n",
    "    global tokenizer \n",
    "    \n",
    "    # CTC decoding for predictions\n",
    "    for i in range(pred_ids.shape[0]):\n",
    "        # Decode predictions (remove repeats, blank=0, and padding)\n",
    "        pred_seq = []\n",
    "        prev_id = None\n",
    "        for token_id in pred_ids[i]:\n",
    "            # Token ID 0 is '<blank>'\n",
    "            if token_id != prev_id and token_id != 0: \n",
    "                pred_seq.append(int(token_id))\n",
    "            prev_id = int(token_id)\n",
    "        \n",
    "        if len(pred_seq) == 0:\n",
    "            pred_texts.append(\"\")\n",
    "        else:\n",
    "            # Use tokenizer to convert IDs to text\n",
    "            pred_text = tokenizer.decode(pred_seq, skip_special_tokens=True)\n",
    "            pred_texts.append(pred_text)\n",
    "        \n",
    "        # Decode references (mask -100)\n",
    "        ref_seq = [int(t) for t in label_ids[i] if t != -100] \n",
    "        \n",
    "        if len(ref_seq) == 0:\n",
    "            ref_texts.append(\"\")\n",
    "        else:\n",
    "            ref_text = tokenizer.decode(ref_seq, skip_special_tokens=True)\n",
    "            ref_texts.append(ref_text)\n",
    "    \n",
    "    if not pred_texts or not ref_texts:\n",
    "        return {\"wer\": 1.0}\n",
    "    \n",
    "    wer_score = wer_metric.compute(predictions=pred_texts, references=ref_texts)\n",
    "    return {\"wer\": wer_score}\n",
    "\n",
    "print(\"‚úÖ Metrics function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc3644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# MAIN EXECUTION\n",
    "# ========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üöÄ STARTING WAV2VEC2 TRAINING\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    SEED = 42\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "    \n",
    "    # Reduce HF verbosity a bit\n",
    "    try:\n",
    "        hf_logging.set_verbosity_warning()\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(\"üìù Loading/Creating tokenizer...\")\n",
    "    tokenizer = load_or_create_tokenizer(TSV_PATH, TOKENIZER_DIR)\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "        feature_size=1, \n",
    "        sampling_rate=SAMPLE_RATE, \n",
    "        padding_value=0.0, \n",
    "        do_normalize=True, \n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "    print(f\"‚úÖ Tokenizer vocabulary size: {len(tokenizer.get_vocab())}\")\n",
    "    \n",
    "    # Prepare dataset\n",
    "    print(\"\\nüìÇ Loading dataset...\")\n",
    "    dataset = prepare_dataset(TSV_PATH, AUDIO_BASE)\n",
    "    print(f\"‚úÖ Dataset loaded: {len(dataset)} samples\")\n",
    "    \n",
    "    # Split dataset\n",
    "    dataset = dataset.train_test_split(test_size=0.1, seed=SEED)\n",
    "    train_dataset = dataset['train']\n",
    "    eval_dataset = dataset['test']\n",
    "    print(f\"‚úÖ Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"‚úÖ Eval dataset size: {len(eval_dataset)}\")\n",
    "    \n",
    "    # Create model\n",
    "    print(\"\\nüèóÔ∏è Creating model...\")\n",
    "    try:\n",
    "        model = W2V2WithCustomHead(\n",
    "            pretrained_name=PRETRAINED_W2V,\n",
    "            processor=processor,\n",
    "            tokenizer=tokenizer,\n",
    "            freeze_w2v=FREEZE_W2V,\n",
    "            use_adapters=USE_ADAPTERS,\n",
    "            adapter_dim=ADAPTER_BOTTLENECK,\n",
    "            trans_layers=CUSTOM_TRANSFORMER_LAYERS,\n",
    "            trans_dim=CUSTOM_TRANSFORMER_DIM,\n",
    "            trans_heads=CUSTOM_TRANSFORMER_HEADS,\n",
    "            dropout=DROPOUT\n",
    "        )\n",
    "        \n",
    "        model.to(DEVICE)\n",
    "        print(f\"‚úÖ Model loaded on {DEVICE}\")\n",
    "        \n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        exit(1)\n",
    "    \n",
    "    # Create data collator\n",
    "    print(\"\\nüì¶ Creating data collator...\")\n",
    "    data_collator = DataCollatorCTCWithAugment(\n",
    "        processor=processor,\n",
    "        tokenizer=tokenizer,\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        apply_spec_augment=USE_SPEC_AUG,\n",
    "        apply_audio_aug=USE_AUDIO_AUG\n",
    "    )\n",
    "    print(\"‚úÖ Data collator created\")\n",
    "    \n",
    "    # Wrap model for HuggingFace\n",
    "    wrapped_model = HFWrapperModel(model)\n",
    "    print(\"‚úÖ Model wrapped for HuggingFace Trainer\")\n",
    "    \n",
    "    # Training arguments\n",
    "    print(\"\\n‚öôÔ∏è Configuring training arguments...\")\n",
    "    \n",
    "    # This block should be correct now, using 'eval_strategy'\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        learning_rate=LR,\n",
    "        warmup_steps=500,\n",
    "        logging_steps=50,\n",
    "        eval_steps=500,\n",
    "        save_steps=500,\n",
    "        \n",
    "        eval_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        \n",
    "        save_total_limit=3,\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_pin_memory=False,\n",
    "        group_by_length=False,\n",
    "        # fp16=torch.cuda.is_available(),\n",
    "        fp16=False,\n",
    "        push_to_hub=False,\n",
    "        report_to=None,\n",
    "        max_grad_norm=1.0,\n",
    "        dataloader_num_workers=0,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"wer\",\n",
    "        greater_is_better=False,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    print(\"‚úÖ Training arguments configured\")\n",
    "    print(f\"   Effective batch size: {BATCH_SIZE * 4}\")\n",
    "    print(f\"   FP16 enabled: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    # Create trainer\n",
    "    print(\"\\nüë®‚Äçüè´ Creating trainer...\")\n",
    "    trainer = Trainer(\n",
    "        model=wrapped_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    print(\"‚úÖ Trainer created\")\n",
    "    \n",
    "    # Start training\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üèãÔ∏è STARTING TRAINING\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"‚úÖ TRAINING COMPLETED\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"üìä Running final evaluation...\")\n",
    "    metrics = trainer.evaluate()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL METRICS\")\n",
    "    print(\"=\"*50)\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    # Save model\n",
    "    print(\"\\nüíæ Saving model...\")\n",
    "    trainer.save_model()\n",
    "    processor.save_pretrained(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    \n",
    "    print(f\"‚úÖ Model saved to {OUTPUT_DIR}\")\n",
    "    print(f\"‚úÖ Processor saved to {OUTPUT_DIR}\")\n",
    "    print(f\"‚úÖ Tokenizer saved to {OUTPUT_DIR}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üéâ ALL DONE!\")\n",
    "    print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Major",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
