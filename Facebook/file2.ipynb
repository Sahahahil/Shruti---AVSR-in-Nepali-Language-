{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fc22f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, subprocess, cv2\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import yt_dlp\n",
    "import mediapipe as mp\n",
    "from aeneas.executetask import ExecuteTask\n",
    "from aeneas.task import Task\n",
    "from faster_whisper import WhisperModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7004d9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- CONFIG ----------\n",
    "DATA_ROOT = \"dataset-1a\"\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a369d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-10-14 17:48:07.721] [ctranslate2] [thread 299079] [warning] The compute type inferred from the saved model is float16, but the target device or backend do not support efficient float16 computation. The model weights have been automatically converted to use the float32 compute type instead.\n"
     ]
    }
   ],
   "source": [
    "# Load Whisper once (faster)\n",
    "DEVICE = \"cuda\" if cv2.cuda.getCudaEnabledDeviceCount() else \"cpu\"\n",
    "whisper_model = WhisperModel(\"medium\", device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7340725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- DOWNLOAD ----------\n",
    "def download_fb_video(url, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    ydl_opts = {\n",
    "        \"outtmpl\": os.path.join(out_dir, \"%(id)s.%(ext)s\"),\n",
    "        \"format\": \"best[ext=mp4]\",\n",
    "        \"quiet\": True,\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(url, download=True)\n",
    "    return os.path.join(out_dir, f\"{info['id']}.mp4\"), info[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62bfed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- AUDIO ----------\n",
    "def extract_audio(video_path, out_dir, start_time=0, end_time=None):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    base = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    out_path = os.path.join(out_dir, f\"{base}.wav\")\n",
    "\n",
    "    cmd = [\n",
    "        \"ffmpeg\", \"-y\", \"-i\", video_path,\n",
    "        \"-ac\", \"1\", \"-ar\", \"16000\"\n",
    "    ]\n",
    "    if start_time or end_time:\n",
    "        cmd += [\"-ss\", str(start_time)]\n",
    "        if end_time:\n",
    "            cmd += [\"-to\", str(end_time)]\n",
    "    cmd += [out_path]\n",
    "\n",
    "    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eedec8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- AUDIO VALIDATION ----------\n",
    "def validate_audio(audio_path, min_duration=1.0):\n",
    "    \"\"\"\n",
    "    Validates audio file and returns detailed info.\n",
    "    \"\"\"\n",
    "    import wave\n",
    "    try:\n",
    "        with wave.open(audio_path, 'rb') as wav_file:\n",
    "            n_channels = wav_file.getnchannels()\n",
    "            sample_rate = wav_file.getframerate()\n",
    "            n_frames = wav_file.getnframes()\n",
    "            duration = n_frames / sample_rate\n",
    "            \n",
    "            info = {\n",
    "                \"channels\": n_channels,\n",
    "                \"sample_rate\": sample_rate,\n",
    "                \"frames\": n_frames,\n",
    "                \"duration\": duration,\n",
    "                \"valid\": duration >= min_duration\n",
    "            }\n",
    "            \n",
    "            print(f\"  üìä Audio Info:\")\n",
    "            print(f\"     Duration: {duration:.2f}s | Sample Rate: {sample_rate}Hz | Channels: {n_channels}\")\n",
    "            \n",
    "            if not info[\"valid\"]:\n",
    "                print(f\"     ‚ö†Ô∏è WARNING: Audio duration ({duration:.2f}s) < minimum ({min_duration}s)\")\n",
    "            \n",
    "            return info\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error reading audio: {e}\")\n",
    "        return {\"valid\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecac2ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- LIP FRAMES ----------\n",
    "def extract_lip_frames(video_path, out_dir, start_time=0, end_time=None, debug=False, verbose=True):\n",
    "    \"\"\"Extract lip-region frames between start_time and end_time.\"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = total_frames / fps if fps else 0\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"  üìπ Video Info:\")\n",
    "        print(f\"     FPS: {fps:.2f} | Total Frames: {total_frames} | Duration: {duration:.2f}s\")\n",
    "\n",
    "    if end_time is None or end_time > duration:\n",
    "        end_time = duration\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_POS_MSEC, start_time * 1000)\n",
    "    frame_idx = 0\n",
    "    detected = 0\n",
    "    skipped = 0\n",
    "\n",
    "    with mp.solutions.face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.3) as face_det, \\\n",
    "         mp.solutions.face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1,\n",
    "                                         refine_landmarks=True,\n",
    "                                         min_detection_confidence=0.3,\n",
    "                                         min_tracking_confidence=0.3) as mesh:\n",
    "\n",
    "        while True:\n",
    "            current_time = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0\n",
    "            if current_time > end_time:\n",
    "                break\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Resize large frames\n",
    "            h, w = frame.shape[:2]\n",
    "            if w > 960:\n",
    "                scale = 960 / w\n",
    "                frame = cv2.resize(frame, None, fx=scale, fy=scale)\n",
    "                h, w = frame.shape[:2]\n",
    "\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            detections = face_det.process(rgb).detections\n",
    "\n",
    "            if detections:\n",
    "                for det in detections:\n",
    "                    bbox = det.location_data.relative_bounding_box\n",
    "                    x1 = int(bbox.xmin * w)\n",
    "                    y1 = int(bbox.ymin * h)\n",
    "                    bw = int(bbox.width * w)\n",
    "                    bh = int(bbox.height * h)\n",
    "\n",
    "                    # Focus on lower face (lips)\n",
    "                    y1 = y1 + int(bh * 0.4)\n",
    "                    bh = int(bh * 0.6)\n",
    "                    x1 = max(x1 - 10, 0)\n",
    "                    y1 = max(y1 - 10, 0)\n",
    "                    x2 = min(x1 + bw + 20, w)\n",
    "                    y2 = min(y1 + bh + 20, h)\n",
    "\n",
    "                    crop = frame[y1:y2, x1:x2]\n",
    "                    if crop.size > 0:\n",
    "                        cv2.imwrite(os.path.join(out_dir, f\"{frame_idx:06d}.jpg\"), crop)\n",
    "                        detected += 1\n",
    "\n",
    "                        if debug:\n",
    "                            dbg = frame.copy()\n",
    "                            cv2.rectangle(dbg, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                            cv2.imshow(\"Lip detection\", dbg)\n",
    "                            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                                break\n",
    "            else:\n",
    "                skipped += 1\n",
    "            \n",
    "            frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    if debug:\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"  üé¨ Frame Extraction:\")\n",
    "        print(f\"     Detected frames: {detected} | Skipped frames: {skipped} | Total processed: {frame_idx}\")\n",
    "        if detected == 0:\n",
    "            print(f\"     ‚ö†Ô∏è WARNING: No faces detected in range {start_time}s - {end_time}s\")\n",
    "        else:\n",
    "            print(f\"     ‚úÖ Saved {detected} lip frames to {out_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0def55f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- ALIGNMENT ----------\n",
    "def align_audio_text(audio_path, text, out_path, whisper_model=None, debug=True):\n",
    "    \"\"\"\n",
    "    Aligns audio with word-level timestamps and writes TSV:\n",
    "    start_time, end_time, word\n",
    "    \n",
    "    Args:\n",
    "        audio_path: Path to audio file\n",
    "        text: Text (not used in transcription, kept for compatibility)\n",
    "        out_path: Output TSV path\n",
    "        whisper_model: Pre-loaded WhisperModel instance\n",
    "        debug: Enable detailed logging\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"\\n  üîÑ Transcribing: {audio_path}\")\n",
    "        \n",
    "        # Validate audio first\n",
    "        audio_info = validate_audio(audio_path, min_duration=0.5)\n",
    "        if not audio_info[\"valid\"]:\n",
    "            print(f\"  ‚ö†Ô∏è Audio validation failed, attempting transcription anyway...\")\n",
    "        \n",
    "        if whisper_model is None:\n",
    "            print(\"  ‚ö†Ô∏è No model provided, creating new instance\")\n",
    "            whisper_model = WhisperModel(\"medium\", device=DEVICE)\n",
    "        \n",
    "        # Transcribe with word-level timestamps\n",
    "        segments, info = whisper_model.transcribe(\n",
    "            audio_path, \n",
    "            beam_size=5, \n",
    "            word_timestamps=True,\n",
    "            language=None  # Auto-detect; set to \"ne\" for Nepali-only\n",
    "        )\n",
    "        \n",
    "        segments_list = list(segments)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"  üìã Whisper Metadata:\")\n",
    "            print(f\"     Detected Language: {info.language} (confidence: {info.language_probability:.2%})\")\n",
    "            print(f\"     Number of segments: {len(segments_list)}\")\n",
    "        \n",
    "        if not segments_list:\n",
    "            print(f\"  ‚ö†Ô∏è No transcription results for {audio_path}\")\n",
    "            with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"# Empty transcription\\n\")\n",
    "            return False\n",
    "        \n",
    "        # Extract word-level timestamps\n",
    "        word_count = 0\n",
    "        segment_details = []\n",
    "        \n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for seg_idx, seg in enumerate(segments_list):\n",
    "                seg_text = seg.text.strip()\n",
    "                seg_info = f\"Segment {seg_idx}: {seg.start:.2f}s - {seg.end:.2f}s | {seg_text}\"\n",
    "                segment_details.append(seg_info)\n",
    "                \n",
    "                if seg.words:\n",
    "                    for w in seg.words:\n",
    "                        word_text = w.word.strip()\n",
    "                        # Filter out very short or noise words\n",
    "                        if word_text and len(word_text) > 0:\n",
    "                            f.write(f\"{w.start:.2f}\\t{w.end:.2f}\\t{word_text}\\n\")\n",
    "                            word_count += 1\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"  üìù Segment Details:\")\n",
    "            for detail in segment_details:\n",
    "                print(f\"     {detail}\")\n",
    "        \n",
    "        if word_count > 0:\n",
    "            print(f\"  ‚úÖ Extracted {word_count} words with timestamps\")\n",
    "            print(f\"     Output: {out_path}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è No valid words extracted (empty segments?)\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error in align_audio_text: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        # Create error TSV\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"# Error: {str(e)}\\n\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d51c7010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- PROCESS ONE SAMPLE ----------\n",
    "def process_sample(url, title, start_time, end_time, whisper_model=None, verbose=True):\n",
    "    try:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üì• PROCESSING VIDEO\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"URL: {url}\")\n",
    "        print(f\"Title: {title}\")\n",
    "        print(f\"Time Range: {start_time}s - {end_time}s\")\n",
    "        \n",
    "        video_path, vid_id = download_fb_video(url, os.path.join(DATA_ROOT, \"videos\"))\n",
    "        print(f\"‚úÖ Downloaded video: {vid_id}\")\n",
    "        \n",
    "        # Extract audio\n",
    "        audio_path = extract_audio(video_path, os.path.join(DATA_ROOT, \"audio\"), start_time, end_time)\n",
    "        print(f\"‚úÖ Extracted audio: {audio_path}\")\n",
    "        \n",
    "        # Extract frames with verbose output\n",
    "        frame_dir = os.path.join(DATA_ROOT, \"frames\", vid_id)\n",
    "        extract_lip_frames(video_path, frame_dir, start_time, end_time, verbose=True)\n",
    "        \n",
    "        # Align audio\n",
    "        align_path = os.path.join(DATA_ROOT, \"alignments\", f\"{vid_id}.tsv\")\n",
    "        os.makedirs(os.path.dirname(align_path), exist_ok=True)\n",
    "        align_success = align_audio_text(audio_path, title, align_path, whisper_model=whisper_model, debug=True)\n",
    "        \n",
    "        # Verify TSV was created\n",
    "        if os.path.exists(align_path):\n",
    "            file_size = os.path.getsize(align_path)\n",
    "            with open(align_path, 'r', encoding='utf-8') as f:\n",
    "                line_count = len(f.readlines())\n",
    "            print(f\"\\n  üìÑ TSV File:\")\n",
    "            print(f\"     Path: {align_path}\")\n",
    "            print(f\"     Size: {file_size} bytes | Lines: {line_count}\")\n",
    "            if line_count == 0:\n",
    "                print(f\"     ‚ö†Ô∏è WARNING: TSV is empty!\")\n",
    "        \n",
    "        # Save metadata\n",
    "        meta = {\n",
    "            \"id\": vid_id,\n",
    "            \"url\": url,\n",
    "            \"text\": title,\n",
    "            \"video\": video_path,\n",
    "            \"audio\": audio_path,\n",
    "            \"frames_dir\": frame_dir,\n",
    "            \"alignment\": align_path,\n",
    "            \"start_time\": start_time,\n",
    "            \"end_time\": end_time,\n",
    "            \"transcription_success\": align_success,\n",
    "        }\n",
    "        meta_path = os.path.join(DATA_ROOT, f\"{vid_id}.json\")\n",
    "        with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(meta, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"‚úÖ Saved metadata: {meta_path}\")\n",
    "        \n",
    "        print(f\"{'='*70}\\n\")\n",
    "        return vid_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"‚ùå ERROR PROCESSING: {url}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e560c618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "# DATASET BUILD STARTING\n",
      "# Device: cpu | Whisper Model: medium\n",
      "######################################################################\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üì• PROCESSING VIDEO\n",
      "======================================================================\n",
      "URL: https://www.facebook.com/share/v/1CdiEnT1xw/\n",
      "Title: ‡§µ‡§ø‡§∞‡§æ‡§ü‡§®‡§ó‡§∞‡§ï‡•ã ‡§ú‡§ø‡§≤‡•ç‡§≤‡§æ ‡§ï‡§æ‡§∞‡§æ‡§ó‡§æ‡§∞ ‡§Æ‡•ã‡§∞‡§ô‡§Æ‡§æ ‡§ï‡•à‡§¶‡§¨‡§®‡•ç‡§¶‡•Ä ‡§ú‡•Ä‡§µ‡§® ‡§¨‡§ø‡§§‡§æ‡§á‡§∞‡§π‡•á‡§ï‡§æ ‡§è‡§ï‡§ú‡§®‡§æ‡§ï‡•ã ‡§Ü‡§ú ‡§¶‡§ø‡§â‡§Å‡§∏‡•ã ‡§Æ‡•É‡§§‡•ç‡§Ø‡•Å ‡§≠‡§è‡§ï‡•ã ‡§õ\n",
      "Time Range: 0.0s - 6.0s\n",
      "\n",
      "======================================================================\n",
      "üì• PROCESSING VIDEO\n",
      "======================================================================\n",
      "URL: https://www.facebook.com/share/v/17YKEbmgt8/\n",
      "Title: ‡§µ‡§ø‡§∞‡§æ‡§ü‡§®‡§ó‡§∞‡§ï‡•ã ‡§µ‡§°‡§æ‡§®‡§Æ‡•ç‡§¨‡§∞ ‡•ß‡•¨ ‡§¨‡§∞‡•ç‡§Æ‡§æ‡§ü‡•ã‡§≤‡§ï‡§æ ‡§¨‡§æ‡§∏‡§ø‡§®‡•ç‡§¶‡§æ ‡§∏‡§ø‡§®‡•ã ‡§§‡§•‡§æ ‡§´‡•ã‡§π‡•ã‡§∞‡§ï‡•ã ‡§¶‡•Å‡§∞‡•ç‡§ó‡§®‡•ç‡§ß‡§ï‡§æ ‡§ï‡§æ‡§∞‡§£ ‡§™‡•Ä‡§°‡§ø‡§§ ‡§≠‡§è‡§ï‡§æ ‡§õ‡§® ‡§´‡•ã‡§π‡•ã‡§∞‡§ï‡§æ ‡§ï‡§æ‡§∞‡§£ ‡§â‡§§‡•ç‡§∏‡§∞‡•ç‡§ú‡§® ‡§π‡•Å‡§®‡•á ‡§π‡§æ‡§®‡§ø‡§ï‡§æ‡§∞‡§£ ‡§ó‡•ç‡§Ø‡§æ‡§Å‡§∏ ‡§∞ ‡§¶‡•Å‡§∞‡•ç‡§ó‡§®‡•ç‡§ß‡§ï‡§æ ‡§ï‡§æ‡§∞‡§£ ‡§∏‡•ã ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡§ï‡§æ ‡§¨‡§æ‡§∏‡§ø‡§®‡•ç‡§¶‡§æ ‡§ò‡§∞‡§ï‡•ã‡§†‡§æ‡§Æ‡•à‡§Ç ‡§Æ‡§æ‡§∏‡•ç‡§ï ‡§≤‡§ó‡§æ‡§è‡§∞ ‡§¨‡§∏‡•ç‡§®‡•Å‡§™‡§∞‡•ç‡§®‡•á ‡§Ö‡§µ‡§∏‡•ç‡§•‡§æ ‡§∏‡•É‡§ú‡§®‡§æ ‡§≠‡§è‡§ï‡•ã ‡§õ\n",
      "Time Range: 0.0s - 17.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Downloaded video: 781989364756468\n",
      "‚úÖ Downloaded video: 2022332995220352\n",
      "‚úÖ Extracted audio: dataset-1a/audio/781989364756468.wav\n",
      "‚úÖ Extracted audio: dataset-1a/audio/2022332995220352.wav\n",
      "  üìπ Video Info:\n",
      "     FPS: 29.97 | Total Frames: 6375 | Duration: 212.71s\n",
      "  üìπ Video Info:\n",
      "     FPS: 29.97 | Total Frames: 1882 | Duration: 62.80s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1760443399.353688  300170 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1760443399.356081  300426 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.2.4-arch1.2), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "I0000 00:00:1760443399.358271  300169 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1760443399.359432  300436 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.2.4-arch1.2), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "I0000 00:00:1760443399.397041  300170 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1760443399.398811  300447 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.2.4-arch1.2), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "I0000 00:00:1760443399.412564  300169 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "I0000 00:00:1760443399.415118  300458 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.2.4-arch1.2), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1760443399.423630  300328 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760443399.423988  300429 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760443399.432754  300439 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760443399.432754  300454 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760443399.469854  300441 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1760443399.470835  300452 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/home/sahil_duwal/MajorProject/Major/lib/python3.10/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üé¨ Frame Extraction:\n",
      "     Detected frames: 181 | Skipped frames: 0 | Total processed: 181\n",
      "     ‚úÖ Saved 181 lip frames to dataset-1a/frames/781989364756468\n",
      "\n",
      "  üîÑ Transcribing: dataset-1a/audio/781989364756468.wav\n",
      "  üìä Audio Info:\n",
      "     Duration: 6.00s | Sample Rate: 16000Hz | Channels: 1\n",
      "  üé¨ Frame Extraction:\n",
      "     Detected frames: 511 | Skipped frames: 0 | Total processed: 511\n",
      "     ‚úÖ Saved 511 lip frames to dataset-1a/frames/2022332995220352\n",
      "\n",
      "  üîÑ Transcribing: dataset-1a/audio/2022332995220352.wav\n",
      "  üìä Audio Info:\n",
      "     Duration: 17.00s | Sample Rate: 16000Hz | Channels: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [07:04<07:04, 424.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üìã Whisper Metadata:\n",
      "     Detected Language: ne (confidence: 79.24%)\n",
      "     Number of segments: 1\n",
      "  üìù Segment Details:\n",
      "     Segment 0: 0.00s - 5.98s | ‡§¨‡§ø‡§∞‡§æ‡§§‡§®‡§ó‡§∞ ‡§ï‡•ã ‡§ú‡§ø‡§≤‡•ç‡§≤‡§æ ‡§ï‡§æ‡§∞‡§æ‡§ó‡§æ‡§∞ ‡§Æ‡•É‡§∞‡•ç‡§≤‡§ó ‡§Æ‡§æ ‡§ï‡§à‡§¶‡•Ä‡§¨‡§£ ‡§¶‡•á ‡§ú‡§ø‡§¨‡•ç‡§£ ‡§¨‡§ø‡§§‡§æ‡§∞‡•Ä ‡§π‡•á‡§ï‡§æ ‡§è‡§ï ‡§ú‡§®‡§æ ‡§ï‡•ã ‡§Ü‡§ú‡•ã ‡§¶‡§ø‡§µ‡•ç‡§∂‡§Æ ‡§∞‡•á ‡§§‡§ø‡§Ø‡•Å‡§´‡§≤‡•ã‡§ó‡•Å.\n",
      "  ‚úÖ Extracted 18 words with timestamps\n",
      "     Output: dataset-1a/alignments/781989364756468.tsv\n",
      "\n",
      "  üìÑ TSV File:\n",
      "     Path: dataset-1a/alignments/781989364756468.tsv\n",
      "     Size: 442 bytes | Lines: 18\n",
      "‚úÖ Saved metadata: dataset-1a/781989364756468.json\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [07:20<00:00, 220.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üìã Whisper Metadata:\n",
      "     Detected Language: ne (confidence: 90.13%)\n",
      "     Number of segments: 2\n",
      "  üìù Segment Details:\n",
      "     Segment 0: 0.00s - 12.52s | ‡§¨‡§ø‡§∞‡§æ‡§Æ‡§æ ‡§±‡•ç‡§π‡§ü‡•Ç‡§≤ ‡§ï‡§æ ‡§¨‡§æ‡§∏‡§ø‡§Ç‡§ß‡§æ ‡§ï‡§ø‡§∏‡•ç‡§•‡•ç‡§∞ ‡§ï‡•ã Oscillation Dedicated To Varma Tol\n",
      "     Segment 1: 12.52s - 16.98s | ‡•™‡•§‡•∑‡•ç‡§¶‡§æ, ‡§Ø‡§æ‡§ó‡§æ ‡§∏‡§Æ‡§ø‡§®‡§æ ‡§ï‡§ø‡§∏‡•ç‡§•‡•ç‡§∞ ‡§§‡§æ‡§™‡§æ‡§∞‡•ç‡§≠‡§æ‡§∞ ‡§∞‡•à‡§ó‡•Ä Ohh Funeral,\n",
      "  ‚úÖ Extracted 19 words with timestamps\n",
      "     Output: dataset-1a/alignments/2022332995220352.tsv\n",
      "\n",
      "  üìÑ TSV File:\n",
      "     Path: dataset-1a/alignments/2022332995220352.tsv\n",
      "     Size: 471 bytes | Lines: 19\n",
      "‚úÖ Saved metadata: dataset-1a/2022332995220352.json\n",
      "======================================================================\n",
      "\n",
      "\n",
      "######################################################################\n",
      "# ‚úÖ DATASET BUILD COMPLETE\n",
      "######################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------- MAIN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    videos = [\n",
    "        (\n",
    "            \"https://www.facebook.com/share/v/1CdiEnT1xw/\",\n",
    "            \"‡§µ‡§ø‡§∞‡§æ‡§ü‡§®‡§ó‡§∞‡§ï‡•ã ‡§ú‡§ø‡§≤‡•ç‡§≤‡§æ ‡§ï‡§æ‡§∞‡§æ‡§ó‡§æ‡§∞ ‡§Æ‡•ã‡§∞‡§ô‡§Æ‡§æ ‡§ï‡•à‡§¶‡§¨‡§®‡•ç‡§¶‡•Ä ‡§ú‡•Ä‡§µ‡§® ‡§¨‡§ø‡§§‡§æ‡§á‡§∞‡§π‡•á‡§ï‡§æ ‡§è‡§ï‡§ú‡§®‡§æ‡§ï‡•ã ‡§Ü‡§ú ‡§¶‡§ø‡§â‡§Å‡§∏‡•ã ‡§Æ‡•É‡§§‡•ç‡§Ø‡•Å ‡§≠‡§è‡§ï‡•ã ‡§õ\",\n",
    "            0.0,\n",
    "            6.0,\n",
    "        ),\n",
    "        (\n",
    "            \"https://www.facebook.com/share/v/17YKEbmgt8/\",\n",
    "            \"‡§µ‡§ø‡§∞‡§æ‡§ü‡§®‡§ó‡§∞‡§ï‡•ã ‡§µ‡§°‡§æ‡§®‡§Æ‡•ç‡§¨‡§∞ ‡•ß‡•¨ ‡§¨‡§∞‡•ç‡§Æ‡§æ‡§ü‡•ã‡§≤‡§ï‡§æ ‡§¨‡§æ‡§∏‡§ø‡§®‡•ç‡§¶‡§æ ‡§∏‡§ø‡§®‡•ã ‡§§‡§•‡§æ ‡§´‡•ã‡§π‡•ã‡§∞‡§ï‡•ã ‡§¶‡•Å‡§∞‡•ç‡§ó‡§®‡•ç‡§ß‡§ï‡§æ ‡§ï‡§æ‡§∞‡§£ ‡§™‡•Ä‡§°‡§ø‡§§ ‡§≠‡§è‡§ï‡§æ ‡§õ‡§® ‡§´‡•ã‡§π‡•ã‡§∞‡§ï‡§æ ‡§ï‡§æ‡§∞‡§£ ‡§â‡§§‡•ç‡§∏‡§∞‡•ç‡§ú‡§® ‡§π‡•Å‡§®‡•á ‡§π‡§æ‡§®‡§ø‡§ï‡§æ‡§∞‡§£ ‡§ó‡•ç‡§Ø‡§æ‡§Å‡§∏ ‡§∞ ‡§¶‡•Å‡§∞‡•ç‡§ó‡§®‡•ç‡§ß‡§ï‡§æ ‡§ï‡§æ‡§∞‡§£ ‡§∏‡•ã ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡§ï‡§æ ‡§¨‡§æ‡§∏‡§ø‡§®‡•ç‡§¶‡§æ ‡§ò‡§∞‡§ï‡•ã‡§†‡§æ‡§Æ‡•à‡§Ç ‡§Æ‡§æ‡§∏‡•ç‡§ï ‡§≤‡§ó‡§æ‡§è‡§∞ ‡§¨‡§∏‡•ç‡§®‡•Å‡§™‡§∞‡•ç‡§®‡•á ‡§Ö‡§µ‡§∏‡•ç‡§•‡§æ ‡§∏‡•É‡§ú‡§®‡§æ ‡§≠‡§è‡§ï‡•ã ‡§õ\",\n",
    "            0.0,\n",
    "            17.0,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(f\"# DATASET BUILD STARTING\")\n",
    "    print(f\"# Device: {DEVICE} | Whisper Model: medium\")\n",
    "    print(f\"{'#'*70}\\n\")\n",
    "\n",
    "    # Pass the global whisper_model to all workers\n",
    "    with ThreadPoolExecutor(max_workers=2) as ex:\n",
    "        futures = [\n",
    "            ex.submit(process_sample, *v, whisper_model=whisper_model) \n",
    "            for v in videos\n",
    "        ]\n",
    "        for future in tqdm(futures, total=len(videos), desc=\"Processing videos\"):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Worker error: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(f\"# ‚úÖ DATASET BUILD COMPLETE\")\n",
    "    print(f\"{'#'*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65e858e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------- MAIN ----------\n",
    "# if __name__ == \"__main__\":\n",
    "#     # List of (URL, title, start_time, end_time)\n",
    "#     videos = [\n",
    "#         (\n",
    "#             \"https://www.facebook.com/share/v/1CdiEnT1xw/\",\n",
    "#             \"‡§µ‡§ø‡§∞‡§æ‡§ü‡§®‡§ó‡§∞‡§ï‡•ã ‡§ú‡§ø‡§≤‡•ç‡§≤‡§æ ‡§ï‡§æ‡§∞‡§æ‡§ó‡§æ‡§∞ ‡§Æ‡•ã‡§∞‡§ô‡§Æ‡§æ ‡§ï‡•à‡§¶‡•Ä‡§¨‡§®‡•ç‡§¶‡•Ä ‡§ú‡•Ä‡§µ‡§® ‡§¨‡§ø‡§§‡§æ‡§á‡§∞‡§π‡•á‡§ï‡§æ ‡§è‡§ï‡§ú‡§®‡§æ‡§ï‡•ã ‡§Ü‡§ú ‡§¶‡§ø‡§â‡§Å‡§∏‡•ã ‡§Æ‡•É‡§§‡•ç‡§Ø‡•Å ‡§≠‡§è‡§ï‡•ã ‡§õ ‡•§\",\n",
    "#             0.0,\n",
    "#             6.0,\n",
    "#         ),\n",
    "#         (\n",
    "#             \"https://www.facebook.com/share/v/17YKEbmgt8/\",\n",
    "#             \"‡§µ‡§ø‡§∞‡§æ‡§ü‡§®‡§ó‡§∞‡§ï‡•ã ‡§µ‡§°‡§æ‡§®‡§Æ‡•ç‡§¨‡§∞ ‡•ß‡•¨ ‡§¨‡§∞‡•ç‡§Æ‡§æ‡§ü‡•ã‡§≤‡§ï‡§æ ‡§¨‡§æ‡§∏‡§ø‡§®‡•ç‡§¶‡§æ ‡§∏‡§ø‡§®‡•ã ‡§§‡§•‡§æ ‡§´‡•ã‡§π‡•ã‡§∞‡§ï‡•ã ‡§¶‡•Å‡§∞‡•ç‡§ó‡§®‡•ç‡§ß‡§ï‡§æ ‡§ï‡§æ‡§∞‡§£ ‡§™‡•Ä‡§°‡§ø‡§§ ‡§≠‡§è‡§ï‡§æ ‡§õ‡§® ‡•§ ‡§´‡•ã‡§π‡•ã‡§∞‡§ï‡§æ ‡§ï‡§æ‡§∞‡§£ ‡§â‡§§‡•ç‡§∏‡§∞‡•ç‡§ú‡§® ‡§π‡•Å‡§®‡•á ‡§π‡§æ‡§®‡§ø‡§ï‡§æ‡§∞‡§£ ‡§ó‡•ç‡§Ø‡§æ‡§Å‡§∏ ‡§∞ ‡§¶‡•Å‡§∞‡•ç‡§ó‡§®‡•ç‡§ß‡§ï‡§æ ‡§ï‡§æ‡§∞‡§£ ‡§∏‡•ã ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡§ï‡§æ ‡§¨‡§æ‡§∏‡§ø‡§®‡•ç‡§¶‡§æ ‡§ò‡§∞‡§ï‡•ã‡§†‡§æ‡§Æ‡•à‡§Ç ‡§Æ‡§æ‡§∏‡•ç‡§ï ‡§≤‡§ó‡§æ‡§è‡§∞ ‡§¨‡§∏‡•ç‡§®‡•Å‡§™‡§∞‡•ç‡§®‡•á ‡§Ö‡§µ‡§∏‡•ç‡§•‡§æ ‡§∏‡•É‡§ú‡§®‡§æ ‡§≠‡§è‡§ï‡•ã ‡§õ ‡•§\",\n",
    "#             0.0,\n",
    "#             17.0,\n",
    "#         ),\n",
    "#     ]\n",
    "\n",
    "#     with ThreadPoolExecutor(max_workers=2) as ex:\n",
    "#         for _ in tqdm(ex.map(lambda v: process_sample(*v), videos), total=len(videos)):\n",
    "#             pass\n",
    "#     print(\"‚úÖ Dataset build complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Major",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
