{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd286df1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmediapipe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Lip landmark indices (MediaPipe standard)\n",
    "OUTER_LIPS = [\n",
    "    61, 146, 91, 181, 84, 17,\n",
    "    314, 405, 321, 375, 291, 308\n",
    "]\n",
    "\n",
    "INNER_LIPS = [\n",
    "    78, 95, 88, 178, 87, 14,\n",
    "    317, 402, 318, 324, 308\n",
    "]\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "\n",
    "            # Convert landmarks to pixel coords\n",
    "            outer_pts = []\n",
    "            inner_pts = []\n",
    "\n",
    "            for idx in OUTER_LIPS:\n",
    "                lm = face_landmarks.landmark[idx]\n",
    "                outer_pts.append([int(lm.x * w), int(lm.y * h)])\n",
    "\n",
    "            for idx in INNER_LIPS:\n",
    "                lm = face_landmarks.landmark[idx]\n",
    "                inner_pts.append([int(lm.x * w), int(lm.y * h)])\n",
    "\n",
    "            outer_pts = np.array(outer_pts, np.int32)\n",
    "            inner_pts = np.array(inner_pts, np.int32)\n",
    "\n",
    "            # Draw lip contour\n",
    "            cv2.polylines(frame, [outer_pts], True, (0, 255, 0), 2)\n",
    "\n",
    "            # Blacken mouth opening\n",
    "            cv2.fillPoly(frame, [inner_pts], (0, 0, 0))\n",
    "\n",
    "    cv2.imshow(\"Lip Contour & Mouth Mask\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC to exit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8764f180",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmediapipe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "OUTER_LIPS = [\n",
    "    61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "    291, 375, 321, 405, 314, 17, 84, 181, 91, 146\n",
    "]\n",
    "\n",
    "INNER_LIPS = [\n",
    "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "    308, 324, 318, 402, 317, 14, 87, 178, 88, 95\n",
    "]\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "\n",
    "            outer_pts = np.array([\n",
    "                [int(face_landmarks.landmark[i].x * w),\n",
    "                 int(face_landmarks.landmark[i].y * h)]\n",
    "                for i in OUTER_LIPS\n",
    "            ], np.int32)\n",
    "\n",
    "            inner_pts = np.array([\n",
    "                [int(face_landmarks.landmark[i].x * w),\n",
    "                 int(face_landmarks.landmark[i].y * h)]\n",
    "                for i in INNER_LIPS\n",
    "            ], np.int32)\n",
    "\n",
    "            # Draw full lip contour\n",
    "            cv2.polylines(frame, [outer_pts], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "            # Blacken mouth opening\n",
    "            cv2.fillPoly(frame, [inner_pts], (0, 0, 0))\n",
    "\n",
    "    cv2.imshow(\"Full Lip Contour + Mouth Opening\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caf0a7fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmediapipe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "OUTER_LIPS = [\n",
    "    61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "    291, 375, 321, 405, 314, 17, 84, 181, 91, 146\n",
    "]\n",
    "\n",
    "INNER_LIPS = [\n",
    "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "    308, 324, 318, 402, 317, 14, 87, 178, 88, 95\n",
    "]\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        face_landmarks = results.multi_face_landmarks[0]\n",
    "\n",
    "        outer_pts = np.array([\n",
    "            [int(face_landmarks.landmark[i].x * w),\n",
    "             int(face_landmarks.landmark[i].y * h)]\n",
    "            for i in OUTER_LIPS\n",
    "        ], np.int32)\n",
    "\n",
    "        inner_pts = np.array([\n",
    "            [int(face_landmarks.landmark[i].x * w),\n",
    "             int(face_landmarks.landmark[i].y * h)]\n",
    "            for i in INNER_LIPS\n",
    "        ], np.int32)\n",
    "\n",
    "        # Bounding box around lips\n",
    "        x, y, bw, bh = cv2.boundingRect(outer_pts)\n",
    "\n",
    "        # Create white canvas\n",
    "        lip_roi = np.ones((bh, bw, 3), dtype=np.uint8) * 255\n",
    "\n",
    "        # Shift points to ROI coordinates\n",
    "        outer_shifted = outer_pts - [x, y]\n",
    "        inner_shifted = inner_pts - [x, y]\n",
    "\n",
    "        # Fill lips area with gray\n",
    "        cv2.fillPoly(lip_roi, [outer_shifted], (180, 180, 180))\n",
    "\n",
    "        # Fill mouth opening with black\n",
    "        cv2.fillPoly(lip_roi, [inner_shifted], (0, 0, 0))\n",
    "\n",
    "        # Optional: draw bounding box on original frame\n",
    "        cv2.rectangle(frame, (x, y), (x + bw, y + bh), (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"Lip ROI (Gray Lips, Black Mouth, White Background)\", lip_roi)\n",
    "\n",
    "    cv2.imshow(\"Camera\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "333c14c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch; print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "514a3c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv2: 4.13.0\n",
      "mediapipe OK\n",
      "torch: 2.7.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import sklearn\n",
    "\n",
    "print(\"cv2:\", cv2.__version__)\n",
    "print(\"mediapipe OK\")\n",
    "print(\"torch:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2acdfce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Full lip landmark loops\n",
    "OUTER_LIPS = [\n",
    "    61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "    291, 375, 321, 405, 314, 17, 84, 181, 91, 146\n",
    "]\n",
    "\n",
    "INNER_LIPS = [\n",
    "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "    308, 324, 318, 402, 317, 14, 87, 178, 88, 95\n",
    "]\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        face_landmarks = results.multi_face_landmarks[0]\n",
    "\n",
    "        outer_pts = np.array([\n",
    "            [int(face_landmarks.landmark[i].x * w),\n",
    "             int(face_landmarks.landmark[i].y * h)]\n",
    "            for i in OUTER_LIPS\n",
    "        ], np.int32)\n",
    "\n",
    "        inner_pts = np.array([\n",
    "            [int(face_landmarks.landmark[i].x * w),\n",
    "             int(face_landmarks.landmark[i].y * h)]\n",
    "            for i in INNER_LIPS\n",
    "        ], np.int32)\n",
    "\n",
    "        # Lip bounding box\n",
    "        x, y, bw, bh = cv2.boundingRect(outer_pts)\n",
    "\n",
    "        # White ROI\n",
    "        lip_roi = np.ones((bh, bw, 3), dtype=np.uint8) * 255\n",
    "\n",
    "        # Shift landmarks to ROI coordinates\n",
    "        outer_shifted = outer_pts - [x, y]\n",
    "        inner_shifted = inner_pts - [x, y]\n",
    "\n",
    "        # Fill lips gray\n",
    "        cv2.fillPoly(lip_roi, [outer_shifted], (180, 180, 180))\n",
    "\n",
    "        # Fill mouth opening black\n",
    "        cv2.fillPoly(lip_roi, [inner_shifted], (0, 0, 0))\n",
    "\n",
    "        # Normalize to [0,1]\n",
    "        lip_norm = lip_roi.astype(np.float32) / 255.0\n",
    "\n",
    "        # Resize for ML\n",
    "        lip_norm = cv2.resize(lip_norm, (128, 64))\n",
    "\n",
    "        # Draw bounding box on original frame\n",
    "        cv2.rectangle(frame, (x, y), (x + bw, y + bh), (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"Normalized Lip ROI\", lip_norm)\n",
    "\n",
    "    cv2.imshow(\"Camera\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3508ac3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ccb52b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.23,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Full lip landmark loops\n",
    "OUTER_LIPS = [\n",
    "    61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "    291, 375, 321, 405, 314, 17, 84, 181, 91, 146\n",
    "]\n",
    "\n",
    "INNER_LIPS = [\n",
    "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "    308, 324, 318, 402, 317, 14, 87, 178, 88, 95\n",
    "]\n",
    "\n",
    "# Padding control (CHANGE THESE)\n",
    "PAD_X_RATIO = 0.15   # 15% width padding\n",
    "PAD_Y_RATIO = 0.15   # 15% height padding\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        face_landmarks = results.multi_face_landmarks[0]\n",
    "\n",
    "        outer_pts = np.array([\n",
    "            [int(face_landmarks.landmark[i].x * w),\n",
    "             int(face_landmarks.landmark[i].y * h)]\n",
    "            for i in OUTER_LIPS\n",
    "        ], np.int32)\n",
    "\n",
    "        inner_pts = np.array([\n",
    "            [int(face_landmarks.landmark[i].x * w),\n",
    "             int(face_landmarks.landmark[i].y * h)]\n",
    "            for i in INNER_LIPS\n",
    "        ], np.int32)\n",
    "\n",
    "        # Base lip bounding box\n",
    "        x, y, bw, bh = cv2.boundingRect(outer_pts)\n",
    "\n",
    "        # Dynamic padding\n",
    "        pad_x = int(PAD_X_RATIO * bw)\n",
    "        pad_y = int(PAD_Y_RATIO * bh)\n",
    "\n",
    "        x = max(0, x - pad_x)\n",
    "        y = max(0, y - pad_y)\n",
    "        bw = min(w - x, bw + 2 * pad_x)\n",
    "        bh = min(h - y, bh + 2 * pad_y)\n",
    "\n",
    "        # White ROI\n",
    "        lip_roi = np.ones((bh, bw, 3), dtype=np.uint8) * 255\n",
    "\n",
    "        # Shift landmarks into ROI space\n",
    "        outer_shifted = outer_pts - [x, y]\n",
    "        inner_shifted = inner_pts - [x, y]\n",
    "\n",
    "        # Fill lips (gray)\n",
    "        cv2.fillPoly(lip_roi, [outer_shifted], (180, 180, 180))\n",
    "\n",
    "        # Fill mouth opening (black)\n",
    "        cv2.fillPoly(lip_roi, [inner_shifted], (0, 0, 0))\n",
    "\n",
    "        # Normalize to [0,1]\n",
    "        lip_norm = lip_roi.astype(np.float32) / 255.0\n",
    "\n",
    "        # Resize (ML-ready)\n",
    "        lip_norm = cv2.resize(lip_norm, (128, 64))\n",
    "\n",
    "        # Draw bounding box on original frame\n",
    "        cv2.rectangle(frame, (x, y), (x + bw, y + bh), (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"Normalized Lip ROI\", lip_norm)\n",
    "\n",
    "    cv2.imshow(\"Camera\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3379bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Full lip landmark loops\n",
    "OUTER_LIPS = [\n",
    "    61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "    291, 375, 321, 405, 314, 17, 84, 181, 91, 146\n",
    "]\n",
    "\n",
    "INNER_LIPS = [\n",
    "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "    308, 324, 318, 402, 317, 14, 87, 178, 88, 95\n",
    "]\n",
    "\n",
    "# Padding control (fixed pixels)\n",
    "PAD_X = 30  # horizontal padding in pixels\n",
    "PAD_Y = 15  # vertical padding in pixels\n",
    "\n",
    "# Fixed aspect ratio (width : height)\n",
    "ASPECT_RATIO = 2.0  # e.g., width is twice height\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        face_landmarks = results.multi_face_landmarks[0]\n",
    "\n",
    "        outer_pts = np.array([\n",
    "            [int(face_landmarks.landmark[i].x * w),\n",
    "             int(face_landmarks.landmark[i].y * h)]\n",
    "            for i in OUTER_LIPS\n",
    "        ], np.int32)\n",
    "\n",
    "        inner_pts = np.array([\n",
    "            [int(face_landmarks.landmark[i].x * w),\n",
    "             int(face_landmarks.landmark[i].y * h)]\n",
    "            for i in INNER_LIPS\n",
    "        ], np.int32)\n",
    "\n",
    "        # Base lip bounding box\n",
    "        x, y, bw, bh = cv2.boundingRect(outer_pts)\n",
    "\n",
    "        # Add fixed padding\n",
    "        x = max(0, x - PAD_X)\n",
    "        y = max(0, y - PAD_Y)\n",
    "        bw = min(w - x, bw + 2 * PAD_X)\n",
    "        bh = min(h - y, bh + 2 * PAD_Y)\n",
    "\n",
    "        # Adjust to fixed aspect ratio (width = ASPECT_RATIO * height)\n",
    "        desired_bw = int(ASPECT_RATIO * bh)\n",
    "        dw = desired_bw - bw\n",
    "        x = max(0, x - dw // 2)\n",
    "        bw = min(w - x, desired_bw)\n",
    "\n",
    "        # White ROI\n",
    "        lip_roi = np.ones((bh, bw, 3), dtype=np.uint8) * 255\n",
    "\n",
    "        # Shift landmarks into ROI space\n",
    "        outer_shifted = outer_pts - [x, y]\n",
    "        inner_shifted = inner_pts - [x, y]\n",
    "\n",
    "        # Fill lips gray\n",
    "        cv2.fillPoly(lip_roi, [outer_shifted], (180, 180, 180))\n",
    "\n",
    "        # Fill mouth opening black\n",
    "        cv2.fillPoly(lip_roi, [inner_shifted], (0, 0, 0))\n",
    "\n",
    "        # Normalize to [0,1]\n",
    "        lip_norm = lip_roi.astype(np.float32) / 255.0\n",
    "\n",
    "        # Resize (ML-ready)\n",
    "        lip_norm = cv2.resize(lip_norm, (128, 64))\n",
    "\n",
    "        # Draw bounding box on original frame\n",
    "        cv2.rectangle(frame, (x, y), (x + bw, y + bh), (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"Normalized Lip ROI\", lip_norm)\n",
    "\n",
    "    cv2.imshow(\"Camera\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "968de698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ------------------------\n",
    "# SETTINGS\n",
    "# ------------------------\n",
    "NUM_CLASSES = 3\n",
    "FRAMES_PER_SAMPLE = 50\n",
    "FRAME_H, FRAME_W = 128, 64\n",
    "FEATURE_DIM = FRAME_H * FRAME_W\n",
    "PAD_X, PAD_Y = 30, 15\n",
    "ASPECT_RATIO = 2.0\n",
    "\n",
    "CLASS_NAMES = [\"mathi\", \"tala\", \"Daya\"]\n",
    "MODEL_PATH = \"lip_lstm.pth\"\n",
    "\n",
    "# ------------------------\n",
    "# MEDIA PIPE\n",
    "# ------------------------\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "OUTER_LIPS = [\n",
    "    61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "    291, 375, 321, 405, 314, 17, 84, 181, 91, 146\n",
    "]\n",
    "INNER_LIPS = [\n",
    "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "    308, 324, 318, 402, 317, 14, 87, 178, 88, 95\n",
    "]\n",
    "\n",
    "# ------------------------\n",
    "# MODEL\n",
    "# ------------------------\n",
    "class LipLSTM(nn.Module):\n",
    "    def __init__(self, feature_dim, hidden_sizes=[256,128,64], num_classes=NUM_CLASSES, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.lstm1 = nn.LSTM(feature_dim, hidden_sizes[0], batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_sizes[0], hidden_sizes[1], batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(hidden_sizes[1], hidden_sizes[2], batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_sizes[2], 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.leaky_relu(out)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.leaky_relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out, _ = self.lstm3(out)\n",
    "        out = self.leaky_relu(out[:, -1, :])\n",
    "        out = self.dropout(out)\n",
    "        out = self.leaky_relu(self.fc1(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.leaky_relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LipLSTM(FEATURE_DIM).to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "# ------------------------\n",
    "# CAMERA\n",
    "# ------------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = deque(maxlen=FRAMES_PER_SAMPLE)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb)\n",
    "\n",
    "    lip_display = None\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        face_landmarks = results.multi_face_landmarks[0]\n",
    "\n",
    "        # Lip ROI\n",
    "        outer_pts = np.array([[int(face_landmarks.landmark[i].x * w),\n",
    "                               int(face_landmarks.landmark[i].y * h)] for i in OUTER_LIPS], np.int32)\n",
    "        inner_pts = np.array([[int(face_landmarks.landmark[i].x * w),\n",
    "                               int(face_landmarks.landmark[i].y * h)] for i in INNER_LIPS], np.int32)\n",
    "\n",
    "        x, y, bw, bh = cv2.boundingRect(outer_pts)\n",
    "        x = max(0, x - PAD_X)\n",
    "        y = max(0, y - PAD_Y)\n",
    "        bw = min(w - x, bw + 2*PAD_X)\n",
    "        bh = min(h - y, bh + 2*PAD_Y)\n",
    "\n",
    "        desired_bw = int(ASPECT_RATIO * bh)\n",
    "        dw = desired_bw - bw\n",
    "        x = max(0, x - dw // 2)\n",
    "        bw = min(w - x, desired_bw)\n",
    "\n",
    "        lip_roi = np.ones((bh, bw, 3), dtype=np.uint8) * 255\n",
    "        outer_shifted = outer_pts - [x, y]\n",
    "        inner_shifted = inner_pts - [x, y]\n",
    "        cv2.fillPoly(lip_roi, [outer_shifted], (180,180,180))\n",
    "        cv2.fillPoly(lip_roi, [inner_shifted], (0,0,0))\n",
    "\n",
    "        # ------------------------\n",
    "        # SHOW LIP CONTOUR WINDOW (correct aspect ratio)\n",
    "        # ------------------------\n",
    "        scale = 2  # upscale for visibility\n",
    "        h_roi, w_roi = lip_roi.shape[:2]\n",
    "        lip_display = cv2.resize(lip_roi, (int(w_roi*scale), int(h_roi*scale)))\n",
    "\n",
    "        # Convert for model\n",
    "        lip_gray = cv2.cvtColor(lip_roi, cv2.COLOR_BGR2GRAY)\n",
    "        lip_norm = cv2.resize(lip_gray, (FRAME_W, FRAME_H))\n",
    "        lip_norm = lip_norm.astype(np.float32) / 255.0\n",
    "        sequence.append(lip_norm.flatten())\n",
    "\n",
    "        # ------------------------\n",
    "        # Predict when we have enough frames\n",
    "        # ------------------------\n",
    "        if len(sequence) == FRAMES_PER_SAMPLE:\n",
    "            input_seq = torch.tensor(np.array(sequence), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                preds = model(input_seq)\n",
    "                probs = torch.softmax(preds, dim=1).cpu().numpy()[0]\n",
    "                pred_class = probs.argmax()\n",
    "                pred_name = CLASS_NAMES[pred_class]\n",
    "\n",
    "            # Display prediction\n",
    "            cv2.putText(frame, f\"Predicted: {pred_name}\", (10,30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "\n",
    "            # Confidence bars\n",
    "            start_y = 50\n",
    "            for i, (name, prob) in enumerate(zip(CLASS_NAMES, probs)):\n",
    "                bar_len = int(prob*200)\n",
    "                cv2.rectangle(frame, (10, start_y + i*25), (10+bar_len, start_y+i*25+20), (0,255,0), -1)\n",
    "                cv2.putText(frame, f\"{name}: {prob*100:.1f}%\", (220, start_y+i*25+15),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1)\n",
    "\n",
    "    # ------------------------\n",
    "    # SHOW WINDOWS\n",
    "    # ------------------------\n",
    "    cv2.imshow(\"Camera\", frame)\n",
    "    if lip_display is not None:\n",
    "        cv2.imshow(\"Lip Contour\", lip_display)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce834cfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'mediapipe' has no attribute 'solutions'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmediapipe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m mp_face_mesh = \u001b[43mmp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msolutions\u001b[49m.face_mesh\n",
      "\u001b[31mAttributeError\u001b[39m: module 'mediapipe' has no attribute 'solutions'"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "mp_face_mesh = mp.solutions.face_mesh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a31a629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All frames converted to numpy arrays and saved in 'npy_databig'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "# ------------------------\n",
    "# SETTINGS\n",
    "# ------------------------\n",
    "DATA_DIR = \"databig\"           # original folder with class_x/sample_y/frames\n",
    "OUTPUT_DIR = \"npy_databig\"     # where numpy arrays will be saved\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ------------------------\n",
    "# COPY FOLDER STRUCTURE & CONVERT\n",
    "# ------------------------\n",
    "classes = sorted([d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))])\n",
    "\n",
    "for class_name in classes:\n",
    "    class_input_folder = os.path.join(DATA_DIR, class_name)\n",
    "    class_output_folder = os.path.join(OUTPUT_DIR, class_name)\n",
    "    os.makedirs(class_output_folder, exist_ok=True)\n",
    "\n",
    "    samples = sorted([s for s in os.listdir(class_input_folder) if os.path.isdir(os.path.join(class_input_folder, s))])\n",
    "    for sample_name in samples:\n",
    "        sample_input_folder = os.path.join(class_input_folder, sample_name)\n",
    "        sample_output_folder = os.path.join(class_output_folder, sample_name)\n",
    "        os.makedirs(sample_output_folder, exist_ok=True)\n",
    "\n",
    "        frames = sorted([f for f in os.listdir(sample_input_folder) if f.endswith(\".png\") or f.endswith(\".jpg\")])\n",
    "        for frame_file in frames:\n",
    "            frame_path = os.path.join(sample_input_folder, frame_file)\n",
    "            img = cv2.imread(frame_path, cv2.IMREAD_GRAYSCALE)  # already grayscale\n",
    "            img_array = np.array(img, dtype=np.float32)         # convert to array\n",
    "            npy_path = os.path.join(sample_output_folder, frame_file.replace(\".png\", \".npy\").replace(\".jpg\", \".npy\"))\n",
    "            np.save(npy_path, img_array)                        # save as .npy\n",
    "\n",
    "print(f\"All frames converted to numpy arrays and saved in '{OUTPUT_DIR}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff39b959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 300\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ------------------------\n",
    "# SETTINGS\n",
    "# ------------------------\n",
    "DATA_DIR = \"npy_databig\"  # folder structure: npy_data/class_x/sample_y/*.npy\n",
    "NUM_CLASSES = 3\n",
    "FRAMES_PER_SAMPLE = 50  # sequence length\n",
    "FRAME_H, FRAME_W = 128, 64\n",
    "FEATURE_DIM = FRAME_H * FRAME_W\n",
    "\n",
    "# ------------------------\n",
    "# DATASET\n",
    "# ------------------------\n",
    "class LipNpyDataset(Dataset):\n",
    "    def __init__(self, data_dir, frames_per_sample=50):\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "\n",
    "        for class_idx in range(NUM_CLASSES):\n",
    "            class_folder = os.path.join(data_dir, f\"class_{class_idx}\")\n",
    "            sample_folders = sorted([s for s in os.listdir(class_folder) if os.path.isdir(os.path.join(class_folder, s))])\n",
    "\n",
    "            for sample_name in sample_folders:\n",
    "                sample_folder = os.path.join(class_folder, sample_name)\n",
    "                npy_files = sorted([f for f in os.listdir(sample_folder) if f.endswith(\".npy\")])\n",
    "\n",
    "                if len(npy_files) < frames_per_sample:\n",
    "                    continue  # skip incomplete samples\n",
    "\n",
    "                sequence = []\n",
    "                for f in npy_files[:frames_per_sample]:\n",
    "                    arr = np.load(os.path.join(sample_folder, f))\n",
    "                    arr = np.resize(arr, (FRAME_H * FRAME_W))  # flatten\n",
    "                    sequence.append(arr)\n",
    "\n",
    "                self.samples.append(np.array(sequence))  # shape: (frames, feature_dim)\n",
    "                self.labels.append(class_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.samples[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "# ------------------------\n",
    "# DATALOADER\n",
    "# ------------------------\n",
    "dataset = LipNpyDataset(DATA_DIR, FRAMES_PER_SAMPLE)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "print(\"Dataset size:\", len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad3ba7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LipLSTM(\n",
      "  (lstm1): LSTM(8192, 256, batch_first=True)\n",
      "  (lstm2): LSTM(256, 128, batch_first=True)\n",
      "  (lstm3): LSTM(128, 64, batch_first=True)\n",
      "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=3, bias=True)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "FEATURE_DIM = 128 * 64 # 128*64=8192\n",
    "SEQUENCE_LENGTH = FRAMES_PER_SAMPLE\n",
    "\n",
    "class LipLSTM(nn.Module):\n",
    "    def __init__(self, feature_dim, hidden_sizes=[256,128,64], num_classes=NUM_CLASSES, dropout=0.3):\n",
    "        super(LipLSTM, self).__init__()\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size=feature_dim, hidden_size=hidden_sizes[0], batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_sizes[0], hidden_size=hidden_sizes[1], batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(input_size=hidden_sizes[1], hidden_size=hidden_sizes[2], batch_first=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_sizes[2], 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.leaky_relu(out)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.leaky_relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out, _ = self.lstm3(out)\n",
    "        out = self.leaky_relu(out)\n",
    "        out = self.dropout(out[:, -1, :])  # take last timestep\n",
    "        out = self.leaky_relu(self.fc1(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.leaky_relu(self.fc2(out))\n",
    "        out = self.fc3(out)  # logits\n",
    "        return out\n",
    "\n",
    "# ------------------------\n",
    "# INITIALIZE MODEL\n",
    "# ------------------------\n",
    "model = LipLSTM(FEATURE_DIM)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "392ff4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LipLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim,\n",
    "        hidden_sizes=[256, 128, 64],\n",
    "        num_classes=NUM_CLASSES,\n",
    "        dropout=0.3,\n",
    "        cnn_channels=128,\n",
    "        kernel_size=3\n",
    "    ):\n",
    "        super(LipLSTM, self).__init__()\n",
    "\n",
    "        # ---------- CNN FRONT ----------\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=feature_dim,\n",
    "                out_channels=cnn_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2\n",
    "            ),\n",
    "            nn.BatchNorm1d(cnn_channels),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # ---------- LSTM STACK ----------\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=cnn_channels,\n",
    "            hidden_size=hidden_sizes[0],\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=hidden_sizes[0],\n",
    "            hidden_size=hidden_sizes[1],\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.lstm3 = nn.LSTM(\n",
    "            input_size=hidden_sizes[1],\n",
    "            hidden_size=hidden_sizes[2],\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # ---------- FC HEAD ----------\n",
    "        self.fc1 = nn.Linear(hidden_sizes[2], 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, time, feature_dim)\n",
    "\n",
    "        # ---- CNN ----\n",
    "        x = x.permute(0, 2, 1)     # (batch, feature_dim, time)\n",
    "        x = self.cnn(x)\n",
    "        x = x.permute(0, 2, 1)     # (batch, time, cnn_channels)\n",
    "\n",
    "        # ---- LSTM ----\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.leaky_relu(out)\n",
    "\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.leaky_relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out, _ = self.lstm3(out)\n",
    "\n",
    "        # last time step\n",
    "        out = self.leaky_relu(out[:, -1, :])\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # ---- FC ----\n",
    "        out = self.leaky_relu(self.fc1(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.leaky_relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7808eb0",
   "metadata": {},
   "source": [
    "MORE COMPLEXITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "602d1cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LipLSTM(\n",
      "  (lstm1): LSTM(8192, 512, batch_first=True)\n",
      "  (lstm2): LSTM(512, 256, batch_first=True)\n",
      "  (lstm3): LSTM(256, 128, batch_first=True)\n",
      "  (lstm4): LSTM(128, 64, batch_first=True)\n",
      "  (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=3, bias=True)\n",
      "  (leaky_relu): LeakyReLU(negative_slope=0.01)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (batchnorm1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "FEATURE_DIM = 128 * 64\n",
    "NUM_CLASSES = 3\n",
    "SEQUENCE_LENGTH = 50  # frames per sample\n",
    "\n",
    "class LipLSTM(nn.Module):\n",
    "    def __init__(self, feature_dim, hidden_sizes=[512, 256, 128, 64], num_classes=NUM_CLASSES, dropout=0.4):\n",
    "        super(LipLSTM, self).__init__()\n",
    "\n",
    "        # 4-layer LSTM stack\n",
    "        self.lstm1 = nn.LSTM(input_size=feature_dim, hidden_size=hidden_sizes[0], batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_sizes[0], hidden_size=hidden_sizes[1], batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(input_size=hidden_sizes[1], hidden_size=hidden_sizes[2], batch_first=True)\n",
    "        self.lstm4 = nn.LSTM(input_size=hidden_sizes[2], hidden_size=hidden_sizes[3], batch_first=True)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_sizes[3], 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, num_classes)\n",
    "\n",
    "        # Activation & regularization\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hidden_sizes[3])\n",
    "        self.batchnorm2 = nn.BatchNorm1d(128)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.leaky_relu(out)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.leaky_relu(out)\n",
    "        out, _ = self.lstm3(out)\n",
    "        out = self.leaky_relu(out)\n",
    "        out, _ = self.lstm4(out)\n",
    "        out = self.leaky_relu(out[:, -1, :])  # last timestep\n",
    "        out = self.batchnorm1(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.leaky_relu(self.fc1(out))\n",
    "        out = self.batchnorm2(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.leaky_relu(self.fc2(out))\n",
    "        out = self.batchnorm3(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.leaky_relu(self.fc3(out))\n",
    "        out = self.fc4(out)  # logits\n",
    "        return out\n",
    "\n",
    "# ------------------------\n",
    "# TEST\n",
    "# ------------------------\n",
    "model = LipLSTM(FEATURE_DIM)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00a8ac34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to lip_lstm.pth\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"lip_lstm.pth\"  # you can change the filename\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "print(f\"Model weights saved to {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53625ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.086323\n",
      "Epoch 2, Loss: 0.778891\n",
      "Epoch 3, Loss: 0.670027\n",
      "Epoch 4, Loss: 0.587809\n",
      "Epoch 5, Loss: 0.570852\n",
      "Epoch 6, Loss: 0.783436\n",
      "Epoch 7, Loss: 1.064317\n",
      "Epoch 8, Loss: 1.021475\n",
      "Epoch 9, Loss: 1.004521\n",
      "Epoch 10, Loss: 1.015887\n",
      "Epoch 11, Loss: 0.984028\n",
      "Epoch 12, Loss: 0.965290\n",
      "Epoch 13, Loss: 1.031117\n",
      "Epoch 14, Loss: 1.058656\n",
      "Epoch 15, Loss: 1.044846\n",
      "Epoch 16, Loss: 0.987508\n",
      "Epoch 17, Loss: 0.987459\n",
      "Epoch 18, Loss: 1.022289\n",
      "Epoch 19, Loss: 0.874060\n",
      "Epoch 20, Loss: 0.786705\n",
      "Epoch 21, Loss: 0.809002\n",
      "Epoch 22, Loss: 0.695024\n",
      "Epoch 23, Loss: 0.744005\n",
      "Epoch 24, Loss: 0.654521\n",
      "Epoch 25, Loss: 0.780717\n",
      "Epoch 26, Loss: 0.645776\n",
      "Epoch 27, Loss: 0.692563\n",
      "Epoch 28, Loss: 0.650617\n",
      "Epoch 29, Loss: 0.721742\n",
      "Epoch 30, Loss: 0.608686\n",
      "Epoch 31, Loss: 0.617331\n",
      "Epoch 32, Loss: 0.555190\n",
      "Epoch 33, Loss: 0.551157\n",
      "Epoch 34, Loss: 0.581331\n",
      "Epoch 35, Loss: 0.589101\n",
      "Epoch 36, Loss: 0.552825\n",
      "Epoch 37, Loss: 0.534206\n",
      "Epoch 38, Loss: 0.501995\n",
      "Epoch 39, Loss: 0.546141\n",
      "Epoch 40, Loss: 0.557523\n",
      "Epoch 41, Loss: 0.548343\n",
      "Epoch 42, Loss: 0.515587\n",
      "Epoch 43, Loss: 0.544016\n",
      "Epoch 44, Loss: 0.513523\n",
      "Epoch 45, Loss: 0.526949\n",
      "Epoch 46, Loss: 0.532539\n",
      "Epoch 47, Loss: 0.612134\n",
      "Epoch 48, Loss: 0.562564\n",
      "Epoch 49, Loss: 0.606740\n",
      "Epoch 50, Loss: 0.561477\n",
      "Epoch 51, Loss: 0.574687\n",
      "Epoch 52, Loss: 0.535585\n",
      "Epoch 53, Loss: 0.531009\n",
      "Epoch 54, Loss: 0.569191\n",
      "Epoch 55, Loss: 0.571503\n",
      "Epoch 56, Loss: 0.550072\n",
      "Epoch 57, Loss: 0.567460\n",
      "Epoch 58, Loss: 0.513700\n",
      "Epoch 59, Loss: 0.521516\n",
      "Epoch 60, Loss: 0.603180\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(60):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(dataset):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cda98e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhoAAAHHCAYAAAD58fFKAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAARLhJREFUeJzt3QmcTfX7wPHn3GFmMAZN9l1ZsmfJ3/ILEf3ys0SJJBFtyJIliYhSlDVbKmRJESpC2QshW3bKWtZsE7Lf/+v5cq+5Y2hm7j0zc+/9vH+v85u555x77rn3aM5zn+/z/X4tp9PpFAAAABs47DgoAAAAgQYAALAVGQ0AAGAbAg0AAGAbAg0AAGAbAg0AAGAbAg0AAGAbAg0AAGAbAg0AAGAbAg0gBdm9e7fUqlVLMmTIIJZlyezZs316/H379pnjTpgwwafH9WfVqlUzCwB7EGgAsfz+++/ywgsvSIECBSQ8PFwiIyOlcuXKMmzYMPnnn39s/bxatGghmzdvlrffflsmTZok5cqVC5jr8+yzz5ogRz/PuD5HDbJ0uy7vv/9+go9/6NAh6dOnj2zcuNFHZwzAF1L55ChAgJg7d6488cQTEhYWJs8884wUL15cLl26JD/99JN07dpVtm7dKh999JEtr60331WrVknPnj2lXbt2trxG3rx5zeukTp1akkOqVKnk/Pnz8u2330rjxo09tk2ZMsUEdhcuXEjUsTXQ6Nu3r+TLl09Kly4d7+d9//33iXo9APFDoAHcsHfvXmnSpIm5GS9evFiyZ8/u/mzatm0rv/32mwlE7HL8+HHzM2PGjLa9hmYL9GaeXDSA0+zQ559/fkugMXXqVKlTp4589dVXSXIuGvCkTZtWQkNDk+T1gGBF0wlww8CBA+Xs2bPyySefeAQZLvfee6906NDB/fjKlSvSr18/ueeee8wNVL9Jv/7663Lx4kWP5+n6//3vfyYr8sADD5gbvTbLfPbZZ+59NOWvAY7SzIkGBPo8V5OD6/eY9Dm6X0w//PCDVKlSxQQrERERUrhwYXNO/1ajoYHVf/7zH0mXLp15bv369WX79u1xvp4GXHpOup/WkrRs2dLctOPrqaeeknnz5snp06fd69auXWuaTnRbbCdPnpQuXbpIiRIlzHvSppf//ve/smnTJvc+S5culfLly5vf9XxcTTCu96k1GJqdWrdunTz44IMmwHB9LrFrNLT5Sq9R7Pdfu3ZtyZQpk8mcAIg/Ag3gBk3nawBQqVKleH0mrVu3lt69e0uZMmVkyJAhUrVqVRkwYIDJisSmN+fHH39cHn74Yfnggw/MDUtv1toUoxo2bGiOoZo2bWrqM4YOHZqga6PH0oBGA5233nrLvE69evVkxYoVd3zewoULzU302LFjJpjo3LmzrFy50mQeNDCJTTMRf//9t3mv+rvezLXJIr70vWoQMHPmTI9sRpEiRcxnGduePXtMUay+t8GDB5tATOtY9PN23fTvu+8+857V888/bz4/XTSocDlx4oQJULRZRT/b6tWrx3l+WouTOXNmE3BcvXrVrBs7dqxpYhkxYoTkyJEj3u8VgIg4ATjPnDnj1P8c6tevH69PY+PGjWb/1q1be6zv0qWLWb948WL3urx585p1y5cvd687duyYMywszPnqq6+61+3du9fsN2jQII9jtmjRwhwjtjfffNPs7zJkyBDz+Pjx47c9b9drjB8/3r2udOnSzixZsjhPnDjhXrdp0yanw+FwPvPMM7e8XqtWrTyO+dhjjzmjoqJu+5ox30e6dOnM748//rizRo0a5verV686s2XL5uzbt2+cn8GFCxfMPrHfh35+b731lnvd2rVrb3lvLlWrVjXbxowZE+c2XWJasGCB2b9///7OPXv2OCMiIpwNGjT41/cI4FZkNAARiY6ONp9D+vTp4/V5fPfdd+anfvuP6dVXXzU/Y9dyFC1a1DRNuOg3Zm3W0G/rvuKq7fj666/l2rVr8XrO4cOHTS8Nza7cdddd7vUlS5Y02RfX+4zpxRdf9His70uzBa7PMD60iUSbO44cOWKabfRnXM0mSpulHI7rf6o0w6Cv5WoWWr9+fbxfU4+jzSrxoV2MteeRZkk0A6NNKZrVAJBwBBqAiGn3V9okEB/79+83Nz+t24gpW7Zs5oav22PKkyfPLcfQ5pNTp0757PN/8sknTXOHNulkzZrVNOF8+eWXdww6XOepN+3YtDnir7/+knPnzt3xvej7UAl5L48++qgJ6r744gvT20TrK2J/li56/tqsVLBgQRMs3H333SZQ+/XXX+XMmTPxfs2cOXMmqPBTu9hq8KWB2PDhwyVLlizxfi6Amwg0gBuBhra9b9myJUGfR+xizNsJCQmJc73T6Uz0a7jqB1zSpEkjy5cvNzUXzZs3NzdiDT40MxF7X294815cNGDQTMHEiRNl1qxZt81mqHfeecdkjrTeYvLkybJgwQJT9FqsWLF4Z25cn09CbNiwwdStKK0JAZA4BBrADVpsqIN16VgW/0Z7iOhNTntKxHT06FHTm8LVg8QXNGMQs4eGS+ysidIsS40aNUzR5LZt28zAX9o0sWTJktu+D7Vz585btu3YscNkD7Qnih00uNCbuWaR4iqgdZkxY4Yp3NTeQLqfNmvUrFnzls8kvkFffGgWR5tZtMlLi0u1R5L2jAGQcAQawA3dunUzN1VtetCAITYNQrRHgiv1r2L3DNEbvNLxIHxFu89qE4FmKGLWVmgmIHY30NhcA1fF7nLrot14dR/NLMS8cWtmR3tZuN6nHTR40O7BH374oWlyulMGJXa2ZPr06fLnn396rHMFRHEFZQnVvXt3OXDggPlc9Jpq92LthXK7zxHA7TFgFxDjhq7dLLW5QesTYo4Mqt099eamRZOqVKlS5sajo4TqjU27Wq5Zs8bcmBo0aHDbrpOJod/i9cb32GOPySuvvGLGrBg9erQUKlTIoxhSCxe16USDHM1UaNp/1KhRkitXLjO2xu0MGjTIdPusWLGiPPfcc2bkUO3GqWNkaHdXu2j25Y033ohXpknfm2YYtOuxNmNoXYd2RY59/bQ+ZsyYMab+QwOPChUqSP78+RN0XpoB0s/tzTffdHe3HT9+vBlro1evXia7ASAB4uiJAgS1Xbt2Odu0aePMly+fMzQ01Jk+fXpn5cqVnSNGjDBdLV0uX75sumTmz5/fmTp1amfu3LmdPXr08NhHadfUOnXq/Gu3ytt1b1Xff/+9s3jx4uZ8Chcu7Jw8efIt3VsXLVpkuufmyJHD7Kc/mzZtat5P7NeI3QV04cKF5j2mSZPGGRkZ6axbt65z27ZtHvu4Xi9291k9lq7XY8e3e+vt3K57q3YDzp49uzk/Pc9Vq1bF2S3166+/dhYtWtSZKlUqj/ep+xUrVizO14x5nOjoaHO9ypQpY65vTJ06dTJdfvW1AcSfpf+XkMAEAAAgvqjRAAAAtiHQAAAAtiHQAAAAtiHQAAAgAC1fvlzq1q1rBiPUcWZ0csKYtERTJ4bUbu46oJ2OTxN7bCDtNt+sWTMzqKH26tKeaTrLdUIQaAAAEIDOnTtnuuKPHDkyzu3aVVuH19cu4atXrzZdwnUm5wsXLrj30SBDZ4bW0XjnzJljghcdxC4h6HUCAECAsyzLDPKn4/y4shma6dCJILt06WLW6cCAOk/ShAkTzPg927dvN6Pj6qi45cqVM/vMnz/fDOT3xx9/mOfHBwN2eUGHoD506JAZHMiXwx8DAOynN1sdAl9vmK4Zgu1w4cIFM/CfL8439r1G5w3SJaH27t1rZk3W5hIXHaRPB7nTaRg00NCf2lziCjKU7q+flWZAdBDB+CDQ8IIGGblz5/bmEACAZHbw4EEzgq5dQUaa9FEiV857fayIiIhb6iN0BNvEjOCrQYbSDEZM+ti1TX/GnrU4VapUZlZj1z7xQaDhBc1kqNCiLcQKif/00/BPB5a+n9ynAMCH/o6Olnvz53b/LbfDJc1kXDkvYUVbiHhzn7h6Sc5um2iCIi3MdElMNiOpEWh4wZXC0iCDQCPwxfyPG0DgSJKm71ThXt0nnJbD/XfIF3+LXBMZ6gSS2uvERR+7JmPUfXTOpJiuXLlieqLcaSLE2Oh1AgCA3SwT0Xix+PZ0dLJBDRYWLVrkXhcdHW1qL3SCRaU/ddLIdevWeUw6qPWJWssRX2Q0AACwm+W4vnjz/ATSeo7ffvvNowB048aNpsYiT5480rFjR+nfv78ULFjQBB46O7EWxrp6pugs1o888oi0adPGdIG9fPmytGvXzhSKxrfHiSLQAAAgAP3yyy9SvXp19+POnTubny1atDBdWLt162bG2tBxMTRzUaVKFdN9NTw83P2cKVOmmOCiRo0aprdJo0aNzNgbCcE4Gl7QNJN2Bwor0YYajSBwau2HyX0KAHz8NzxrVAYzfoRdNVjRrvvE/S+LFZL4wk3n1YtyccMoW8/VLmQ0AAAIwKaTlMJ/zxwAAKR4ZDQAALCbdaP3iDfP91MEGgAA2M7hZfOH/zZA+O+ZAwCAFI+MBgAAdrNoOgEAALYFGg56nQAAAPgaTScAANjNoukEAADYFmg4grbphIwGAAB2s4I3o+G/IRIAAEjxyGgAAGA3i6YTAABga9OJw7vn+ymaTgAAgG1oOgEAwG4O6/rizfP9FIEGAAB2s4K3RsN/zxwAAKR4ZDQAALCbFbzjaBBoAABgN4umEwAAAJ8jowEAgN0smk4AAIBtgYYjaHudkNEAAMBuVvBmNPw3RAIAACkeGQ0AAOxm0XQCAABsCzQsmk4AAAB8jaYTAABs5/Cy54j/llQSaAAAYDeLphMAAACfI6MBAECSZDQc3j3fTxFoAABgNyt4u7f675kDAIAUj4wGAAB2s4K3GJRAAwAAu1nB23RCoAEAgN2s4M1o+G+IBAAAUjwyGgAA2M2i6QQAANgWaFg0nQAAAPgaTScAANjMsiyzeHEA8VcEGgAA2MwK4kCDXicAAMA2ZDQAALCbdWPx5vl+ikADAACbWTSdAAAA+B4ZDQAAbGYFcUaDQAMAAJtZQRxoBEWvk3379pkLvHHjxtvuM2HCBMmYMaMEq0r33yOfD35Btn33tpxa+6E8WrXkLfv0eKGObJ/3thz6cbDMGtlOCuTO7LE9Y2Ra+ahfC9m/ZJDsWzxQhr/xlKRLE5qE7wK+NO7LZVKyXm/JVrmj1Hx2kKzbuo8POEBxrZMu0LC8WPxVwAUazz77rDRo0CDBz3vyySdl165dEqzSpgmTLbv+lK4Dv4hze4dnasoLT1aVzgOmycMt35fz/1ySr0a0lbDQm0mxcf1aSJEC2aVhuw+lSacxUun+e2Xo608l4buAr8z8fp28MXSWdG/9X1k6qbsUL5hTGrUfKcdP/s2HHGC41rBbwAUaiZUmTRrJkiWLBKuFK7fJ22PmyNylv8a5/cWm1eX9TxfIvOWbZetvh+SlNz+TbHdnkDpVS5nthfJllZqViskr/afKuq375edNe6T7+9OlYa0yZj/4l1FTF8szDSpJs3oVTfA4uEcTSRseKpO/WZXcpwYf41oncfdWy4vFTyVroFGtWjVp3769dOzYUTJlyiRZs2aVcePGyblz56Rly5aSPn16uffee2XevHlm/6tXr8pzzz0n+fPnN4FB4cKFZdiwYe7j9enTRyZOnChff/21O9W0dOlS9/Y9e/ZI9erVJW3atFKqVClZtermH81gbzq5k7w5o0ywsHTNDve66HMXTCq9fMl85nH5EvnldPR52bj9gHufpWt2yrVrTilbPG+ynDcS59LlK7Jxx0Gp9kBh9zqHwyFVHygsazfv5WMNIFzrpGPRdJJ8NDC4++67Zc2aNSboeOmll+SJJ56QSpUqyfr166VWrVrSvHlzOX/+vFy7dk1y5col06dPl23btknv3r3l9ddfly+//NIcq0uXLtK4cWN55JFH5PDhw2bR47j07NnT7KO1GoUKFZKmTZvKlStXkvHd+4esUZHm5/ETnmnzYyf+liw3tuk+x095br969Zqcij7vfj78w4nTZ821y3xXeo/1me+KlGMnopPtvOB7XGsERdOJZhbeeOMNKViwoPTo0UPCw8NN4NGmTRuzToOJEydOyK+//iqpU6eWvn37Srly5UxWo1mzZibz4Qo0IiIiTKYjLCxMsmXLZpbQ0JvFiBpk1KlTxwQZepz9+/fLb7/9Fu9zvXjxokRHR3ssAADEb5Z4y4vFfz/jZA80Spa82bshJCREoqKipESJEu512pyijh07Zn6OHDlSypYtK5kzZzaBxUcffSQHDhxI8Gtlz57d47jxMWDAAMmQIYN7yZ07twSDoze+xWaO8vyGmyUqvfsbru6TOZPn9pAQh2SKTOt+PvxDVMYIc+1iF34ePxntzmAhMHCtk46l//Mm0PDjIo1kDzQ0SxGTfqAx17m69GizybRp00xWQus0vv/+e9MEohmNS5cuJfi1Yh43vjTjcubMGfdy8OBBCQb7/zwhR/46I1XL32yzT58uXMoWyydrf73e5VHb7rV7a6kiN4OvB8sVEofDknVb9ifLeSNxQlOnktJFcsuytTvd6/S/k+Vrd5laHAQOrjWSgl8N2LVixQpTc/Hyyy+71/3+++8e+2hTiRaN2kGbZHQJRDreRf4Y42LkzRElxQvllNNnzssfR0/JmM+XSJdWj8ieg8dN4PH6i3VM8DF32Saz/659R2Xhyq0yrOdTpgts6lQhMrBrY5n5/XqzH/zLy089JC/3nST335dHyhTLJ6M/XyLn/rkozer+X3KfGnyMa500rCAesMuvAg2t2fjss89kwYIFpkZj0qRJsnbtWvO7S758+cz2nTt3mmYYbeLAvyt9X16ZM7aD+/E7nRuZn1Pn/Cxt+06WYZ8tNGNtDHm9qWSISCM/b/pdHn9llFy8dLOYtk2viTKoa2OZPaq9OJ1O+WbxRnnt/el8/H6oYa2y8tfps/LO2Lmm6LdEoZwyY3hbmk4CENc6iVjM3uoXXnjhBdmwYYMZXEsjQ+01otkNV/dXpUWk2qVVC0bPnj0rS5YsMcEH7mzF+t2SqXy7O+4zYOxcs9yOdm9t02sCH3WAeL5xVbMg8HGtYSfLqV89kSja60QzJmEl2ogVwlDbgU6HZgcQWH/Ds0ZlMDV3kZGRtt4nMjX9RByhaRN9nGuXzsupz5+z9Vzt4ldNJwAABGONhkWNBgAAINBIgd1bAQCAb2nvy169ermn7LjnnnukX79+plDfRX/XQTF1XCndp2bNmrJ7924fnwmBBgAAATep2nvvvSejR4+WDz/8ULZv324eDxw4UEaMGOHeRx8PHz5cxowZI6tXr5Z06dJJ7dq15cKFCz5969RoAAAQYDUaK1eulPr165tpN5T2vvz888/NvGKubMbQoUPNFCC6n9LhI3Q07tmzZ0uTJk3EV2g6AQDAT0THmm9L5+CKiw5uuWjRItm1a5d5vGnTJvnpp5/kv//9r3m8d+9eOXLkiGkucdHeMRUqVPCY2dwXyGgAAOAnGY3csebYevPNN6VPnz637P/aa6+ZQKRIkSJmHjGt2Xj77bfNZKRKg4yY84m56GPXNl8h0AAAwE8CjYMHD3qMo3G7aTF0VvMpU6bI1KlTpVixYmZusI4dO0qOHDmkRYsWkpQINAAA8BORkZHxGrCra9euJqvhqrXQWdH3799vZiHXQCNbtmxm/dGjR92zmbsely5d2qfnTI0GAAA2s7yZIj4R2ZDz58+Lw+F5i9cmFNeM5drtVYMNreNw0aYW7X1SsWJF8SUyGgAABNikanXr1jU1GXny5DFNJzpP2ODBg6VVq1bXD2dZpimlf//+ZsJSDTx03A1tWmnQoIH4EoEGAAABZsSIESZw0IlHjx07ZgIInZhUB+hy6datm5w7d06ef/55OX36tFSpUkXmz58v4eHhPj0XJlXzApOqBRcmVQMCS1JOqpat1WSvJ1U78unTTKoGAABuZTGpGgAAsIsVxIEGvU4AAIBtKAYFACDAep2kJAQaAADYzKLpBAAAwPfIaAAAYDMriDMaBBoAANjMEi8DDT8u0qDXCQAAsA0ZDQAAbGbRdAIAAOyLNCRou7fSdAIAAGxD0wkAADazaDoBAAAEGr5HRgMAAJtZ1vXFm+f7K2o0AACAbchoAACQJBkNy6vn+ysCDQAA7GZ5GSz4caBB0wkAALANGQ0AAGxm0b0VAADYF2gIvU4AAAB8jaYTAABs5nBYZkkspxfPTW4EGgAA2IymEwAAABuQ0QAAwGYWvU4AAIB9gYYEba8TMhoAANjMCuKMBiODAgAA25DRAADAZlYQZzQINAAAsJkVxDUaNJ0AAADbkNEAAMBmlnjZdOLH88QTaAAAYDOLphMAAADfI6MBAIDNLHqdAAAA+wINodcJAACAr9F0AgCAzSyaTgAAgH2BhgRt0wkZDQAAbGYFcUaDkUEBAIBtyGgAAGA3y8vmD/9NaBBoAABgN4umEwAAAN+j6QQAAJtZ9DoBAAD2BRoWvU4AAAB8jaYTAABsZtF0AgAA7As0LJpOAAAAfI2mEwAAbGYFcUaDQAMAAJtZ1GgAAAD7Ag0raDMaTKoGAABsQ9MJAAA2s2g6AQAA9gUaFk0nAAAAvkbTCQAANrNuNJ9483x/RaABAIDNHJZlFm+e76/odQIAAGxDRgMAAJtZ9DoBAAD2BRoWvU4AAIA9HJb3S0L9+eef8vTTT0tUVJSkSZNGSpQoIb/88ot7u9PplN69e0v27NnN9po1a8ru3bt9+8ap0QAAIPCcOnVKKleuLKlTp5Z58+bJtm3b5IMPPpBMmTK59xk4cKAMHz5cxowZI6tXr5Z06dJJ7dq15cKFCz49F2o0AACwm+XlfCUJfOp7770nuXPnlvHjx7vX5c+f3yObMXToUHnjjTekfv36Zt1nn30mWbNmldmzZ0uTJk3EV+h1AgBAEhWDWl4sKjo62mO5ePFinK/3zTffSLly5eSJJ56QLFmyyP333y/jxo1zb9+7d68cOXLENJe4ZMiQQSpUqCCrVq3y6Xsn0AAAwE/kzp3bBASuZcCAAXHut2fPHhk9erQULFhQFixYIC+99JK88sorMnHiRLNdgwylGYyY9LFrm6/QdAIAgM2sG//z5vnq4MGDEhkZ6V4fFhYW5/7Xrl0zGY133nnHPNaMxpYtW0w9RosWLSQpkdEAAMBPep1ERkZ6LLcLNLQnSdGiRT3W3XfffXLgwAHze7Zs2czPo0ePeuyjj13bfPbefXo0AACQ7CpXriw7d+70WLdr1y7JmzevuzBUA4pFixa5t2vNh/Y+qVixok/PhaYTAAACbMCuTp06SaVKlUzTSePGjWXNmjXy0UcfmcV1vI4dO0r//v1NHYcGHr169ZIcOXJIgwYNJMkDDa1eja969ep5cz4AAAQcK4mHIC9fvrzMmjVLevToIW+99ZYJJLQ7a7Nmzdz7dOvWTc6dOyfPP/+8nD59WqpUqSLz58+X8PDwxJ9oXOfu1M60/8LhiF8Li0ZIV69elWChaSat+g0r0UaskNDkPh3Y7NTaD/mMgQD7G541KoOcOXPGo8DSjvvEo8OXSOo0EYk+zuV/zsp3r1S39VztEq+MhlavAgCAxHEE8TTxXtVo6DClvk6xAAAQaKwgnr01wb1OtGmkX79+kjNnTomIiDCDgigtIvnkk0/sOEcAAAKiGNTyYgmaQOPtt9+WCRMmmMlYQkNv1iUUL15cPv74Y1+fHwAA8GMJDjR00hXtHqOVqyEhIe71pUqVkh07dvj6/AAA8HuWj+Y6CYoaDZ3f/t57742zYPTy5cu+Oi8AAAKGI4iLQROc0dAhTX/88cdb1s+YMcOMpQ4AAJDojEbv3r3NhCya2dAsxsyZM80wp9qkMmfOnIQeDgCAgGfdWLx5ftBkNOrXry/ffvutLFy4UNKlS2cCj+3bt5t1Dz/8sD1nCQCAH7OCuNdJosbR+M9//iM//PCD788GAAAElEQP2PXLL7+YTIarbqNs2bK+PC8AAAKGI8ZU74l9ftAEGn/88Yc0bdpUVqxYIRkzZjTrdDIWnSVu2rRpkitXLjvOEwAAv2Ul8eytfl2j0bp1a9ONVbMZJ0+eNIv+roWhug0AACDRGY1ly5bJypUrpXDhwu51+vuIESNM7QYAALiV5b9JiaQNNHLnzh3nwFw6B0qOHDl8dV4AAAQMi6aT+Bs0aJC0b9/eFIO66O8dOnSQ999/35YLBABAIBSDOrxYAjqjkSlTJo9ClHPnzkmFChUkVarrT79y5Yr5vVWrVtKgQQP7zhYAAAReoDF06FD7zwQAgABlBXHTSbwCDR1yHAAAJI4VxEOQJ3rALnXhwgW5dOmSx7rIyEhvzwkAAASIBAcaWp/RvXt3+fLLL+XEiRNx9j4BAAA3MU18AnTr1k0WL14so0ePlrCwMPn444+lb9++pmurzuAKAAA8WZb3S9BkNHSWVg0oqlWrJi1btjSDdN17772SN29emTJlijRr1syeMwUAAIE/BLkOOV6gQAF3PYY+VlWqVJHly5f7/gwBAPBzVhBPE5/gQEODjL1795rfixQpYmo1XJkO1yRr8D+V7r9HPh/8gmz77m05tfZDebRqyVv26fFCHdk+72059ONgmTWynRTIndlje8bItPJRvxayf8kg2bd4oAx/4ylJlyY0Cd8FfGncl8ukZL3ekq1yR6n57CBZt3UfH3CA4lrbzwrippMEBxraXLJp0ybz+2uvvSYjR46U8PBw6dSpk3Tt2lVSqmeffZbBxO4gbZow2bLrT+k68Is4t3d4pqa88GRV6Txgmjzc8n05/88l+WpEWwkLvdn6Nq5fCylSILs0bPehNOk0Rirdf68Mff0p319M2G7m9+vkjaGzpHvr/8rSSd2leMGc0qj9SDl+8m8+/QDDtUaKCzQ0oHjllVfM7zVr1pQdO3bI1KlTZcOGDWYY8qSg9SEdO3ZMktcKFgtXbpO3x8yRuUt/jXP7i02ry/ufLpB5yzfL1t8OyUtvfibZ7s4gdaqWMtsL5csqNSsVk1f6T5V1W/fLz5v2SPf3p0vDWmXMfvAvo6YulmcaVJJm9Sqa4HFwjyaSNjxUJn+zKrlPDT7GtU7aXicOL5agCTRi0yLQhg0bSsmSt6baERjy5owywcLSNTvc66LPXTCp9PIl85nH5Uvkl9PR52Xj9gPufZau2SnXrjmlbPG8yXLeSJxLl6/Ixh0HpdoDN2dodjgcUvWBwrJ28/VmUwQGrnXSsYK46SRevU6GDx8e7wO6sh12NoHoVPW6DBs2zKz77bff5J133jHdbo8cOSJ58uSRl19++Y4Zlvnz50v//v1ly5YtEhISIhUrVjTHu+eee2w9f3+UNer6IGzHT3imzY+d+Fuy3Nim+xw/5bn96tVrcir6vPv58A8nTp811y7zXek91me+K1J27zuabOcF3+NaJx2LIcjvbMiQIfH+IO0ONDQY2LVrlxQvXlzeeust96RvuXLlkunTp0tUVJSsXLlSnn/+ecmePbs0btz4tgOPde7c2WRizp49K71795bHHntMNm7caL69xeXixYtmcYmOjrbpXQIAEBjildFw9TJJCTJkyCChoaGSNm1ayZYtm3u9Dhrmkj9/flm1apXpEXO7QKNRo0Yejz/99FPJnDmzbNu2zQQxcRkwYIDH6wSLoyeuB1SZo9K7f1dZotLL5l1/uPfJnMnzG3BIiEMyRab1eA5SvqiMEebaxS78PH4y2p3BQmDgWicdh5e1Cl7XOSQjfz53D9r7pWzZsiZYiIiIkI8++kgOHLhZLxDb7t27pWnTpqa7ro4Hki/f9VqDOz2nR48ecubMGfdy8OBBCQb7/zwhR/46I1XL32yzT58uXMoWyydrf73e5VHb7rV7a6kiud37PFiukDgclqzbsj9ZzhuJE5o6lZQukluWrd3pXnft2jVZvnaXqcVB4OBaJx0riMfR8GpStZRi2rRp0qVLF/nggw9MrUX69Oll0KBBsnr16ts+p27duqaQddy4cWb4dP1DqpmM2JPExaRDrusSiHS8i/wxxsXImyNKihfKKafPnJc/jp6SMZ8vkS6tHpE9B4+bwOP1F+uY4GPusutdnXftOyoLV26VYT2fMl1gU6cKkYFdG8vM79eb/eBfXn7qIXm57yS5/748UqZYPhn9+RI5989FaVb3/5L71OBjXGvYzS8DDW06iTl524oVK6RSpUqmANTl999/v+3zdTK4nTt3miBDh1BXP/30kwSz0vfllTljbxbPvtP5etPS1Dk/S9u+k2XYZwvNWBtDXm8qGSLSyM+bfpfHXxklFy9dcT+nTa+JMqhrY5k9qr04nU75ZvFGee396cnyfuCdhrXKyl+nz8o7Y+eaot8ShXLKjOFtaToJQFzrpGFZ2sXVu+f7K78MNLSZQ7MV+/btM80kBQsWNPOvLFiwwNRnTJo0SdauXWt+j4sWj2rRqDavaMGoNpfo4GPBbMX63ZKpfLs77jNg7Fyz3I52b23Ta4INZ4fk8HzjqmZB4ONa28/hZaDhzXOTm1/WaGgziXZJLVq0qKnJqF27thnL48knn5QKFSqYjEXM7EZs2qtEm1vWrVtnmkt0EDJtagEAAL5lOTXHnUA//vijjB071jRPzJgxQ3LmzGmyCJpB0MnVgoV2b9VeMGEl2ogVwpwegU7ngAEQWH/Ds0ZlMMX92inAzvtE22m/SFjaiEQf5+L5szKySTlbzzXFZDS++uork0FIkyaNGXbcNa6EvnkdNAsAAMTddOLwYvFXCQ40dDTNMWPGmELK1KlTu9dXrlxZ1q9f7+vzAwAAEkTFoNpb48EHH7xlvaaGTp8+7avzAgAgYFhezldiBVNGQ0fj1LlFYtPuoTr4FQAAiHWztZi9Nd7atGljJivT7qU6UtmhQ4dkypQppifISy+9xL8tAABiBxri/RI0TSc63oSOolmjRg05f/68aUbR0TI10Gjfvr09ZwkAAIIj0NAsRs+ePaVr166mCUVnPtXxLHTgLAAAENe9U4K2RiOVN8OAa4ABAADuzCHXazS8eX7QBBrVq1e/4yxyixcv9vacAABAgEhwoFG6dGmPx5cvX5aNGzfKli1bpEWLFr48NwAAAoJF00n8DRkyJM71ffr0MfUaAADAk4NJ1bz39NNPy6effsq/LQAA4Ptp4letWiXh4eG+OhwAAAHVdOLwohg0qHqd6HTsMenkr4cPH5ZffvlFevXq5ctzAwAgIFjUaMSfzmkSk8PhkMKFC8tbb70ltWrV8vnFAQAAQZLRuHr1qrRs2VJKlCghmTJlsu+sAAAIIA6KQeMnJCTEZC2YpRUAgPizfPA/f5XgeVqKFy8ue/bssedsAAAI4IyGw4slaAKN/v37mwnU5syZY4pAo6OjPRYAAIAE12hoseerr74qjz76qHlcr149j6HItfeJPtY6DgAAcFMw12jEO9Do27evvPjii7JkyRJ7zwgAgABjWdYd5wmLz/MDPtDQjIWqWrWqnecDAACCtXurP0dUAAAkFwdNJ/FTqFChfw02Tp486ZOLAgBAoLAYGTT+dRqxRwYFAADwSdNJkyZNJEuWLAl5CgAAQc9hWV5NqubNc/1mHA3qMwAA8M8Bu959911zH+/YsaN73YULF6Rt27YSFRUlERER0qhRIzl69GjyBRquXicAAMB/rF27VsaOHSslS5b0WN+pUyf59ttvZfr06bJs2TI5dOjQLTO0J2mgce3aNZpNAABIDOtmQWhilsROdXL27Flp1qyZjBs3zmMy1DNnzsgnn3wigwcPloceekjKli0r48ePl5UrV8rPP/+cvEOQAwCAhN5sLa8XFXvaj4sXL97xdbVppE6dOlKzZk2P9evWrZPLly97rC9SpIjkyZNHVq1a5dPLS6ABAIDNLC8zGq5a0Ny5c5ven65lwIABt33NadOmyfr16+Pc58iRIxIaGioZM2b0WJ81a1azLdl6nQAAgORz8OBBiYyMdD8OCwu77X4dOnSQH374QcLDwyU5kdEAAMBPep1ERkZ6LLcLNLRp5NixY1KmTBlJlSqVWbTgc/jw4eZ3zVxcunRJTp8+7fE87XWSLVs2n753MhoAAATYOBo1atSQzZs3e6xr2bKlqcPo3r27aYJJnTq1LFq0yHRrVTt37pQDBw5IxYoVxZcINAAACDDp06eX4sWLe6xLly6dGTPDtf65556Tzp07y1133WWyI+3btzdBxv/93//59FwINAAACMK5ToYMGSIOh8NkNLT3Su3atWXUqFE+fx0CDQAAbOYQL5tOEjuQRgxLly71eKxFoiNHjjSLnSgGBQAAtiGjAQBAEDadJBUCDQAAbObwsgnBn5sf/PncAQBACkdGAwAAm1mWZRZvnu+vCDQAALCZlfgJWN3P91cEGgAABNjIoCkJNRoAAMA2ZDQAAEgCVpB+ygQaAADYzAricTRoOgEAALYhowEAgM0surcCAAC7OBgZFAAAwPdoOgEAwGYWTScAAMC2QEOCd2RQep0AAADb0HQCAIDNLJpOAACAXRxB3OuEjAYAADazgjij4c9BEgAASOHIaAAAYDMriHudEGgAAGAzi0nVAAAAfI+MBgAANnOIZRZvnu+vCDQAALCZRdMJAACA75HRAADAZtaN/3nzfH9FoAEAgM0smk4AAAB8j4wGAAA2s7zsdULTCQAAuH2gYF1fEh1o+G+JBhkNAADsZgVxoMGkagAAwDbUaAAAYDOL7q0AAMAuDuv64s3z/RVNJwAAwDY0nQAAYDOLphMAAGBboGHR6wQAAMDnaDoBAMBmlpeje/pxLSiBBgAAdnPQ6wQAAMD3aDoBAMBmFr1OAACAbYGGFby9TshoAACQJMWgiefHcQYjgwIAAPuQ0QAAwGYOscThRfuHPt9fEWgAAGAzi6YTAAAA3yOjAQCA3azgTWkQaAAAYDMriMfRcCT3CQAAgMBFRgMAALtZXg665b8JDQINAADsZgVviQZNJwAAwD40nQAAYDcreFMaBBoAANjMCuJeJwQaAADYzAri2Vvp3goAAGxDRgMAAJtZwVuiQaABAIDtrOCNNGg6AQAAtiHQAAAgiXqdWF78LyEGDBgg5cuXl/Tp00uWLFmkQYMGsnPnTo99Lly4IG3btpWoqCiJiIiQRo0aydGjR338zgk0AABIsl4nlhdLQixbtswEET///LP88MMPcvnyZalVq5acO3fOvU+nTp3k22+/lenTp5v9Dx06JA0bNvT5e6cYFACAADN//nyPxxMmTDCZjXXr1smDDz4oZ86ckU8++USmTp0qDz30kNln/Pjxct9995ng5P/+7/98di40nQAAkES1oJYXi4qOjvZYLl68GK/X18BC3XXXXeanBhya5ahZs6Z7nyJFikiePHlk1apVPn3vBBoAAPhJpJE7d27JkCGDe9FajH9z7do16dixo1SuXFmKFy9u1h05ckRCQ0MlY8aMHvtmzZrVbPMlmk4AAPATBw8elMjISPfjsLCwf32O1mps2bJFfvrpJ0kOBBoAAPjJXCeRkZEegca/adeuncyZM0eWL18uuXLlcq/Pli2bXLp0SU6fPu2R1dBeJ7rNl2g6AQAgwHqdOJ1OE2TMmjVLFi9eLPnz5/fYXrZsWUmdOrUsWrTIvU67vx44cEAqVqwovkRGAwCAABsYtG3btqZHyddff23G0nDVXWhdR5o0aczP5557Tjp37mwKRDVL0r59exNk+LLHiSLQAAAgwIwePdr8rFatmsd67cL67LPPmt+HDBkiDofDDNSlvVdq164to0aN8vm5EGjAqHT/PdK+eU0pVSSPZM+cQZp1+Ui+W/arx6fT44U68kyDSpIhIo2s/nWPvPruF7Ln4HH39oyRaWVg1yekdpXiJm33zeKN0uODGXLun0t8yn5o3JfLZMTkRXLsRLQUL5hT3uv6hJQtli+5Tws24FoHXkrD6XT+6z7h4eEycuRIs9iJGg0YadOEyZZdf0rXgV/E+Yl0eKamvPBkVek8YJo83PJ9Of/PJflqRFsJC70Zq47r10KKFMguDdt9KE06jZFK998rQ19/ik/YD838fp28MXSWdG/9X1k6qbsJNBq1HynHT/6d3KcGH+NaB+YQ5ClJigo0NJ1jWZZZtEhF+/M+/PDD8umnn5p+wLDPwpXb5O0xc2TuUs8shsuLTavL+58ukHnLN8vW3w7JS29+JtnuziB1qpYy2wvlyyo1KxWTV/pPlXVb98vPm/ZI9/enS8NaZcx+8C+jpi422atm9Sqa4HFwjyaSNjxUJn/j24F8kPy41giqQEM98sgjcvjwYdm3b5/MmzdPqlevLh06dJD//e9/cuXKleQ+vaCUN2eUCRaWrtnhXhd97oKs27pPype8nkovXyK/nI4+Lxu3H3Dvs3TNTrl2zSlli+dNlvNG4ly6fEU27jgo1R4o7F6n7bhVHygsazfv5WMNIFzrwO11kpKkuEBDBx/RPrw5c+aUMmXKyOuvv26qZjXo0LHa1eDBg6VEiRKSLl06M0rayy+/LGfPnjXbdMIYrZ6dMWOGx3Fnz55t9v/77+up3+7du0uhQoUkbdq0UqBAAenVq5cZjhW3yhp1vc/28ROeafNjJ/6WLDe26T7HT3luv3r1mpyKPu9+PvzDidNnzbXLfFd6j/WZ74o09RoIHFxr/xuC3B+luEAjLjrhS6lSpWTmzJnub1fDhw+XrVu3ysSJE00f4W7dupltGkw0adLEVNbGpI8ff/xx081H6U8NXLZt2ybDhg2TcePGmQrcO9Gq3NjjzAMAAD8PNFyTvWhzitIx27VJJV++fCYI6d+/v3z55ZfufVu3bi0LFiwwTTDq2LFj8t1330mrVq3c+7zxxhtSqVIlc4y6detKly5dPI4RFx1TPuYY85pNCQZHb3yLzRzl+Q03S1R69zdc3SdzJs/tISEOyRSZ1v18+IeojBHm2sUu/Dx+MtqdwUJg4FonISt4Uxp+E2hoVx0tElULFy6UGjVqmOYVzUw0b95cTpw4IefPnzfbH3jgASlWrJjJdqjJkydL3rx5zdS4Ll988YWZYEabaSIiIkzgoSOi3UmPHj3MDHiuRcecDwb7/zwhR/46I1XL32yzT58u3HR1XPvr9eBP2+61e2upIjeDrwfLFRKHw5J1W/Yny3kjcUJTp5LSRXLLsrU73eu0GHv52l2mFgeBg2uddCx6naR827dvN0OoalZDC0NLliwpX331lZnq1tUHWMdtj5nVcNV0aLNJy5Yt3YGKToHbrFkzefTRR80Y8Bs2bJCePXt6PP929SOuceYTOt58SpcuTagUL5TTLCpvjijze66smczjMZ8vkS6tHpH/PlhCit6TQ0b3aW6Cj7nLNpntu/YdlYUrt8qwnk9JmaJ5pULJAjKwa2OZ+f16sx/8y8tPPSSfzV4pn8/5WXbuPSKd3/1Czv1zUZrV9e2IgUh+XGvYzS8G7NIajM2bN0unTp1MYKHfrj744ANTq6HiavJ4+umnTd2G1nJoHUaLFi3c21auXGkyHBpcuOzfH9zfukvfl1fmjO3gfvxO50bm59Q5P0vbvpNl2GcLzVgbQ15vagbs+nnT7/L4K6Pk4qWbPYHa9Joog7o2ltmj2rsH7Hrt/enJ8n7gnYa1yspfp8/KO2PnmqLfEoVyyozhbWk6CUBc66RhedlzxJ97naS4QEMLLnVM9qtXr5pZ5ObPn29qIzSL8cwzz5ipbrV3yIgRI0xtxYoVK2TMmDG3HCdTpkzSsGFD6dq1q9SqVctj1rqCBQuaZpJp06ZJ+fLlZe7cuWbimWC2Yv1uyVS+3R33GTB2rlluR7u3tul1PYsE//d846pmQeDjWgfeXCcpSYqr0dDAInv27KZIU8fUWLJkiclKaBfXkJAQ0/tEu7e+9957Urx4cZkyZYoJROKiE8Zoc0jMIlBVr149kx3Rme1Kly5tMhzavRUAAFtYwVsMajnjMyC6n5o0aZIJKA4dOiShoaE+P752b9XeJ2El2ogV4vvjI2U5tfbD5D4FAD7+G541KoMp7rer5i76xn1i3e7DEpE+8a9x9u9oKVswu63nGjRNJ76gvU+0a+u7774rL7zwgi1BBgAA8WV5OV8Jc52kMAMHDjTjbmjXVe2SCgBAsrK8HH7cj5tOUlyNhi/06dPHFIwuWrTIjJEBAACSR0A2nQAAkJJYQdzrhEADAAC7WcEbaQRk0wkAAEgZyGgAAGAzK4h7nRBoAABgMyuIhyCn6QQAANiGjAYAADazgrcWlEADAADbWcEbaZDRAADAZlYQF4NSowEAAGxDRgMAgKRoObG8e76/ItAAAMBmVvCWaNB0AgAA7ENGAwAAm1lBPGAXgQYAALazgrbxhF4nAADANmQ0AACwmUXTCQAAsC3QkGBtOKHpBAAA2IimEwAAbGbRdAIAAGwLNCR45zohowEAgN2s4C3SoHsrAACwDRkNAABsZgVvQoNAAwAAu1lBXAxK0wkAALANTScAANjMotcJAACwMdKQYC3SoOkEAADYhqYTAABsZgVvQoNAAwAAu1n0OgEAAPA9mk4AALCd5eV8Jf7beEKgAQCAzSyaTgAAAHyP7q0AAMA2NJ0AAGAzK4ibTgg0AACwmRXEQ5DTdAIAAGxDRgMAAJtZNJ0AAADbAg0J3iHIaToBAAC2oekEAAC7WcGb0iDQAADAZha9TgAAAHyPjAYAADaz6HUCAABsCzQkaEs06HUCAECSRRqWF0sijBw5UvLlyyfh4eFSoUIFWbNmjSQ1urcCABCAvvjiC+ncubO8+eabsn79eilVqpTUrl1bjh07lqTnQaABAEAS9TqxvPhfQg0ePFjatGkjLVu2lKJFi8qYMWMkbdq08umnn0pSItAAACCJikEtL5aEuHTpkqxbt05q1qzpXudwOMzjVatWSVKi14kXnE7n9Z9XL/nqeiAFi46OTu5TAOBDf9/4b9r1tzwl//2IvvH82McJCwszS2x//fWXXL16VbJmzeqxXh/v2LFDkhKBhhf+/vtv8/PStom+uh5IwbJGjUvuUwBg09/yDBky2PLZhoaGSrZs2aRg/txeHysiIkJy5/Y8jtZf9OnTR1IyAg0v5MiRQw4ePCjp06cXK6F5LT+l0bT+Q9f3HRkZmdynAxtxrYNLMF5vzWRokKF/y+0SHh4ue/fuNU0Zvjjf2PeauLIZ6u6775aQkBA5evSox3p9rIFPUiLQ8IK2d+XKlUuCkf4hCpY/RsGOax1cgu1625XJiB1shIeHS1LSTErZsmVl0aJF0qBBA7Pu2rVr5nG7du2S9FwINAAACECdO3eWFi1aSLly5eSBBx6QoUOHyrlz50wvlKREoAEAQAB68skn5fjx49K7d285cuSIlC5dWubPn39LgajdCDSQINoeqMVHt2sXRODgWgcXrndgateuXZI3lcRmOZOiXw8AAAhKDNgFAABsQ6ABAABsQ6ABAABsQ6ABn9q3b58ZUGbjxo233WfChAmSMWNGPnk/8eyzz7r74QNAQhFoIMlvQNrlateuXXzyyaBatWrSsWNHPvsA/e9Rg3xdUqdObbowPvzww2amTh2oCUguBBpIcmnSpJEsWbLwyQM+9sgjj8jhw4dNZnHevHlSvXp16dChg/zvf/+TK1eu8HkjWRBoBNE32fbt25tvs5kyZTLfdsaNG+ceJU7na7n33nvNHyels/4999xzkj9/fhMYFC5cWIYNG+Y+nk7iM3HiRPn666/d36KWLl3q3r5nzx7zRy5t2rRSqlQpj2mJaTpJvm+8y5YtM9fRdc1+//33O17nuOiAP1WqVDHNX1FRUeYmpsdByhgLQ+exyJkzp5QpU0Zef/1189+o/net/92pwYMHS4kSJSRdunRmbpOXX35Zzp49a7bp3wMdfnzGjBkex509e7bZ3zWRZPfu3aVQoULmv+8CBQpIr1695PLly8nwjuEPCDSCiAYGOtHOmjVrTNDx0ksvyRNPPCGVKlWS9evXS61ataR58+Zy/vx5k2rVeVymT58u27ZtMyPL6R+tL7/80hyrS5cu0rhxY/c3KF30OC49e/Y0+2ithv5Batq0Kd+okpkGEBUrVpQ2bdq4r5le4ztd57jozUiHNv7ll1/MvAk6589jjz1Gej6Feuihh0ywP3PmTPNYr9fw4cNl69at5m/C4sWLpVu3bmabBhNNmjSR8ePHexxDHz/++OPmC4nSnxq46L8Z/XelX1qGDBmSDO8OfkEH7ELgq1q1qrNKlSrux1euXHGmS5fO2bx5c/e6w4cP6+BtzlWrVsV5jLZt2zobNWrkftyiRQtn/fr1PfbZu3evOcbHH3/sXrd161azbvv27ebx+PHjnRkyZPDp+0P8/x106NDhjvvE5zrHdPz4cXN9N2/ezGVIRne6Tk8++aTzvvvui3Pb9OnTnVFRUe7Hq1evdoaEhDgPHTpkHh89etSZKlUq59KlS2/72oMGDXKWLVvW6/eAwERGI4iULFnS/btOH6xpb02hurjGvz927Jj5OXLkSDP7X+bMmSUiIkI++ugjOXDgQIJfK3v27B7HRcqS0Ou8e/duk6HSlLmm2fPly2fWx/ffBpJezOnFFy5cKDVq1DDNK5qZ0CzmiRMnTCZT6eRbxYoVM9kONXnyZMmbN688+OCD7uN98cUXUrlyZdNMo/9m3njjDa4/botAI4hoJXpMrur0mI+VNptMmzbNNH1o+/33339vmkC0luPSpUsJfq2Yx0XKkpjrXLduXTl58qRJl69evdosKr7/NpD0tm/fbupwtEhUa2r0i8BXX30l69atM4Fm7OvXunVrd02HNpvovwnXf8dab9WsWTN59NFHZc6cObJhwwbTVMr1x+0wqRritGLFClNzoYViLrEL/kJDQ03RKPxH7GsWn+sck37z3blzpwky/vOf/5h1P/30k81nDW9oDcbmzZulU6dOJrDQgP+DDz4wtRoqrnqcp59+2tRtaC2H1mHoVOMuK1euNBkODS5c9u/fz0XCbZHRQJwKFixoiv0WLFhgxrzQqvK1a9d67KMp819//dXceP766y+qzv2AXjPNQOg3W71m8bnOMWmPJW1y0+aV3377zdzEtDAUKcPFixfNdOB//vmnKfB+5513pH79+iaL8cwzz5ieZdo7ZMSIEaZn2KRJk2TMmDFxXueGDRtK165dTZG4Fgy76L8ZbSbTbJgGpRqMzJo1K4nfKfwJgQbi9MILL5g/NDq4VoUKFcw32ZjfepX2XtDukOXKlTPt+/rtGCmbNpNofU7RokXNNatdu/a/XueY9Fuw3mD0m3Hx4sXNt+RBgwYl6XvAnbsea02UBpTaI2zJkiUmENAurnrdtfeJdm997733zPWbMmWKDBgwIM5jaXOaNoe0atXKY329evXMddepx0uXLm0yHBqgArfDNPEAgFtotkMDikOHDpkmNyCxqNEAALhp7xMdY+Xdd981mU2CDHiLphMAgNvAgQOlSJEiputqjx49+GTgNZpOAACAbchoAAAA2xBoAAAA2xBoAAAA2xBoAAAA2xBoAH7u2WeflQYNGrgfV6tWTTp27Jjk57F06VIzH8bp06dvu49unz17dryP2adPHzMolDd0FFR9XZ3HBUDSI9AAbLr5681NFx2HQId+fuutt+TKlSu2f94zZ86Ufv36+Sw4AABvMGAXYBMdAlpnvtT5J7777jtp27atmdU2rrEJdKhnXw2MdNddd/nkOADgC2Q0AJuEhYWZQY90psuXXnpJatasKd98841Hc8fbb78tOXLkMHPGqIMHD0rjxo0lY8aMJmDQCbE09e+iM6/qJGa6XSc30xk2nU6nx+vGbjrRQKd79+6SO3duc06aXfnkk0/McatXr+6eREszG3peSmf41DkwdGrxNGnSmDkyZsyY4fE6GjwVKlTIbNfjxDzP+NLz0mOkTZtWChQoYObM0Em/Yhs7dqw5f91PP58zZ854bP/444/lvvvuk/DwcDPY1KhRoxJ8LgDsQaABJBG9IWvmwmXRokVm5tsffvhB5syZY26wOslZ+vTp5ccffzST1EVERJjMiOt5Or33hAkT5NNPPzXTs588efJfZ87UWTs///xzM7nW9u3bzU1bj6s37q+++srso+ehw04PGzbMPNYg47PPPjMze27dutXMeaFThy9btswdEOlkbHXr1jW1D61bt5bXXnstwZ+Jvld9PzoVub62Tj8/ZMgQj310llidyvzbb781k4Zt2LDBY+I3nRisd+/eJmjT96czlmrAMnHixASfDwAbOAH4XIsWLZz169c3v1+7ds35ww8/OMPCwpxdunRxb8+aNavz4sWL7udMmjTJWbhwYbO/i25PkyaNc8GCBeZx9uzZnQMHDnRvv3z5sjNXrlzu11JVq1Z1dujQwfy+c+dOTXeY14/LkiVLzPZTp0651124cMGZNm1a58qVKz32fe6555xNmzY1v/fo0cNZtGhRj+3du3e/5Vix6fZZs2bddvugQYOcZcuWdT9+8803nSEhIc4//vjDvW7evHlOh8PhPHz4sHl8zz33OKdOnepxnH79+jkrVqxoft+7d6953Q0bNtz2dQHYhxoNwCaapdDMgWYqtCniqaeeMr0oXEqUKOFRl7Fp0ybz7V2/5cd04cIF+f33301zgWYddDp3l1SpUkm5cuVuaT5x0WyDTg9etWrVeJ+3noNOrPXwww97rNesyv33329+18xBzPNQFStWlIT64osvTKZF39/Zs2dNsWxkZKTHPnny5JGcOXN6vI5+npqF0c9Kn6tTmrdp08a9jx4nQ4YMCT4fAL5HoAHYROsWRo8ebYIJrcPQoCCmdOnSeTzWG23ZsmVNU0BsmTNnTnRzTULpeai5c+d63OCV1nj4yqpVq6RZs2bSt29f02SkgcG0adNM81BCz1WbXGIHPhpgAUh+BBqATTSQ0MLL+CpTpoz5hp8lS5ZbvtW7ZM+eXVavXi0PPvig+5v7unXrzHPjolkT/favtRVajBqbK6OiRaYuRYsWNQHFgQMHbpsJ0cJLV2Gry88//ywJsXLlSlMo27NnT/e6/fv337KfnsehQ4dMsOZ6HYfDYQpos2bNatbv2bPHBC0AUh6KQYEUQm+Ud999t+lposWge/fuNeNcvPLKK/LHH3+YfTp06CDvvvuuGfRqx44dpijyTmNg5MuXT1q0aCGtWrUyz3EdU4srld7otbeJNvMcP37cZAi0OaJLly6mAFQLKrVpYv369TJixAh3geWLL74ou3fvlq5du5omjKlTp5qizoQoWLCgCSI0i6GvoU0ocRW2ak8SfQ/atKSfi34e2vNEe/QozYho8ao+f9euXbJ582bTrXjw4MEJOh8A9iDQAFII7bq5fPlyU5OgPTo0a6C1B1qj4cpwvPrqq9K8eXNz49VaBQ0KHnvssTseV5tvHn/8cROUaNdPrWU4d+6c2aZNI3qj1h4jmh1o166dWa8DfmnPDb2B63lozxdtStHurkrPUXusaPCiXV+1d4r29kiIevXqmWBGX1NH/9QMh75mbJoV0s/j0UcflVq1aknJkiU9uq9qjxft3qrBhWZwNAujQY/rXAEkL0srQpP5HAAAQIAiowEAAGxDoAEAAGxDoAEAAGxDoAEAAGxDoAEAAGxDoAEAAGxDoAEAAGxDoAEAAGxDoAEAAGxDoAEAAGxDoAEAAGxDoAEAAMQu/w8QXWdOpqxp9wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Settings\n",
    "FRAME_H, FRAME_W = 128, 64\n",
    "FEATURE_DIM = FRAME_H * FRAME_W\n",
    "CLASS_NAMES = [\"mathi\",\"tala\",\"Daya\"]\n",
    "DATA_DIR = \"databig\"  # test dataset path\n",
    "\n",
    "# Load model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()  # already loaded\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Loop over classes\n",
    "for class_idx, class_name in enumerate(CLASS_NAMES):\n",
    "    class_path = os.path.join(DATA_DIR, f\"class_{class_idx}\")\n",
    "    if not os.path.exists(class_path):\n",
    "        continue\n",
    "    for sample_folder in os.listdir(class_path):\n",
    "        sample_path = os.path.join(class_path, sample_folder)\n",
    "        if not os.path.isdir(sample_path):\n",
    "            continue\n",
    "\n",
    "        # Collect frames in this sample\n",
    "        frames = []\n",
    "        for frame_file in sorted(os.listdir(sample_path)):\n",
    "            frame_path = os.path.join(sample_path, frame_file)\n",
    "            img = cv2.imread(frame_path)  # shape: (H,W,3)\n",
    "            if img is None:\n",
    "                continue\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            gray = cv2.resize(gray, (FRAME_W, FRAME_H))\n",
    "            gray = gray.astype(np.float32)/255.0\n",
    "            frames.append(gray.flatten())\n",
    "\n",
    "        # Make prediction if we have full sequence\n",
    "        if len(frames) != 50:\n",
    "            continue  # skip incomplete sequences\n",
    "\n",
    "        input_seq = torch.tensor(np.array(frames), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = model(input_seq)\n",
    "            probs = torch.softmax(preds, dim=1).cpu().numpy()[0]\n",
    "            predicted_class = probs.argmax()\n",
    "\n",
    "        y_true.append(class_idx)\n",
    "        y_pred.append(predicted_class)\n",
    "\n",
    "# ------------------------\n",
    "# Confusion matrix\n",
    "# ------------------------\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=CLASS_NAMES)\n",
    "disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a0943ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, ConfusionMatrixDisplay\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# ------------------------\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# SETTINGS\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# ------------------------\u001b[39;00m\n\u001b[32m     10\u001b[39m NUM_CLASSES = \u001b[32m8\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\MajorProject\\utsavenv\\Lib\\site-packages\\sklearn\\__init__.py:73\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[32m     70\u001b[39m     __check_build,\n\u001b[32m     71\u001b[39m     _distributor_init,\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     76\u001b[39m _submodules = [\n\u001b[32m     77\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcalibration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     78\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    114\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompose\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    115\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\MajorProject\\utsavenv\\Lib\\site-packages\\sklearn\\base.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_missing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_scalar_nan\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_repr_html\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReprHTMLMixin, _HTMLDocumentationLinkMixin\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\MajorProject\\utsavenv\\Lib\\site-packages\\sklearn\\utils\\__init__.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m metadata_routing\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_indexing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     _safe_indexing,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     18\u001b[39m     resample,\n\u001b[32m     19\u001b[39m     shuffle,\n\u001b[32m     20\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\MajorProject\\utsavenv\\Lib\\site-packages\\sklearn\\utils\\_chunking.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[32m     15\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\MajorProject\\utsavenv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m     21\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\MajorProject\\utsavenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeprecation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _deprecate_force_all_finite\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ComplexWarning, _preserve_dia_indices_dtype\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\MajorProject\\utsavenv\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexternals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_api_extra \u001b[38;5;28;01mas\u001b[39;00m xpx\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray_api_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# TODO: complete __all__\u001b[39;00m\n\u001b[32m     23\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mxpx\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# we import xpx here just to re-export it, need this to appease ruff\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\MajorProject\\utsavenv\\Lib\\site-packages\\sklearn\\utils\\fixes.py:20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m optimize\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m     22\u001b[39m     pd = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\MajorProject\\utsavenv\\Lib\\site-packages\\pandas\\__init__.py:22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev  \u001b[38;5;66;03m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\MajorProject\\utsavenv\\Lib\\site-packages\\pandas\\compat\\__init__.py:18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_typing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m F\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     19\u001b[39m     is_numpy_dev,\n\u001b[32m     20\u001b[39m     np_version_under1p21,\n\u001b[32m     21\u001b[39m )\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     23\u001b[39m     pa_version_under1p01,\n\u001b[32m     24\u001b[39m     pa_version_under2p0,\n\u001b[32m   (...)\u001b[39m\u001b[32m     31\u001b[39m     pa_version_under9p0,\n\u001b[32m     32\u001b[39m )\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\MajorProject\\utsavenv\\Lib\\site-packages\\pandas\\compat\\numpy\\__init__.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\" support numpy compatibility across versions \"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# numpy versioning\u001b[39;00m\n\u001b[32m      7\u001b[39m _np_version = np.__version__\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\MajorProject\\utsavenv\\Lib\\site-packages\\pandas\\util\\__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# pyright: reportUnusedImport = false\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_decorators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m      3\u001b[39m     Appender,\n\u001b[32m      4\u001b[39m     Substitution,\n\u001b[32m      5\u001b[39m     cache_readonly,\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhashing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m      9\u001b[39m     hash_array,\n\u001b[32m     10\u001b[39m     hash_pandas_object,\n\u001b[32m     11\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(name):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\MajorProject\\utsavenv\\Lib\\site-packages\\pandas\\util\\_decorators.py:14\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      7\u001b[39m     Any,\n\u001b[32m      8\u001b[39m     Callable,\n\u001b[32m      9\u001b[39m     Mapping,\n\u001b[32m     10\u001b[39m     cast,\n\u001b[32m     11\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mproperties\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cache_readonly\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_typing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     F,\n\u001b[32m     17\u001b[39m     T,\n\u001b[32m     18\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_exceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find_stack_level\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\MajorProject\\utsavenv\\Lib\\site-packages\\pandas\\_libs\\__init__.py:13\u001b[39m\n\u001b[32m      1\u001b[39m __all__ = [\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mNaT\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mNaTType\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mInterval\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minterval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtslibs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     15\u001b[39m     NaT,\n\u001b[32m     16\u001b[39m     NaTType,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     iNaT,\n\u001b[32m     22\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\MajorProject\\utsavenv\\Lib\\site-packages\\pandas\\_libs\\interval.pyx:1\u001b[39m, in \u001b[36minit pandas._libs.interval\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# ------------------------\n",
    "# SETTINGS\n",
    "# ------------------------\n",
    "NUM_CLASSES = 8\n",
    "FRAMES_PER_SAMPLE = 50\n",
    "FRAME_H, FRAME_W = 128, 64\n",
    "FEATURE_DIM = FRAME_H * FRAME_W\n",
    "MODEL_PATH = \"lip_lstm.pth\"\n",
    "DATA_DIR = \"data\"  # your dataset folder\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    \"Hello\",\n",
    "    \"Yes\",\n",
    "    \"No\",\n",
    "    \"Thank You\",\n",
    "    \"Love\",\n",
    "    \"Smile\",\n",
    "    \"Open\",\n",
    "    \"Close\"\n",
    "]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ------------------------\n",
    "# MODEL\n",
    "# ------------------------\n",
    "class LipLSTM(nn.Module):\n",
    "    def __init__(self, feature_dim, hidden_sizes=[256,128,64], num_classes=NUM_CLASSES, dropout=0.3):\n",
    "        super(LipLSTM, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=feature_dim, hidden_size=hidden_sizes[0], batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_sizes[0], hidden_size=hidden_sizes[1], batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(input_size=hidden_sizes[1], hidden_size=hidden_sizes[2], batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_sizes[2], 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.leaky_relu(out)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.leaky_relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out, _ = self.lstm3(out)\n",
    "        out = self.leaky_relu(out[:, -1, :])\n",
    "        out = self.dropout(out)\n",
    "        out = self.leaky_relu(self.fc1(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.leaky_relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# ------------------------\n",
    "# LOAD MODEL\n",
    "# ------------------------\n",
    "model = LipLSTM(FEATURE_DIM).to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "# ------------------------\n",
    "# LOAD DATA & PREDICT\n",
    "# ------------------------\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for class_idx in range(NUM_CLASSES):\n",
    "    class_folder = os.path.join(DATA_DIR, f\"class_{class_idx}\")\n",
    "    samples = sorted(os.listdir(class_folder))\n",
    "\n",
    "    for sample in samples:\n",
    "        sample_folder = os.path.join(class_folder, sample)\n",
    "        # load frames\n",
    "        frames = sorted(os.listdir(sample_folder))\n",
    "        sequence = []\n",
    "        for f in frames[:FRAMES_PER_SAMPLE]:\n",
    "            img_path = os.path.join(sample_folder, f)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, (FRAME_W, FRAME_H)).astype(np.float32) / 255.0\n",
    "            sequence.append(img.flatten())\n",
    "        sequence = np.array(sequence)\n",
    "        input_seq = torch.tensor(sequence, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        # predict\n",
    "        with torch.no_grad():\n",
    "            output = model(input_seq)\n",
    "            pred_class = output.argmax(dim=1).item()\n",
    "\n",
    "        y_true.append(class_idx)\n",
    "        y_pred.append(pred_class)\n",
    "\n",
    "# ------------------------\n",
    "# CONFUSION MATRIX\n",
    "# ------------------------\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=CLASS_NAMES)\n",
    "disp.plot(cmap='Blues', xticks_rotation=45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2245b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LipLSTM(nn.Module):\n",
    "    def __init__(self, feature_dim, hidden_sizes=[256,128,64], num_classes=3, dropout=0.3):\n",
    "        super(LipLSTM, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=feature_dim, hidden_size=hidden_sizes[0], batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_sizes[0], hidden_size=hidden_sizes[1], batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(input_size=hidden_sizes[1], hidden_size=hidden_sizes[2], batch_first=True)\n",
    "        \n",
    "        # FC layers exactly as in checkpoint\n",
    "        self.fc1 = nn.Linear(hidden_sizes[2], 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.leaky_relu(out)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.leaky_relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out, _ = self.lstm3(out)\n",
    "        out = self.leaky_relu(out[:, -1, :])  # last timestep\n",
    "        out = self.dropout(out)\n",
    "        out = self.leaky_relu(self.fc1(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.leaky_relu(self.fc2(out))\n",
    "        out = self.fc3(out)  # logits\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc5ff0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LipLSTM:\n\tMissing key(s) in state_dict: \"cnn.0.weight\", \"cnn.0.bias\", \"cnn.1.weight\", \"cnn.1.bias\", \"cnn.1.running_mean\", \"cnn.1.running_var\". \n\tUnexpected key(s) in state_dict: \"lstm4.weight_ih_l0\", \"lstm4.weight_hh_l0\", \"lstm4.bias_ih_l0\", \"lstm4.bias_hh_l0\", \"fc4.weight\", \"fc4.bias\", \"batchnorm1.weight\", \"batchnorm1.bias\", \"batchnorm1.running_mean\", \"batchnorm1.running_var\", \"batchnorm1.num_batches_tracked\", \"batchnorm2.weight\", \"batchnorm2.bias\", \"batchnorm2.running_mean\", \"batchnorm2.running_var\", \"batchnorm2.num_batches_tracked\", \"batchnorm3.weight\", \"batchnorm3.bias\", \"batchnorm3.running_mean\", \"batchnorm3.running_var\", \"batchnorm3.num_batches_tracked\". \n\tsize mismatch for lstm1.weight_ih_l0: copying a param with shape torch.Size([2048, 8192]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for lstm1.weight_hh_l0: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for lstm1.bias_ih_l0: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm1.bias_hh_l0: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm2.weight_ih_l0: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for lstm2.weight_hh_l0: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for lstm2.bias_ih_l0: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for lstm2.bias_hh_l0: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for lstm3.weight_ih_l0: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for lstm3.weight_hh_l0: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for lstm3.bias_ih_l0: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for lstm3.bias_hh_l0: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([128, 64]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for fc1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([64, 128]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for fc3.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([3, 32]).\n\tsize mismatch for fc3.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([3]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 132\u001b[39m\n\u001b[32m    130\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    131\u001b[39m model = LipLSTM(FEATURE_DIM).to(device)\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m model.eval()\n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m# ------------------------\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# CAMERA\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# ------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\majorproj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2593\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2585\u001b[39m         error_msgs.insert(\n\u001b[32m   2586\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2587\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2588\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2589\u001b[39m             ),\n\u001b[32m   2590\u001b[39m         )\n\u001b[32m   2592\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2594\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2595\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2596\u001b[39m         )\n\u001b[32m   2597\u001b[39m     )\n\u001b[32m   2598\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for LipLSTM:\n\tMissing key(s) in state_dict: \"cnn.0.weight\", \"cnn.0.bias\", \"cnn.1.weight\", \"cnn.1.bias\", \"cnn.1.running_mean\", \"cnn.1.running_var\". \n\tUnexpected key(s) in state_dict: \"lstm4.weight_ih_l0\", \"lstm4.weight_hh_l0\", \"lstm4.bias_ih_l0\", \"lstm4.bias_hh_l0\", \"fc4.weight\", \"fc4.bias\", \"batchnorm1.weight\", \"batchnorm1.bias\", \"batchnorm1.running_mean\", \"batchnorm1.running_var\", \"batchnorm1.num_batches_tracked\", \"batchnorm2.weight\", \"batchnorm2.bias\", \"batchnorm2.running_mean\", \"batchnorm2.running_var\", \"batchnorm2.num_batches_tracked\", \"batchnorm3.weight\", \"batchnorm3.bias\", \"batchnorm3.running_mean\", \"batchnorm3.running_var\", \"batchnorm3.num_batches_tracked\". \n\tsize mismatch for lstm1.weight_ih_l0: copying a param with shape torch.Size([2048, 8192]) from checkpoint, the shape in current model is torch.Size([1024, 128]).\n\tsize mismatch for lstm1.weight_hh_l0: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for lstm1.bias_ih_l0: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm1.bias_hh_l0: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm2.weight_ih_l0: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for lstm2.weight_hh_l0: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for lstm2.bias_ih_l0: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for lstm2.bias_hh_l0: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for lstm3.weight_ih_l0: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for lstm3.weight_hh_l0: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for lstm3.bias_ih_l0: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for lstm3.bias_hh_l0: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([128, 64]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for fc1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([64, 128]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for fc3.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([3, 32]).\n\tsize mismatch for fc3.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([3])."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import deque\n",
    "\n",
    "# ------------------------\n",
    "# SETTINGS\n",
    "# ------------------------\n",
    "NUM_CLASSES = 3\n",
    "FRAMES_PER_SAMPLE = 50\n",
    "FRAME_H, FRAME_W = 128, 64\n",
    "FEATURE_DIM = FRAME_H * FRAME_W\n",
    "MODEL_PATH = \"lip_lstm.pth\"\n",
    "\n",
    "PAD_X, PAD_Y = 30, 15\n",
    "ASPECT_RATIO = 2.0\n",
    "\n",
    "CLASS_NAMES = [\"mathi\", \"tala\", \"Daya\"]\n",
    "\n",
    "# ------------------------\n",
    "# MEDIA PIPE\n",
    "# ------------------------\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "OUTER_LIPS = [\n",
    "    61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "    291, 375, 321, 405, 314, 17, 84, 181, 91, 146\n",
    "]\n",
    "INNER_LIPS = [\n",
    "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "    308, 324, 318, 402, 317, 14, 87, 178, 88, 95\n",
    "]\n",
    "\n",
    "# ------------------------\n",
    "# MODEL\n",
    "# ------------------------\n",
    "\n",
    "class LipLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim,\n",
    "        hidden_sizes=[256, 128, 64],\n",
    "        num_classes=NUM_CLASSES,\n",
    "        dropout=0.3,\n",
    "        cnn_channels=128,\n",
    "        kernel_size=3\n",
    "    ):\n",
    "        super(LipLSTM, self).__init__()\n",
    "\n",
    "        # ---------- CNN FRONT ----------\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=feature_dim,\n",
    "                out_channels=cnn_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=kernel_size // 2\n",
    "            ),\n",
    "            nn.BatchNorm1d(cnn_channels),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # ---------- LSTM STACK ----------\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=cnn_channels,\n",
    "            hidden_size=hidden_sizes[0],\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=hidden_sizes[0],\n",
    "            hidden_size=hidden_sizes[1],\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.lstm3 = nn.LSTM(\n",
    "            input_size=hidden_sizes[1],\n",
    "            hidden_size=hidden_sizes[2],\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # ---------- FC HEAD ----------\n",
    "        self.fc1 = nn.Linear(hidden_sizes[2], 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, time, feature_dim)\n",
    "\n",
    "        # ---- CNN ----\n",
    "        x = x.permute(0, 2, 1)     # (batch, feature_dim, time)\n",
    "        x = self.cnn(x)\n",
    "        x = x.permute(0, 2, 1)     # (batch, time, cnn_channels)\n",
    "\n",
    "        # ---- LSTM ----\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.leaky_relu(out)\n",
    "\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.leaky_relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out, _ = self.lstm3(out)\n",
    "\n",
    "        # last time step\n",
    "        out = self.leaky_relu(out[:, -1, :])\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # ---- FC ----\n",
    "        out = self.leaky_relu(self.fc1(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.leaky_relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# LOAD MODEL\n",
    "# ------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LipLSTM(FEATURE_DIM).to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "# ------------------------\n",
    "# CAMERA\n",
    "# ------------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = deque(maxlen=FRAMES_PER_SAMPLE)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb)\n",
    "\n",
    "    lip_display = None  # window for processed lip ROI\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        face_landmarks = results.multi_face_landmarks[0]\n",
    "\n",
    "        outer_pts = np.array([[int(face_landmarks.landmark[i].x * w),\n",
    "                               int(face_landmarks.landmark[i].y * h)] for i in OUTER_LIPS], np.int32)\n",
    "        inner_pts = np.array([[int(face_landmarks.landmark[i].x * w),\n",
    "                               int(face_landmarks.landmark[i].y * h)] for i in INNER_LIPS], np.int32)\n",
    "\n",
    "        x, y, bw, bh = cv2.boundingRect(outer_pts)\n",
    "        x = max(0, x - PAD_X)\n",
    "        y = max(0, y - PAD_Y)\n",
    "        bw = min(w - x, bw + 2 * PAD_X)\n",
    "        bh = min(h - y, bh + 2 * PAD_Y)\n",
    "\n",
    "        # Adjust to keep aspect ratio\n",
    "        desired_bw = int(ASPECT_RATIO * bh)\n",
    "        dw = desired_bw - bw\n",
    "        x = max(0, x - dw // 2)\n",
    "        bw = min(w - x, desired_bw)\n",
    "\n",
    "        lip_roi = np.ones((bh, bw, 3), dtype=np.uint8) * 255\n",
    "        outer_shifted = outer_pts - [x, y]\n",
    "        inner_shifted = inner_pts - [x, y]\n",
    "        cv2.fillPoly(lip_roi, [outer_shifted], (180, 180, 180))\n",
    "        cv2.fillPoly(lip_roi, [inner_shifted], (0, 0, 0))\n",
    "\n",
    "        # Normalize & resize\n",
    "        lip_gray = cv2.cvtColor(lip_roi, cv2.COLOR_BGR2GRAY)\n",
    "        lip_norm = cv2.resize(lip_gray, (FRAME_W, FRAME_H))\n",
    "        lip_flat = lip_norm.astype(np.float32) / 255.0\n",
    "        sequence.append(lip_flat.flatten())\n",
    "\n",
    "        # Keep a display version\n",
    "        lip_display = cv2.resize(lip_gray, (FRAME_W*4, FRAME_H*4))  # scale up for visibility\n",
    "\n",
    "        # Predict\n",
    "        if len(sequence) == FRAMES_PER_SAMPLE:\n",
    "            input_seq = torch.tensor(np.array(sequence), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                preds = model(input_seq)\n",
    "                predicted_class = preds.argmax(dim=1).item()\n",
    "                predicted_name = CLASS_NAMES[predicted_class]\n",
    "\n",
    "            cv2.putText(frame, f\"Predicted: {predicted_name}\",\n",
    "                        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "\n",
    "    # Show both windows\n",
    "    cv2.imshow(\"Webcam Feed\", frame)\n",
    "    if lip_display is not None:\n",
    "        cv2.imshow(\"Processed Lip ROI\", lip_display)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC to quit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fac3cba8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LipLSTM:\n\tUnexpected key(s) in state_dict: \"lstm4.weight_ih_l0\", \"lstm4.weight_hh_l0\", \"lstm4.bias_ih_l0\", \"lstm4.bias_hh_l0\", \"fc4.weight\", \"fc4.bias\", \"batchnorm1.weight\", \"batchnorm1.bias\", \"batchnorm1.running_mean\", \"batchnorm1.running_var\", \"batchnorm1.num_batches_tracked\", \"batchnorm2.weight\", \"batchnorm2.bias\", \"batchnorm2.running_mean\", \"batchnorm2.running_var\", \"batchnorm2.num_batches_tracked\", \"batchnorm3.weight\", \"batchnorm3.bias\", \"batchnorm3.running_mean\", \"batchnorm3.running_var\", \"batchnorm3.num_batches_tracked\". \n\tsize mismatch for lstm1.weight_ih_l0: copying a param with shape torch.Size([2048, 8192]) from checkpoint, the shape in current model is torch.Size([1024, 8192]).\n\tsize mismatch for lstm1.weight_hh_l0: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for lstm1.bias_ih_l0: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm1.bias_hh_l0: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm2.weight_ih_l0: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for lstm2.weight_hh_l0: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for lstm2.bias_ih_l0: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for lstm2.bias_hh_l0: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for lstm3.weight_ih_l0: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for lstm3.weight_hh_l0: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for lstm3.bias_ih_l0: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for lstm3.bias_hh_l0: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([128, 64]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for fc1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([64, 128]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for fc3.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([3, 32]).\n\tsize mismatch for fc3.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([3]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 77\u001b[39m\n\u001b[32m     75\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     76\u001b[39m model = LipLSTM(FEATURE_DIM).to(device)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m model.eval()\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# ------------------------\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# CAMERA\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# ------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\majorproj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2593\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2585\u001b[39m         error_msgs.insert(\n\u001b[32m   2586\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2587\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2588\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2589\u001b[39m             ),\n\u001b[32m   2590\u001b[39m         )\n\u001b[32m   2592\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2594\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2595\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2596\u001b[39m         )\n\u001b[32m   2597\u001b[39m     )\n\u001b[32m   2598\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for LipLSTM:\n\tUnexpected key(s) in state_dict: \"lstm4.weight_ih_l0\", \"lstm4.weight_hh_l0\", \"lstm4.bias_ih_l0\", \"lstm4.bias_hh_l0\", \"fc4.weight\", \"fc4.bias\", \"batchnorm1.weight\", \"batchnorm1.bias\", \"batchnorm1.running_mean\", \"batchnorm1.running_var\", \"batchnorm1.num_batches_tracked\", \"batchnorm2.weight\", \"batchnorm2.bias\", \"batchnorm2.running_mean\", \"batchnorm2.running_var\", \"batchnorm2.num_batches_tracked\", \"batchnorm3.weight\", \"batchnorm3.bias\", \"batchnorm3.running_mean\", \"batchnorm3.running_var\", \"batchnorm3.num_batches_tracked\". \n\tsize mismatch for lstm1.weight_ih_l0: copying a param with shape torch.Size([2048, 8192]) from checkpoint, the shape in current model is torch.Size([1024, 8192]).\n\tsize mismatch for lstm1.weight_hh_l0: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for lstm1.bias_ih_l0: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm1.bias_hh_l0: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm2.weight_ih_l0: copying a param with shape torch.Size([1024, 512]) from checkpoint, the shape in current model is torch.Size([512, 256]).\n\tsize mismatch for lstm2.weight_hh_l0: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for lstm2.bias_ih_l0: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for lstm2.bias_hh_l0: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for lstm3.weight_ih_l0: copying a param with shape torch.Size([512, 256]) from checkpoint, the shape in current model is torch.Size([256, 128]).\n\tsize mismatch for lstm3.weight_hh_l0: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([256, 64]).\n\tsize mismatch for lstm3.bias_ih_l0: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for lstm3.bias_hh_l0: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([128, 64]) from checkpoint, the shape in current model is torch.Size([64, 64]).\n\tsize mismatch for fc1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([64, 128]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for fc3.weight: copying a param with shape torch.Size([32, 64]) from checkpoint, the shape in current model is torch.Size([3, 32]).\n\tsize mismatch for fc3.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([3])."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import deque\n",
    "\n",
    "# ------------------------\n",
    "# SETTINGS\n",
    "# ------------------------\n",
    "NUM_CLASSES = 3\n",
    "FRAMES_PER_SAMPLE = 50\n",
    "FRAME_H, FRAME_W = 128, 64\n",
    "FEATURE_DIM = FRAME_H * FRAME_W\n",
    "MODEL_PATH = \"lip_lstm.pth\"\n",
    "\n",
    "PAD_X, PAD_Y = 30, 15\n",
    "ASPECT_RATIO = 2.0\n",
    "\n",
    "CLASS_NAMES = [\"mathi\", \"tala\", \"Daya\"]\n",
    "\n",
    "# ------------------------\n",
    "# MEDIA PIPE\n",
    "# ------------------------\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "OUTER_LIPS = [\n",
    "    61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "    291, 375, 321, 405, 314, 17, 84, 181, 91, 146\n",
    "]\n",
    "INNER_LIPS = [\n",
    "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "    308, 324, 318, 402, 317, 14, 87, 178, 88, 95\n",
    "]\n",
    "\n",
    "# ------------------------\n",
    "# MODEL\n",
    "# ------------------------\n",
    "class LipLSTM(nn.Module):\n",
    "    def __init__(self, feature_dim, hidden_sizes=[256,128,64], num_classes=NUM_CLASSES, dropout=0.3):\n",
    "        super(LipLSTM, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=feature_dim, hidden_size=hidden_sizes[0], batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_sizes[0], hidden_size=hidden_sizes[1], batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(input_size=hidden_sizes[1], hidden_size=hidden_sizes[2], batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_sizes[2], 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.leaky_relu(out)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.leaky_relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out, _ = self.lstm3(out)\n",
    "        out = self.leaky_relu(out[:, -1, :])\n",
    "        out = self.dropout(out)\n",
    "        out = self.leaky_relu(self.fc1(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.leaky_relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# ------------------------\n",
    "# LOAD MODEL\n",
    "# ------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LipLSTM(FEATURE_DIM).to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "# ------------------------\n",
    "# CAMERA\n",
    "# ------------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = deque(maxlen=FRAMES_PER_SAMPLE)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb)\n",
    "\n",
    "    lip_display = None  # window for processed lip ROI\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        face_landmarks = results.multi_face_landmarks[0]\n",
    "\n",
    "        outer_pts = np.array([[int(face_landmarks.landmark[i].x * w),\n",
    "                               int(face_landmarks.landmark[i].y * h)] for i in OUTER_LIPS], np.int32)\n",
    "        inner_pts = np.array([[int(face_landmarks.landmark[i].x * w),\n",
    "                               int(face_landmarks.landmark[i].y * h)] for i in INNER_LIPS], np.int32)\n",
    "\n",
    "        x, y, bw, bh = cv2.boundingRect(outer_pts)\n",
    "        x = max(0, x - PAD_X)\n",
    "        y = max(0, y - PAD_Y)\n",
    "        bw = min(w - x, bw + 2 * PAD_X)\n",
    "        bh = min(h - y, bh + 2 * PAD_Y)\n",
    "\n",
    "        # Adjust to keep aspect ratio\n",
    "        desired_bw = int(ASPECT_RATIO * bh)\n",
    "        dw = desired_bw - bw\n",
    "        x = max(0, x - dw // 2)\n",
    "        bw = min(w - x, desired_bw)\n",
    "\n",
    "        lip_roi = np.ones((bh, bw, 3), dtype=np.uint8) * 255\n",
    "        outer_shifted = outer_pts - [x, y]\n",
    "        inner_shifted = inner_pts - [x, y]\n",
    "        cv2.fillPoly(lip_roi, [outer_shifted], (180, 180, 180))\n",
    "        cv2.fillPoly(lip_roi, [inner_shifted], (0, 0, 0))\n",
    "\n",
    "        # Normalize & resize\n",
    "        lip_gray = cv2.cvtColor(lip_roi, cv2.COLOR_BGR2GRAY)\n",
    "        lip_norm = cv2.resize(lip_gray, (FRAME_W, FRAME_H))\n",
    "        lip_flat = lip_norm.astype(np.float32) / 255.0\n",
    "        sequence.append(lip_flat.flatten())\n",
    "\n",
    "        # Keep a display version\n",
    "        lip_display = cv2.resize(lip_gray, (FRAME_W*4, FRAME_H*4))  # scale up for visibility\n",
    "\n",
    "        # Predict\n",
    "        if len(sequence) == FRAMES_PER_SAMPLE:\n",
    "            input_seq = torch.tensor(np.array(sequence), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                preds = model(input_seq)\n",
    "                predicted_class = preds.argmax(dim=1).item()\n",
    "                predicted_name = CLASS_NAMES[predicted_class]\n",
    "\n",
    "            cv2.putText(frame, f\"Predicted: {predicted_name}\",\n",
    "                        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "\n",
    "    # Show both windows\n",
    "    cv2.imshow(\"Webcam Feed\", frame)\n",
    "    if lip_display is not None:\n",
    "        cv2.imshow(\"Processed Lip ROI\", lip_display)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC to quit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18dc2b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import deque\n",
    "\n",
    "# ------------------------\n",
    "# SETTINGS\n",
    "# ------------------------\n",
    "NUM_CLASSES = 3\n",
    "FRAMES_PER_SAMPLE = 50\n",
    "FRAME_H, FRAME_W = 128, 64\n",
    "FEATURE_DIM = FRAME_H * FRAME_W\n",
    "MODEL_PATH = \"lip_lstm.pth\"  # trained model path\n",
    "\n",
    "PAD_X, PAD_Y = 30, 15\n",
    "ASPECT_RATIO = 2.0\n",
    "\n",
    "# Map class index to names\n",
    "CLASS_NAMES = [\n",
    "    \"mathi\",\n",
    "    \"tala\",\n",
    "    \"Daya\"]\n",
    "   # \"Baya\",\n",
    "  #  \"roka\",\n",
    "  #  \"agadi\",\n",
    "  #  \"pachadi\",\n",
    "  #  \"Jau\"\n",
    "#]\n",
    "\n",
    "# ------------------------\n",
    "# MEDIA PIPE\n",
    "# ------------------------\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "OUTER_LIPS = [\n",
    "    61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "    291, 375, 321, 405, 314, 17, 84, 181, 91, 146\n",
    "]\n",
    "INNER_LIPS = [\n",
    "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "    308, 324, 318, 402, 317, 14, 87, 178, 88, 95\n",
    "]\n",
    "\n",
    "# ------------------------\n",
    "# MODEL\n",
    "# ------------------------\n",
    "class LipLSTM(nn.Module):\n",
    "    def __init__(self, feature_dim, hidden_sizes=[256,128,64], num_classes=NUM_CLASSES, dropout=0.3):\n",
    "        super(LipLSTM, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=feature_dim, hidden_size=hidden_sizes[0], batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_sizes[0], hidden_size=hidden_sizes[1], batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(input_size=hidden_sizes[1], hidden_size=hidden_sizes[2], batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_sizes[2], 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.leaky_relu(out)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.leaky_relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out, _ = self.lstm3(out)\n",
    "        out = self.leaky_relu(out[:, -1, :])\n",
    "        out = self.dropout(out)\n",
    "        out = self.leaky_relu(self.fc1(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.leaky_relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# ------------------------\n",
    "# LOAD MODEL\n",
    "# ------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LipLSTM(FEATURE_DIM).to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval()\n",
    "\n",
    "# ------------------------\n",
    "# CAMERA\n",
    "# ------------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = deque(maxlen=FRAMES_PER_SAMPLE)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        face_landmarks = results.multi_face_landmarks[0]\n",
    "\n",
    "        outer_pts = np.array([[int(face_landmarks.landmark[i].x * w),\n",
    "                               int(face_landmarks.landmark[i].y * h)] for i in OUTER_LIPS], np.int32)\n",
    "        inner_pts = np.array([[int(face_landmarks.landmark[i].x * w),\n",
    "                               int(face_landmarks.landmark[i].y * h)] for i in INNER_LIPS], np.int32)\n",
    "\n",
    "        x, y, bw, bh = cv2.boundingRect(outer_pts)\n",
    "        x = max(0, x - PAD_X)\n",
    "        y = max(0, y - PAD_Y)\n",
    "        bw = min(w - x, bw + 2 * PAD_X)\n",
    "        bh = min(h - y, bh + 2 * PAD_Y)\n",
    "\n",
    "        desired_bw = int(ASPECT_RATIO * bh)\n",
    "        dw = desired_bw - bw\n",
    "        x = max(0, x - dw // 2)\n",
    "        bw = min(w - x, desired_bw)\n",
    "\n",
    "        lip_roi = np.ones((bh, bw, 3), dtype=np.uint8) * 255\n",
    "        outer_shifted = outer_pts - [x, y]\n",
    "        inner_shifted = inner_pts - [x, y]\n",
    "        cv2.fillPoly(lip_roi, [outer_shifted], (180, 180, 180))\n",
    "        cv2.fillPoly(lip_roi, [inner_shifted], (0, 0, 0))\n",
    "\n",
    "        # Convert to grayscale\n",
    "        lip_gray = cv2.cvtColor(lip_roi, cv2.COLOR_BGR2GRAY)\n",
    "        lip_norm = cv2.resize(lip_gray, (FRAME_W, FRAME_H))\n",
    "        lip_norm = lip_norm.astype(np.float32) / 255.0\n",
    "        lip_flat = lip_norm.flatten()\n",
    "\n",
    "        sequence.append(lip_flat)\n",
    "\n",
    "        # Predict when we have 50 frames\n",
    "        if len(sequence) == FRAMES_PER_SAMPLE:\n",
    "            input_seq = torch.tensor(np.array(sequence), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                preds = model(input_seq)\n",
    "                probs = torch.softmax(preds, dim=1).cpu().numpy()[0]\n",
    "                predicted_class = probs.argmax()\n",
    "                predicted_name = CLASS_NAMES[predicted_class]\n",
    "\n",
    "            # Display predicted class\n",
    "            cv2.putText(frame, f\"Predicted: {predicted_name}\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "\n",
    "            # Display confidence bars\n",
    "            start_y = 50\n",
    "            for i, (name, prob) in enumerate(zip(CLASS_NAMES, probs)):\n",
    "                bar_length = int(prob * 200)  # max 200 pixels\n",
    "                cv2.rectangle(frame, (10, start_y + i*25), (10 + bar_length, start_y + i*25 + 20), (0,255,0), -1)\n",
    "                cv2.putText(frame, f\"{name}: {prob*100:.1f}%\", (220, start_y + i*25 + 15),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1)\n",
    "\n",
    "    cv2.imshow(\"Lip ROI Live Test\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e00086fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# CAMERA\n",
    "# ------------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = deque(maxlen=FRAMES_PER_SAMPLE)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb)\n",
    "\n",
    "    lip_display = None  # new window image\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        face_landmarks = results.multi_face_landmarks[0]\n",
    "\n",
    "        outer_pts = np.array([[int(face_landmarks.landmark[i].x * w),\n",
    "                               int(face_landmarks.landmark[i].y * h)] for i in OUTER_LIPS], np.int32)\n",
    "        inner_pts = np.array([[int(face_landmarks.landmark[i].x * w),\n",
    "                               int(face_landmarks.landmark[i].y * h)] for i in INNER_LIPS], np.int32)\n",
    "\n",
    "        x, y, bw, bh = cv2.boundingRect(outer_pts)\n",
    "        x = max(0, x - PAD_X)\n",
    "        y = max(0, y - PAD_Y)\n",
    "        bw = min(w - x, bw + 2 * PAD_X)\n",
    "        bh = min(h - y, bh + 2 * PAD_Y)\n",
    "\n",
    "        desired_bw = int(ASPECT_RATIO * bh)\n",
    "        dw = desired_bw - bw\n",
    "        x = max(0, x - dw // 2)\n",
    "        bw = min(w - x, desired_bw)\n",
    "\n",
    "        # ------------------------\n",
    "        # Lip contour image for model\n",
    "        # ------------------------\n",
    "        lip_roi = np.ones((bh, bw, 3), dtype=np.uint8) * 255\n",
    "        outer_shifted = outer_pts - [x, y]\n",
    "        inner_shifted = inner_pts - [x, y]\n",
    "        cv2.fillPoly(lip_roi, [outer_shifted], (180, 180, 180))\n",
    "        cv2.fillPoly(lip_roi, [inner_shifted], (0, 0, 0))\n",
    "\n",
    "        # Show converted lip contour in separate window\n",
    "        lip_display = cv2.resize(lip_roi, (FRAME_W*2, FRAME_H*2))  # scaled up for visibility\n",
    "\n",
    "        # Convert to grayscale for model\n",
    "        lip_gray = cv2.cvtColor(lip_roi, cv2.COLOR_BGR2GRAY)\n",
    "        lip_norm = cv2.resize(lip_gray, (FRAME_W, FRAME_H))\n",
    "        lip_norm = lip_norm.astype(np.float32) / 255.0\n",
    "        lip_flat = lip_norm.flatten()\n",
    "\n",
    "        sequence.append(lip_flat)\n",
    "\n",
    "        # Predict when we have 50 frames\n",
    "        if len(sequence) == FRAMES_PER_SAMPLE:\n",
    "            input_seq = torch.tensor(np.array(sequence), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                preds = model(input_seq)\n",
    "                probs = torch.softmax(preds, dim=1).cpu().numpy()[0]\n",
    "                predicted_class = probs.argmax()\n",
    "                predicted_name = CLASS_NAMES[predicted_class]\n",
    "\n",
    "            # Display predicted class\n",
    "            cv2.putText(frame, f\"Predicted: {predicted_name}\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "\n",
    "            # Display confidence bars\n",
    "            start_y = 50\n",
    "            for i, (name, prob) in enumerate(zip(CLASS_NAMES, probs)):\n",
    "                bar_length = int(prob * 200)  # max 200 pixels\n",
    "                cv2.rectangle(frame, (10, start_y + i*25), (10 + bar_length, start_y + i*25 + 20), (0,255,0), -1)\n",
    "                cv2.putText(frame, f\"{name}: {prob*100:.1f}%\", (220, start_y + i*25 + 15),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1)\n",
    "\n",
    "    # ------------------------\n",
    "    # Show windows\n",
    "    # ------------------------\n",
    "    cv2.imshow(\"Lip ROI Live Test\", frame)\n",
    "    if lip_display is not None:\n",
    "        cv2.imshow(\"Converted Lip Contour\", lip_display)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5fe6f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "majorproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
